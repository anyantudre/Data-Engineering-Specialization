# Orchestration, Monitoring and Automating Data Pipelines


#### Before Orchestration

en
​
Interactive Transcript - Enable basic transcript mode by pressing the escape key
You may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key
Before talking about how you might use an orchestration tool to define and run your data pipelines. I'd like to back up and just spend a moment describing how you might have set up your data pipelines before orchestration tools. Before orchestration, the simplest way to automate a data pipeline, or any set of software tasks for that matter, was to set up a series of Cron jobs. So Cron is a command line utility that was first introduced in the 1970s to execute a particular command at a specified date and time. To schedule a job with cron, you simply write a command or entry known as a Cron Job, where you specify five numbers separated by spaces and followed by the command you want to schedule. So the structure of a Cron Job looks like this, where you have the command you want to execute over here on the right, and then from right to left. These five numbers indicate the day of the week, 0 to 6, for Sunday to Saturday, the month of the year from 1 to 12. 
The day of the month from 1 to 31, the hour of the day from 0 to 23, and the minute of the hour from 0 to 59. You can also include an asterisk or star in place of any of these five numbers to indicate no restriction on that value. So, for example, this cron job that says 0011 echo, Happy New Year. Will print Happy New Year to the terminal of the computer that it's running on every year at midnight on January 1. These first two zeros indicate the 0th minute of the 0th hour of the day, which is midnight, and the next two ones indicate the first day of the first month, which is January 1. And then the star and the day of the week position just says you don't care what day of the week it is when the other conditions are met. So how would you have scheduled the tasks in your data pipeline? 
Well, for example, suppose you had a data pipeline where you were ingesting data from a REST API, and suppose you wanted to ingest data from the API every night at midnight. And you had a python script called Ingest_from_rest_api.py to do the ingestion step. In that case, you could have written a cron job like this. 00 star star star, python ingestion python rest_api.py. So here, the first two zeros say that you want this command to be executed at midnight. And the asterisks in the other positions just indicate that you want to do this no matter what day, month, or day of the week it is. Then maybe you wanted to do some in flight cleaning or processing to the API data. And let's suppose you had another Python script written to perform that step. 
And maybe you knew that it always took less than an hour to ingest all the new data from the API. Then you could have set up the next step, the in flight transformations to take place at 1AM every morning. That would look like this as a cron job, 01 star star star python transform_api_data.py. And then maybe you were also ingesting data from a database every night at midnight. So you would have written another cron job that says 00 star star star python ingest_from_database.py. And maybe you wanted to combine the transform API data with the data from the database. And so you would have written yet another cron job, maybe one that kicks off at 2 AM every morning so that it happens after the other jobs are complete. 
So that would look like 02 star star star python combine_api_and_database.py and so you get the idea. And you could have written all the Cron jobs you needed to get the pipeline implemented, carefully timed to execute in a sequential manner. This is what's known as a pure scheduling approach, and this is how many data pipelines were automated before orchestration tools were available. In fact, this is still how many simple data pipelines are automated today. Just to be clear, I'm not suggesting you forego orchestration and set up your data pipelines as Cron jobs. The problem with a setup like this is that there are many ways in which you can fail. For example, if one task fails to run, or takes longer than expected, or produces some unexpected result, your entire pipeline can fail. 
And you would have essentially no way of knowing exactly how or why it failed without implementing tests and debugging to determine what went wrong. Or even worse, since you don't have any built-in monitoring or alerts to tell you how things are going. You might not learn about the failure until your downstream stakeholders come to you asking why the data looks funny. And so why am I talking about scheduling with Cron if I'm not recommending it? Well, first off, it can be a nice, intuitive way of wrapping your head around what it means to automate your data pipelines. And a Cron job can work great for some simple tasks, like a data download that needs to happen on a regular basis and doesn't have any downstream dependencies. Or if you're in the prototyping phase and testing various aspects of your data pipelines, using Cron can be a quick and easy way to get started. 
In the next video, I'll talk a little bit about orchestration tools in general and how they have evolved in recent years, and after that you'll get into orchestrating your data pipelines with airflow. I'll see you in the next video. 


#### Evolution of Orchestration Tools
Orchestration has long been a key capability for data processing. But until the past decade or so, orchestration was really only accessible to the largest companies. And this is because open source or managed orchestration tools did not exist yet, and it was complicated and expensive to build your own in house solutions. In the late 2000s, things began to change. Facebook developed a tool called Data Swarm for internal use, which they still use today. Another tool called Apache Oozie became extremely popular in the 2010s. But it's designed to work within a Hadoop cluster and was more difficult to use in a more heterogeneous environment. 
Inspired by these earlier tools, particularly Data Swarm, Airbnb introduced Airflow in 2014, which has become the industry standard orchestration tool. Today, there are many other orchestration tools under development and the landscape of orchestration will no doubt continue to evolve in the future. Now throughout these courses we have for the most part, actively avoided discussing any particular tool or technology too extensively. Instead, I have been aiming to focus on the skills and the knowledge that will be broadly applicable no matter where you work as a data engineer. In certain cases, however, I'm going to make an exception for a tool or technology that you will very likely use in your job as a data engineer. And one such tool is Airflow. Right now when it comes to orchestration, more teams are using Airflow than any other tool, and so it's a skill set that recruiters are looking for. 
With that being said, Airflow is not without its shortcomings, and I'm excited about some of the other newer open source tools that are emerging in the orchestration space. And so I'll mention a few of those too. Airflow was developed by Maxime Beauchemin and other collaborators at Airbnb. They were primarily interested in serving their own internal data orchestration needs. However, from the very beginning, they built Airflow as a noncommercial open source project with the vision that the tools they were developing to serve Airbnb's internal needs would also be useful to other teams solving similar challenges. The framework quickly grew significant mind share outside of Airbnb becoming an Apache incubator project in 2016 and a full Apache sponsored project in 2019. Today, Airflow offers many advantages as an orchestration platform, largely because of its dominant position in the open source marketplace. 
Airflow is written in Python, making it accessible to almost any use case imaginable. Beyond that, the Airflow open source project is very active with a high rate of commits and a quick response time for bugs and security issues. As of many open source tools and data engineering, Airflow is also available as a managed service through a number of vendors, including AWS, GCP, and astronomer.io for anyone who is looking for more comprehensive support. With all that being said, Airflow is certainly not the only orchestration tool out there. In terms of things like scalability, ensuring data integrity, and streaming pipelines, Airflow either does not solve for these issues or there are significant room for improvement. Many other interesting open source orchestration projects exist, such as Luigi and Conductor, and newer tools like Prefect, Dagster, and Mage have been getting traction as they aim to mimic the best elements of Airflows core design while improving on it in key areas. For example, Prefect provides more scalable orchestration solutions in Airflow, while Dagster certain Mage provide built in capabilities for data quality testing and data transformations among other things. 
Still other tools are focused on providing better orchestration support for streaming pipelines. I think it's entirely possible that one or more of these newer orchestration tools will become widely used in the coming years as alternatives to Airflow. It's also possible that one of these alternative tools could serve your needs better depending on what pipelines you're setting up. With that being said, my recommendation is to learn Airflow for now because Airflow is what many people are using today, but also keep learning about other tools and keep up on new developments in the field of orchestration so you can stay current as things continue to evolve. Join me in the next video to dive into the details of orchestration and practice. 


#### Orchestration Basics

en
​
Interactive Transcript - Enable basic transcript mode by pressing the escape key
You may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key
When it comes to orchestration, there are a number of concepts and components that are common to whatever set of tools you're using for implementation, and that's what I'd like to discuss in this video. While setting up proper orchestration of your data pipelines does come with more operational overhead than civil Cron scheduling. It also comes with the option to set up dependencies, monitor tasks, get alerts, and create fallback plans if something doesn't go as expected. When we looked at Cron scheduling in a previous video, we had a data pipeline that looked like this with data flowing in from two sources, going through some transformations, and eventually making its way to a data warehouse. As we've mentioned a couple of times already in these courses, the sort of visual representation of a data pipeline is referred to as a directed acyclic graph or DAG for short. Think of each task as a node in the graph. Then in graph terminology, you can call these arrows edges between the nodes. 
You can see that data flows only in one direction between the nodes or tasks in the graph. There's an overall sense of direction to your data pipeline. There are no circles or cycles, and so this is what it means for the graph to be directed and acyclic. Remember, with Cron scheduling, we could set up a pipeline like this, but if one task took longer to run than expected and the next task kicked off before the previous one finished, it could break everything downstream. Now, with orchestration, this is where the idea of dependencies comes in. You could, for example, build in dependencies between tasks in this pipeline that require the previous task to finish before the next task starts. Most orchestration frameworks allow you to, and in fact, require you to define your data pipelines as DAGs. 
In many cases, they include a user interface where you can visualize your DAG, as well as debug, troubleshoot, and monitor your data pipelines. In Airflow, you will define your DAGs programmatically by writing python code that looks like this to define all the tasks and dependencies in your data pipeline. You can visualize a pipeline you've defined using the Airflow UI, which looks like this. Here you can trigger the DAG to run. Monitor the progress of tasks, visualize the DAG you defined previously in code and troubleshoot any issues. When you have defined your pipeline as a DAG, you can set up the dependencies or conditions on which the DAG should run. These conditions could be time based if you want to run the DAG on a particular schedule or event-based if you want to trigger the DAG based on an event. 
For example, here's how you could define a DAG in airflow that should run every day at midnight. By setting the parameter scheduled to daily. To make your DAG event-based, you can use the same parameter schedule, but define it differently. More specifically, by defining the name of a dataset, you can schedule your DAG to run when the dataset is updated. You could also make a portion of your DAG, wait for some external process to complete. For example, some external process is going to upload a CSV file to an S3 bucket, then you can set your DAG to wait for the presence of that file in the S3 bucket. Here's how you could define a task and airflow called a sensor that listens for a file upload event. 
The portion of the DAG that starts with this task won't run until the myfile.csv is available in the S3 bucket. You can also set up monitoring and logging as well as alerts for the tasks in your DAG. For example, you might want to get an alert if a particular task fails or monitor how long a task takes to run. You can also set up data quality checks along the way to ensure that the data flowing through your data pipeline meets your expectations. That might include checking for things like the null values or the range of some set of values or just verifying that the schema of the data you're ingesting is what you expect. Again, I'm showing all this right now in Airflow, but the concepts I'm demonstrating are the kinds of things you can expect from any orchestration platform. Through the next several videos, I'm going to walk you through the steps of setting up orchestration of your data pipelines and airflow. I'll see you there. 


#### Airflow - Core Components
Before you head to the first lab of this week, I'd like to spend some time introducing you to Airflow. By the end of this series of videos, you will know how to build a simple DAG in Airflow and be ready for the first lab. So in this first video, we will start with the underlying architecture of Airflow. When you write your DAGs in Airflow, there are several components that work together behind the scenes to automatically run your DAGs, check that the dependencies between tasks are met, and transfer the status of your DAGs to the Airflow user interface. This figure shows the main components of Airflow, and these include a web server where the Airflow user interface runs, a scheduler, workers, a metadata database, and finally a DAG directory. When you create your Airflow environment, whether by directly installing Airflow or by using an Airflow managed service, all of these components will be present in your setup. You'll mainly interact with the DAG directory and the user interface, or UI for short. 
The remaining components will be running behind the scenes. The DAG directory is a folder in which you store the Python scripts that define your DAGs, and this DAG directory is connected to the web server on which the Airflow UI runs. So for any DAG that you create and add to the DAG directory, you will automatically be able to visualize it in the UI. And not only that, you can also use the UI to monitor, manually trigger, and troubleshoot the behavior of your DAGs and the tasks within each DAG. But you don't always need to manually trigger your DAGs from the UI. You can also trigger them based on a schedule, or with an event, or rather you can do so with the help of the scheduler component of Airflow. The scheduler constantly monitors all the DAGs you defined in the DAG directory and the corresponding tasks. 
Once per minute, by default, the scheduler checks whether any tasks should be triggered given a particular schedule, or by checking if their dependencies are complete. Once the scheduler identifies a task that is ready to be triggered, it pushes the task to a queue and uses an executor to manage the execution of tasks. The executor, which is part of the scheduler, extracts the tasks from the queue and sends them to the workers that run the tasks. As the scheduler triggers a given task, you will see the status of the task changes from scheduled to queued. Then once the workers execute the task, you will see the status change to running, and finally success or failed. The scheduler and the workers store the status of the tasks as well as the state of the DAGs in the metadata database. Then the web server extracts those states from the database and displays them to you in the UI. 
And so that was a quick tour of the core components of Airflow. When you choose a managed service for Airflow, like Amazon Managed Workflows for Apache Airflow, or MWAA for short, all of these components will be automatically created and managed for you. You'll notice in the architectural diagram how Amazon MWAA organizes the core components of Airflow on the cloud. For instance, it uses an Amazon S3 bucket as the DAG directory and an Aurora PostgreSQL database as the metadata database. The other components are AWS networking components and additional AWS services that support securing your data as well as logging and monitoring your environment. In the labs this week, you will first be guided to set up the Airflow environment. You will then write your DAGs as Python scripts, upload them to the S3 bucket, and finally open the UI to review the created DAGs. 
Understanding the Airflow environment and how components interact with each other will help you troubleshoot issues when they occur. Join me in the next video to go through some of the features of the Airflow UI. 


#### Airflow - The Airflow UI
One of the main features you'll interact with when using Airflow is the user interface or UI. This is where you can monitor the status of your dags and their individual tasks, get insights into your historical dag runs and troubleshoot issues with your pipelines. Let's take a look at some of the basic features of the Airflow UI. When you open the UI, you will first land on this page, which is called the Dag View. Here you can find a list of all the dags that you've created in your Dag directory. For each dag, you can see some basic metadata like the Dag ID, the tag, the owner and the schedule. You can also check when the dag was last run as well as the status of the Dag runs, how many dags are currently queued or currently running, and how many were completed successfully or have failed. 
There's also a more granular review for each dag where you can check the status of all tasks, including how many tasks are queued or running or successfully executed or failed, and also how many were skipped up for retry, up for reschedule, and so on. Let me show you how you can interact with the dags in this Dag View. Here on the left, you can see the toggle to pause or unpause a dag and here on the right, you can manually trigger a dag or delete it from the view. Finally, you can filter the dags you see on this page by the status or by custom tags you assign them. You can click on the Dag ID to get more detailed information about that dag. Let's take a closer look at this dag. This new view is called the grid view, and it gives you more detailed insights into each dag run and its corresponding task instances. 
On the left side, you see a bar chart of all the previous runs of this dag. Here the chart shows two dag runs. In other words, the dag was run twice. You can see the duration of each run, which is represented by the height of the bar. The status of each run is color coded, where red represents the failed status and green represents the success status. For each run, you can also check the outcome of all individual task instances. Here, the dag consists of two tasks for the dag run that failed. 
You can see that the first task had failed and the second one, it's orange coded, which means it did not start because the upstream task had failed. You can always refer to the color legend here at the top of the page to check what each color means. On the right side of the grid view, you can find four tabs, details, graph, gant, and code. The details tab displays detailed information about the historical dag runs, such as the total number of times the dag has run, the total number of successful runs, the total number of failed runs, and the minimum mean and max duration of the dag runs. You can use all of these metrics as indicators for the health of your pipeline. To access the details of a specific dag run, let's say the one that has failed, you can select it here in the bar and then check its specific information on the right. Let's check out the graph tab. 
Here, you can see a visualization of your dag, which helps you explore your dag structure and ensure that you have correctly configured the dependencies between your tasks. The Dag visualization you're seeing now does not correspond to any specific dag run. If you'd like to visualize the status of a specific dag run, let's say the one that has failed, you can click on it on the left side. Each task is now color coded based on its status within this run. Suppose you want to understand why the first task extract from API has failed. Let's click on this task. A new tab labeled Logs has just appeared on the top, and you can find all the error messages in this tab. 
Based on these messages, you can try to fix the code for this tag, and when you're done fixing it and ready to retry this task, you can also click on the clear task which will rerun the task. If it runs and it is completed successfully, then all the remaining tasks in the pipeline will run as well. Next, the gant chart tab shows you the cute duration in gray and the run duration for each task of a specific dag run. You may find this chart helpful whenever you need to identify any bottleenecks in your pipeline. Finally, the code tab shows a code that corresponds to the given dag. This is not where you will interact or edit your Dag code but you can use a tab to ensure that the code in the UI is in sync with your code in the Dag directory. There are many more features in the Airflow UI and more things you can do to interact with your dags. 
But in this video, we covered the basic UI features that you will use most of the time. In the next video, we'll go through the steps to build a simple dag. See you there. 


#### Airflow - Creating a DAG
In this video, we'll go through the details of building a simple DAG using some core Airflow concepts such as a DAG and operator classes. Let's take the example of an ETL process. The following diagram shows a DAG representation of such a process, which consists of three tasks, extract, transform, and load. So here I'm not going to get into the specific details of each task, but rather I'll focus on showing you how to set up a DAG that has this structure. So here I'm working in this environment. I have already created this folder in which I'm going to save the Python scripts of the DAG. Let's start with creating an empty Python script in this folder, and let's call it my underscore first underscore DAG dot py. 
In this script, you need to import some packages, starting with the DAG class and the date time module. You're going to use both of them to create the DAG instance, which is the next thing that we'll do. To create the DAG instance, you will use the with statement known as the context manager, which will help you group and define all the tasks that belong to this DAG here. Now you need to specify the parameters of the DAG. The first parameter is the DAG underscore ID. This is the name that you will use to identify your DAG in the DAG view in the Airflow UI. So when you choose to use your DAG underscore ID, make sure it's unique. 
For this example, I've chosen to use my underscore first underscore DAG as the DAG ID. You can also specify a DAG description if you wish to provide more details related to the DAG. The DAG description will appear when you hover your mouse over the DAG name in the Airflow UI. And on the other hand, you can also set the tags parameter to be a list of tags that you can use to filter DAGs in the UI. So for instance, one of the tags can be data underscore engineering underscore team. And the next important parameter is schedule, which you can use to define when the DAG will run. You can assign the schedule parameter a cron expression, just 0, 8, star, star, star, which means the DAG will run every day at 8 a.m. 
Or you can use cron presets such as at daily, at monthly, at hourly, at weekly, etc. Or you can pass a time delta object from the date time package. For example, this time delta object means that the DAG will run every three days. And in this example, I'm going to set the schedule to at daily. Besides the schedule, you need to also specify the start underscore date, which is the first date your DAG will be executed on. Here I'm using a date time object to specify the year, month, and day of the start date. The last parameter that I'm going to specify here is catch up. 
This is a Boolean parameter that is set by default to true. How is this parameter useful? Let's say that you paused your DAG for a period of time and then unpause a DAG to make it active again. If you set catch up to true, the scheduler will kick off a DAG run for any missed interval when the DAG was paused. Additionally, it will trigger a DAG run upon the first time you run the DAG if the start date of the DAG precedes its creation date. In this example, I'm going to set catch up to false. So that was the first step where you define the DAG instance and all of its parameters. 
Let's now move into defining the tasks for the DAG. You need to use Airflow operators to define each task. Operators are Python classes that are used to encapsulate the logic of the tasks or how data should be processed in your pipeline. Operators are provided by Airflow, which you can import to your code and create each task as an instance of an Airflow operator. There are several operators that you can choose from. You can use a Python operator if you want to execute a Python script that contains the logic of your task. You can choose a Bash operator if you want to execute Bash commands. 
You can use an empty operator if you want to organize your DAGs, such as marking the start and end of the pipeline. And finally, you can choose an email operator if you want Airflow to send you a notification via email. There's also a special type of operator known as sensors, which you can use to make your DAGs event-driven. In the example here, I'm going to use the Python operator to define each of the three tasks. First, you need to import the class Python operator, like you see here. Then you'll go inside the context manager to define the first task, which is the extraction step. To do so, you will create an instance of Python operator, which needs two parameters. 
The first parameter is task underscore ID, which you can use to specify the name of the task. This name will be used to reference the task in the Airflow UI. I've chosen to use extract as the ID for the first task. And the second parameter is Python underscore callable, which expects a Python function that contains what needs to be done in the extract step. You can define this function here in the same file, or you could define it in another file and import it into the code here. In a moment, you'll define a function called extract underscore data, but for now, let's assume that this function exists so that we can set it as a second parameter, Python underscore callable. Then we can repeat the same steps to define the second and third tasks, assuming that the two Python functions transform underscore data and load underscore data already exist. 
Now we need to go back and create these Python functions in this file. You can define these functions before the DAG definition is shown here, or as you will see in the next lab, you could place each function definition next to the corresponding task within your DAG. Note that to keep things simple in this first DAG example, the task just contains a simple print statement. So now you have the DAG and the tasks defined. You just need to specify the dependencies between the tasks or the order in which the tasks need to be executed. Here's how you can define the dependencies using the bit shift operator. This statement means that task one should be executed and completed before tasks two starts, and task two should be completed before task three starts. 
Note how in this statement, I use the variables that represent the task instances. And that's it. You now know how to build a simple DAG in Airflow. In the lab, you'll get a chance to build another DAG on your own. And once you're done, you'll be guided to upload your Python script to the Airflow S3 toolkit, which acts as a DAG directory, and then open the Airflow UI to check and monitor this DAG. After that, I'll see you in the next video to look at the Airflow concepts you'll encounter in the second lab, XCOM and variables. 


#### Additional Notes About Airflow Basic Concepts
This reading item contains additional notes about scheduling in Airflow,  using Airflow Operators and defining dependencies between tasks.

Scheduling Your DAG & Other DAG Parameters
When instantiating the DAG in the previous video, I specified the following parameters: dag_id, tags, description, schedule, start_date and catchup. You can also specify other DAG parameters. Check out the 
 Airflow documentation
 to learn more.

In this reading item, we'll take a closer look at what the start_date parameter does. When you orchestrate your pipeline in Airflow, you may encounter the terms "data interval " and "logical date"  in the Airflow UI or in the Airflow documentation. Each DAG run is associated with a data interval that represents the time range it operates in. Let’s say you instantiated a DAG to run daily using the cron preset `@daily` and the start date is March 1. As shown in the following figure, each DAG run operates in a data interval that starts each day at midnight (00:00) and ends at midnight (24:00).


The “logical date” is a term associated with a specific DAG run, and it denotes the start of the data interval.


The start_date argument for the DAG marks the "logical date" or the start of the first "data interval". 

Given a data interval, the DAG is executed at the end of the data interval, not the beginning. This is because Airflow was developed as a solution for ETL needs, where you typically need to aggregate data collected over a time interval. So if you want to analyze the data for March 1, you would need to wait till March 2 midnight after all data for March 1 becomes available. This is why a DAG is always executed at the end of the data interval, and the logical date of a DAG run (start of the data interval) represents the date for which the DAG run is executed, not when the DAG is actually executed. So the first DAG run will only be scheduled one interval after start_date.

Check out these links if you want to learn more about scheduling in Airflow:

Data-interval
 

What does execution date mean?
 

You can customize your DAG scheduling using  
timetables
. In addition to scheduling DAGs, you can make your DAG data-aware, meaning that it is triggered when a data object is updated in another task. Here's an 
example
 of this.

Airflow Operators
You learned about some of the Airflow operators such as EmptyOperator, PythonOperator, BashOperator, EmailOperator. You can learn more about these operators by checking out the 
Airflow documentation
. In addition to those 
core operators
 provided by Airflow, there’s a 
list of other operators
 that are released independently of the Airflow core that allows you to connect to external systems. For example, 
this link
 shows all the possible operators that you can use to interact with each AWS service, and 
this link 
includes the operators you can use to copy data, for example, from a database to S3. It is generally recommended to use the available operators instead of writing your own code from scratch.

In the previous video, the two parameters that were specified in the PythonOperator were `task_id` and `python_callable`. You can always review the 
Airflow documentation
 to see what other parameters you can specify for PythonOperator. 
Here
 is another set of parameters that you can pass to any operators, it includes the following parameters:

email (str or list[str]): the ‘to’ email address(es) used in email alerts. 

email_on_retry (bool): indicates whether email alerts should be sent when a task is retried

email_on_failure (bool): indicates whether email alerts should be sent when a task failed

retries (int): the number of retries that should be performed before failing the task

Check out this 
link
 if you'd like to learn more about Airflow sensors, another special kind of operator.

Defining Dependencies
You learned that you can use the bit-shift operator (>>) to specify the dependencies between tasks. Here are some examples:


Additional References
An introduction to the Airflow UI

Airflow UI - screenshots


#### Airflow - XCom and Variables
In the first lab, you got some experience building DAGs, interacting with the Airflow UI, and troubleshooting issues in your DAG. Now you're ready to explore additional airflow features and look at some best practices for implementing orchestration with airflow. In this video, you'll learn how to pass data from one task to another using airflow XCom, and how to create global variables in the airflow UI. Let's get started. Here's a DAG implemented in the previous lab. In the task get_random_book, you requested the data of a randomly chosen book from a book API and then store that data in an S3 bucket to be used in subsequent tasks. You essentially passed data from one task to another using an intermediate storage, in this case, the S3 bucket. 
This method is appropriate when you want to pass large datasets between tasks. But for small amounts of data, there's another method called XCom that you can use. XCom, which is short for cross-communication is a key airflow feature for sharing data among tasks. It's designed to pass information like metadata, dates, single value metrics, or simple computations between tasks. In a given task, if there's a value that you'd like to use in another task, you can sort in an XCom variable by calling the xcom_push method. The XCom variable is then pushed to a metadata database. Each XCom contains the following information. 
A key, which is the name of the XCom variable, the stored value, the timestamp at which the variable was created, as well as a DAG ID and the task ID from which the XCom variable originated. To extract the value stored in an XCom variable in any given task, you can call the method xcom_pull. Let's go over an example. The DAG you see here consists of two tasks. The extract metric task connects to an API, sends a request for a certain data and then computes a metric based on the return data. The second task prints the metric computed by the first task. Since you need to pass some data between the first and second task, you can use the XCom feature here. 
The first task uses the function extract_from_api. This is where you need to call xcom_push. The second task uses the function print_data. This is where you need to call xcom_pull. Let's take a closer look at each function. This is the code for the extract_from_api function. Here I called the REST API of a job site to get the latest 40 remote job postings for data engineering in the US. 
Then I computed the ratio of those jobs and asked for a senior data engineer. Now, you want to pass this value to the second task. You need to first store the obtained value in an XCom variable by calling the method xcom_push. This method expects two parameters, the key of the variable and the computed value. XCom_push is a method associated with a task instance. This means that you need a task instance here on the left in order to call the method. By task instance, I mean the object that represents the currently running task. 
Airflow has a set of built-in variables that contain information about the task that is currently running, including the task instance. This information is stored in a dictionary called airflow context that you need to pass to the extract_from_api function as an argument, like you see here. To get the task instance object from a context dictionary, you can use context and in brackets, you'll see TI or TI stands for task instance. Then you can call xcom_push. On the tasking so the object you got. Then to access a computed ratio of the XCom variable in the second task, you need to call xcom_pull like this and pass in the key of the variable in the task ID where the XCom variable was created. Similar to what you did in the extract_from_api function. 
We also need a task instance in order to call xcom_pull. Let's pass the context dictionary, as an argument to the print_data function, and then use context and then brackets TI again to get the task instance, and finally, call the method xcom_pull on this object. When you run your DAG, you can check your XComs in the Airflow UI by clicking on Admin and then navigating to XComs. Here's the XCom variable that we just created along with this corresponding value. I want to offer you a word of caution about using XComs. They are not designed to pass large datasets like DataFrames between tasks as they degrade the performance of your DAGs and the metadata database. If you need to share large datasets between tasks, you should follow what you did in the previous lab and use an intermediate storage like S3. 
Now let's discuss another airflow feature using the same DAG as an example. If you examine the API requests of the first task, you see that the value of certain parameters, such as count and geo are hard-coded, which means they are directly included in the code. But what if you don't want those values to be fixed because you may need to change them in future DAG runs, or maybe you want to experiment with more than one value? You can update the values in the code, but this approach might be error prone and not the most efficient, especially if the values are repeated many times in your code. Instead of including hard-coded values within your DAG or task definitions, you can create global variables in the Airflow UI or create environmental variables in your development environment and use these variables inside your code. Let's create two variables in the Airflow UI, one for the number of posts and another for location. If you click on the Admin tab and then select the Variables option, you'll see your list of variables. 
Prior to a new variable, you can click on this plus sign and then specify the key and value of the variable. For the locations variable, I specify the list of countries. I'm interested in gathering job posts as a JSON objects. For the number of post variable, I'll assign it the value of 20. To use those variables inside your code, you need to import the variable module, and then inside your code, you use the variable.get method to retrieve the total number of posts and the list of locations. Here you set the parameter deserialize_json=True if you want this method to return the JSON object as a dictionary instead of a string. In the next lab, you'll practice creating variables in the Airflow UI, as well as using airflow built-in variables. 
You'll also use XCom variables to pass data between tasks. Before you go to the next lab, make sure you check out the reading materials after this video that lists some of the best practices for writing DAGs to ensure that your code is efficient, readable, and reproducible. 


#### Best Practices for Writing Airflow DAGs
When writing DAGs, there are some best practices that help ensure your code is efficient, readable, idempotent and reproducible (like with any code). Let’s go through some of these basic practices.

Best practices

Explanation/Example of a bad code

Keep tasks simple and atomic

When you prepare your pipeline for orchestration, you need to identify the tasks or steps of your pipeline. Keep your tasks simple such that each task represents one operation. You don’t want to end up with one task that does everything, otherwise you'll lose visibility into your data pipeline and reduce the readability of your code, which does not support idempotency.  

For example, in an ETL or ELT process, you would need to create at least three tasks: extract, transform, load, instead of creating just one task that handles the entire process.

Avoid top-level code

In the following code,

call_some_function()

perform_computation()

with DAG(dag_id="example_xcom", start_date=datetime(2024, 3, 13), schedule='@daily',catchup=False):

        task_1 = PythonOperator(task_id='extract',python_callable=extract_api)

        task_2 = PythonOperator(task_id='load_data',python_callable=load)

        task_1 >> task_2

call_some_function() and perform_computation() are both high-level codes. In general any code that isn’t part of your DAG or operator instantiations is considered to be top-level code. This type of code will be executed at the time when the DAG is parsed by the scheduler. On the other hand, any code that is part of an operator is executed when the task runs, not when the DAG is parsed. Top-level code can cause performance issues because the scheduler checks the DAG directory and parses the DAG files every 30 seconds. So it may not be efficient to execute the high-level code this frequently especially if the code makes some requests to an API or a database.

Use variables (user-created variables, Airflow 
built-in variables and macros
)

User-created variables: Including hard-coded values directly in your code is generally not a good practice in software development. This is because they make your code less readable and more error-prone -- you may need to use the same value in multiple places and updating the same value in multiple places can be error-prone. The same principle also applies to when you write code to define your pipelines. Instead of including hard-coded values within your DAG or task definitions, you can store these values by creating variables in the Airflow UI or creating environmental variables and use these variables dynamically inside your code.

Recommendations from Airflow documentation regarding using Variables
: “Variables are global, and should only be used for overall configuration that covers the entire installation; to pass data from one Task/Operator to another, you should use XComs instead. We also recommend that you try to keep most of your settings and configuration in your DAG files, so it can be versioned using source control; Variables are really only for values that are truly runtime-dependent."

Airflow built-in Variables: You learned that Airflow has a set of built-in variables that contain information about the currently running DAG and its tasks, such as the logical date of the DAG run and task instance (for a list of such variables, check 
here
). You learned that you can access these variables within a task function by passing the context dictionary as an argument to the function. You can also pass these variables directly to the PythonOperator using a syntax known as Jinja templating, which looks like this : “{{ds}}”. You use double curly brackets and inside the brackets you specify the variable you’d like to access. In this example, ds represents the logical date of the DAG run. 

Let’s see an example: Assume that your python_callable is a function that expects the name of a file. For example, this function loads some data to an s3 bucket and requires that you pass the file name.  And let’s say you want to include the logical date in the file_name.

def load_to_s3(file_name):    

    #code that loads data    

    print(file_name)


So you can specify this information in the PythonOperator as follows:

task_load_s3 = PythonOperator(task_id="load_to_d3",

         python_callable=load_to_s3,

         op_kwargs={'file_name': "data/created{{ds}}/file.csv"})


The parameter op_kwargs allows you to specify the arguments that you need to pass to the function load_to_s3. Note how the logical date was included in the file name using templating (“{{ds}}”).

Task groups

In the Airflow UI, you can group tasks using Task Groups to organize your DAGs and make them more readable. Inside the task group, you can define tasks and their dependencies using the bit-shift operators <<  and  >>.  You can create a Task Group using the  "with" statement, as shown in the following example.  


from airflow.utils.task_group import TaskGroup  

with DAG(...):     

    start = DummyOperator(...)

    with TaskGroup('task_group')as task_group:

       task_a = PythonOperator(...)

       task_b = PythonOperator(...)

       task_a >> task_b

    end = DummyOperator(...)      

    start >> task_group >> end   

Other practices (Airflow is an orchestrator not an executor)

Heavy processing should be assumed by execution frameworks (e.g. Spark) not Airflow

For large datasets, don’t use XComs (push dataframes). Use intermediary data storage instead.

Including code that is not part of your DAG or operator makes your DAG hard to maintain and read: consider keeping any extra code that is needed for your tasks in a separate file.

Additional References (if you’d like to learn more)

Airflow best practices

Other best practices

Functional-data-engineering-a-modern-paradigm-for-batch-data-processing
 by Maxime Beauchemin


#### Airflow - Taskflow API
I hope you're now starting to feel comfortable with Airflow. In the upcoming lab, you will build a pipeline using Airflow that incorporates data quality tests into your DAG using Great Expectations, and you'll use other Airflow features, such as branching and the TaskFlow API to write your DAG in a more concise way. But before you start the last lab, let me introduce TaskFlow API and show you an example that follows this paradigm. Until now, to define your DAG, you instantiated a DAG object and to create your task instances you use Python operators. This is known as the traditional paradigm. Airflow 2.0 introduced another paradigm known as TaskFlow API. The goal of this new paradigm is not to replace a traditional one, but to make writing DAGs easier and more concise, especially when the DAG uses lots of Python functions. 
This new paradigm relies on the use of decorators that help with the creation of a DAG and its tasks and simplifies the writing of code at the same time. Before I show an example of how to write a DAG using this new paradigm, I'd like to clarify that the API in TaskFlow API is not related to REST API. You can think of TaskFlow API as an interface that provides you with a more user-friendly programming experience. Let's go back to the first DAG example you saw in a previous video tutorial, and we'll read-write it using the TaskFlow API paradigm. To define your DAG, you use the context manager as shown here. With a TaskFlow API, instead of explicitly calling the DAG constructor, you can use the decorator @dag, pass in the DAG parameters to the decorator, and then define the content of your DAG as a Python function directly after the decorator. In this case, the function name will be used as a DAG ID to identify the DAG in the Airflow UI. 
Inside this function, you define the tasks and their dependencies. Once you're done, you need to call the DAG function as shown here. Otherwise, your DAG won't show up in the Airflow UI. The role of the DAG decorator is to implicitly call the DAG constructor to create the DAG instance. Note that to use a DAG decorator, you need to import it as shown here. So far, you may not have notice a big difference between these two paradigms when it comes to defining the DAG. But let's see how creating the task is different with a TaskFlow API. 
In the traditional paradigm, you use the Python operator to create your tasks shown here. In doing so, you needed to keep track of the task ID, the name of the Python function, and the name of the variable that represents the task. With a TaskFlow API, you'll keep track of fewer names. Instead of explicitly calling the Python operator, you use the @task decorator to define your tasks. Here inside your DAG function, you can need the @task decorator to create the first task, which is the extraction step. Then you define the extract data function directly after the decorator. Similar to before, the function name will be used as the task ID to identify the task in the Airflow UI. 
The job of the decorator is to implicitly call the Python operator, which simplifies your code. To define the remaining task, you just repeat the same steps, similar to the DAG decorator. To use a task decorator, you need to import it as shown here. Finally, to define the dependencies between the tasks, you'll still use the bit-shift operator. But this time, you will call the functions that represent each task as follows. This is how you can use TaskFlow API to define a DAG. This is equivalent to how you defined your DAG previously, using the traditional paradigm, in terms of arriving at the same result. 
But TaskFlow API makes your code simpler. Let's take a look at one more example to see how you can use XCom with TaskFlow API. In the traditional approach, you call xcom_push to store the data you want to pass to other tasks. Then you call xcom_pull in the function for the task that uses the data. With TaskFlow API, you can simply include a return statement, as shown here in this extract from API function, to store the data you want to share in an XCom variable. Then for the task that uses the data, you can specify that the function expects an input representing the data shared by a previous task. Finally, when you want to define the dependencies between two tasks, you call the function of the first task, and assign the value it returns to a variable. 
Then you can pass this variable to a second task as shown here, or you can combine both statements into one statement like this. You can still use xcom_pull and xcom_push with TaskFlow. For example, in the second task, you can pass in the Airflow context to the function and use it to explicitly call xcom_pull inside the function. Then to define the dependencies, you can use your regular operator as shown here. Note that the decorator does not replace all operators. This is why you may still need to use both paradigms and maybe combine both of them in the same code, depending on your use case. Before you jump into the last lab, make sure you check out the reading material on branching that's provided after this video. 
It will prepare you for creating a dynamic pipeline in the lab. Then after you finish the lab, Morgan will walk you through some options you have for orchestrating data engineering tasks on AWS. 



#### Orchestration on AWS
Now that you've gotten some practice using Airflow to orchestrate your data pipelines, those skills are going to help you with your orchestration work going forward, whether you're using Airflow or another orchestration tool. In this video, I'd like to briefly introduce you to some of the other options you have when orchestrating data engineering tasks on AWS. First off, with Airflow itself, there are multiple ways you can host and run your workflows on AWS. The most hands-on way is that you can run the open-source version of Airflow on an Amazon EC2 instance or in a container. In the labs this week, you were using open-source Airflow in a set of containers that were running on an EC2 instance. By using this approach, you have full control over the configuration and scaling of your Airflow environment. But this also comes with more responsibility as you need to manage all of the underlying infrastructure and integrations yourself. 
As I've said before, when it comes to choosing tools and services to build your data pipelines, it often comes down to evaluating the trade-offs between convenience versus control and the requirements for your particular use case. If, instead of control, you were aiming to optimize for convenience when running Airflow on AWS, you could choose to use Amazon Managed Workflows for Apache Airflow, or MWAA, which is a managed service that runs Airflow for you and handles tasks like the provisioning and scaling of the underlying infrastructure. When you use MWAA, it sets up an Apache Airflow environment for you using the same Airflow user interface and open-source code that you can download from the Internet. This is a diagram showing how MWAA is architected. The scheduler and worker components that you're now familiar with are hosted in containers on AWS Fargate, and they can access an Airflow metadata database, which is hosted on Amazon Aurora, to store the status of tasks and the state of the DAGs. MWAA also integrates with other AWS services. For example, MWAA post Airflow logs and metrics to Amazon CloudWatch, uses Amazon S3 as the DAG directory where you place your Python scripts, and it uses AWS Key Management Service to encrypt data at rest. 
Apart from MWAA, there are other orchestration services on AWS that support similar functionality. AWS Glue workflows is similar to Airflow in that it allows you to create, run, and monitor complex ETL workflows, where multiple Glue jobs and crawlers may be dependent on each other. A Glue workflow contains jobs, crawlers, and triggers. You would build out these components before building your workflow, and then you can use the AWS Glue console to visually build out and see the structure of your workflow graph. You can then run these Glue workflows based on triggers, which could be on a schedule, on-demand, or an event coming from Amazon EventBridge. Another option for orchestration on AWS is AWS Step Functions. Step Functions allow you to orchestrate multiple AWS services and states into workflows called state machines. 
The states in a state machine can do various things. States can be tasks which do work like a Lambda function that processes data, a Glue job that transforms a dataset, or an ECS task that runs a containerized application, just to name a few examples. States can also make decisions based on their input, perform actions from those inputs, and then pass output to other states. Like Airflow, Step Functions are designed to coordinate tasks and manage dependencies between them, allowing you to define complex workflows involving multiple steps or states. When it comes to choosing between Airflow, either managed or open-source, versus Step Functions or Glue workflows or some other orchestration tool entirely, just like anything else, it's going to come down to your requirements and what you're trying to optimize for. Airflow with its Python-based DAGs and plug-in ecosystem offers a lot of flexibility and is good for complex workflows. Step functions provide a serverless option with extensive native AWS service integration, which can be ideal for AWS-centric workflows. 
AWS Glue workflows are specifically designed for ETL processes, allowing you to orchestrate Glue jobs, crawlers, and triggers in a serverless environment. As Joe mentioned earlier, the landscape of orchestration tools is evolving rapidly. So your best bet, in addition to getting practice with popular tools like Airflow, is to stay up to date on alternative and emerging tools so you can make the best choices in your own data architectures going forward. Now I'll pass back to Joe to wrap up this week. 
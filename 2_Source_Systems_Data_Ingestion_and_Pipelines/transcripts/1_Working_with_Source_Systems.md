## Working with Source Systems


en
​
Interactive Transcript - Enable basic transcript mode by pressing the escape key
You may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key
Welcome to the second course of the data engineering specialization. In the first course, you've got a high-level overview of the field of data engineering, the principles of good data architecture, and how to translate stakeholder needs into requirements and tooling for your data systems. In this course, you learn more about ingesting data from source systems, as well as DataOps and the orchestration of end-to-end data pipeline. I'm here again with your instructor, Joe Reis, who is also co-author of the best-selling Fundamentals of Data Engineering. Joe, can we see a little bit about what learners will see in this course? Sure thing, Andrew. This course, as you said, includes a focus on the first two stages of the data engineering life cycle, which are data generation and source systems and data ingestion from those source systems. 
We'll start by taking a look at different types of source systems, maybe things like databases or object storage and streaming systems. We'll get into the details of how you'll interact with the source systems in your work as a data engineer. After that, we'll look at ingesting data from source systems, as well as the DataOps and orchestration aspects of building data pipelines. That sounds great. For so many AI systems, the data engineering or the data ingestion is like 80% of the work, and then the machine learning modeling is maybe 20% of the work. But people's attention on these two topics is often flip where 80% of the attention is on AI modeling and not enough attention, and best practices, and, too, frankly, is on the data ingestion. In fact, for some of my earlier work, when I was at a very large tech company, I was responsible for the company's user data warehouse, and so every piece of data that touched an individual user was supposed to come into my data warehouse, and that created a lot of value for the company. 
But that intellectual work to design the database, the database schema, the ingestion system, the data to maintain it, that turned out to be pretty massive undertaking. It is a massive undertaking. I've always found that when we're talking about ingesting data from source systems, this is everything. If you can't get the data, there's not much else you can do with it. That should seem simple enough, but as you point out, it feels like, a lot of times, ingestion is ignored, or focus on other things. So this is a fundamental thing you need to get correct. If you can't get the data, you can't really do anything else. 
In this part of the course, we'll definitely be talking about, understanding the source systems from which you're going to get your data, different ways of ingesting it. As well as different ways of orchestrating these data pipeline workflows and monitoring them to make sure that you're preserving data quality, as well as ingestion, performance, and other characteristics, etc., very important to your work as a data engineer. In fact, I find this to hold true for many different data workflows, from the structure table data to, as the word processes more on structured data, like texts and images. This seems to have stayed the same. In fact, even when I chat to my friends training the large language models and comes to leading AI teams, a lot of the time, not all, but a lot of the time is actually spent thinking about the data. There's some also on the model training, but the data ingestion occupies so much time for all of these AI workloads. It's really interesting. I guess what are they seeing in terms of the complexities with ingesting data because I imagine that working at a very massive scale. 
There is actually a lot of Internet data. Things like Common Crawl has a lot of data. But given the data, how do you ingest it, process it, screen it to be high quality. Then also, if you have gaps in the data, if you notice your model does want on these topics, how do you figure out where it doesn't do quite as well on, and where on Earth do you go to get data if it even exists to fill in those gaps. So these are a lot of the intellectually challenging things that find people on the ground training, even some of the largest OMs, MMs, large multi-modal models, and other large foundation models, spend time worrying about. That's fascinating. One of the things we're talking about in this course that you'll be learning about is ingesting data from multiple types of source systems. 
Obviously, we're going to talk about ingesting tabular data, but that's such a small subset of the total data universe these days. When we're talking about unstructured data sets from text all the way to images and video, this is increasingly becoming a much bigger universe. I would say we traditionally have used or thought about in the data world. One of the things we'll be covering in this course, again, is not just structured datasets from databases, but we're also going to be getting into working with text, image data, and so forth. So that'll help you prepare as a data engineer, not just for the workloads up today, maybe if you're working in data warehouse, but for the workloads of the future. Most of the value of data has probably been structured data up to this point. But as our ability to process unstructured data grows, we'll see if that shifts. 
Maybe it's already shifting. The volume of unstructured data in the world is much greater than the volume of structured data in the world. I think this would be even more challenges and even more jobs for data engineers. Exactly. Hopefully, this is going to be a very exciting introduction to the data engineering life cycle. Again, ingestion from source systems, orchestrating these workloads, and monitoring these workloads is so critical to your work as a data engineer. Again, as Andrew pointed out, it's only going to get more interesting, more exciting, and much bigger. 
Lots of exciting things. Let's go on to the next video to dive into all of these topics. 

#### Overview
Welcome to week one of this course on source systems, ingestion, and data pipelines. This week, we're going to get started by looking at different types of source systems and how you can interact with these systems. As you saw in the first course of the specialization, data generation in source systems is the first stage of the data engineering lifecycle. And as a data engineer, you are typically not responsible for generating this data yourself or maintaining these source systems. But ingestion from source systems is where all your data pipelines will begin. So it's important that you understand how this data is generated, where and how it's stored. And some of its characteristics so that you can build robust data pipelines with these upstream systems as your data source. 
And so in this first week of this course, we'll dive into some of the details of some common source systems, including the different types of databases, object storage, and streaming sources. In the labs, you'll get to work with these source systems on AWS. Then in the second week, we'll focus on setting up different types of ingestion from source systems. After that, in week 3 of this course, we'll take a look at the DataOps under current. You'll use infrastructure as code to automate some of your pipeline tasks, and you'll use various tools to monitor data quality. And then finally, in the fourth and final week course, we'll get into orchestration to coordinate the tasks in your data pipelines. You'll be setting up directed acyclic graphs, or DAGs, using airflow, working with infrastructure as code frameworks, and implementing monitoring solutions for your data pipelines. 
So we're going to cover a lot of ground in this course. Join me in the next video to get started with a closer look at the different types of source systems. 


#### Different Types of Source Systems

en
​
Interactive Transcript - Enable basic transcript mode by pressing the escape key
You may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key
The specific source systems you'll work with as a data engineer will often vary depending on what kind of data you're ingesting from those systems. The most common type of data you'll work with is structured data, which is to say, data organized as tables of rows and columns. Chances are you've worked with structured data in the past, whether that was in a spreadsheet or a relational database, or maybe even using Python to read a CSV file. The other types of data you'll encounter as a data engineer are semi-structured and unstructured data. Semi-structured data is data that is not in tabular form, so it's not made up of rows and columns, but it still has some structure. A common semi-structured data format you'll run into is what's known as JavaScript object notation, or JSON. A JSON file contains a series of key-value pairs. 
In this example, the first key is firstName with the corresponding value Joe. The next key is lastName with the corresponding value Reis. And with JSON format, all these key-value pairs are listed inside of curly braces like this. And I can have more key-value pairs, including whatever information I want to record in this format. Each value can take on a different data type, so for example, a number, string, or an array. In fact, the value in a key-value pair can even be another series of key-value pairs. And as you can see in this example, the value for address has a key city, postal code, and country with the corresponding values. 
This creates what is called a nested JSON format. So, as you can see, even though the data isn't presented as a table, there's still some structure to this data. Unstructured data, on the other hand, has no predefined structure. For example, text, video, audio, and images are all examples of unstructured data. But you'll note that things like video, audio, and images do have an inherent structure behind the scenes, in the sense that there are dimensions of pixels as well as colors such as red, blue, and green. We'll dive more into unstructured data throughout this course. When it comes to ingesting these different types of data, I'd like to classify the relevant source systems you could encounter into three general types, databases, files, and streaming systems. 
While these three types of source systems don't necessarily correspond one-to-one with the three types of data I just mentioned, you could say that from databases you'll most often be ingesting structured and semi-structured data. From streaming systems, you will often be ingesting semi-structured messages as a data format. And files, well, files could be anything from text to image, audio, video, or even regular old rows and columns of tabular data. Let's start by taking a closer look at databases. Databases store information in an organized way that allows you to find, retrieve, update, and delete data. The way this works is through a transactional pattern known as CRUD, which stands for create, read, update, and delete. The C comes first because, of course, data has to be created before it can be read, updated, or deleted. 
Makes sense, right? There's typically a software interface called the database management system, or DBMS, that sits between the physical database storage and the person or application interacting with the database. The DBMS is what allows you to access and manipulate the data stored in the database. There are two types of databases that we'll look at this week. Relational databases, that store information in tables with rows and columns, and non-relational, also known as NoSQL or not only SQL, databases that store non-tabular data. We'll look more closely at both of these types of databases later this week. Apart from databases, the next most common type of source system you'll interact with is files. 
You no doubt already have lots of experience working with files of various types. These might be documents you store on your computer, or images, or videos you record with your phone, or maybe even a CSV file you receive in an email from a coworker. It might seem strange to think of regular old files as a source system for data engineering, but at its core, a file is just a sequence of bytes that represent information. Applications of all types write data to files, and so files are a universal medium of data exchange. And believe it or not, they're one of the most common source systems you'll work with as a data engineer. Files, just like data, can be structured like a spreadsheet, semi-structured like a JSON or XML file, or unstructured like a text, image, video, or audio file. You might be receiving or accessing these files from a file system like Google Drive or an object storage system like Amazon S3, or simply as an attachment to an email. 
The third type of source system you're likely to be ingesting data from is streaming systems. And you can think of streaming systems as providing a continuous flow of data, recorded as messages that contain information about events. And those events include something that happened in the world or a change to the state of a system. In practice, you might be interacting with a stream of events via message queues or other streaming platforms. For example, an IoT device like a smart thermostat might record an event that contains the latest temperature reading and publish that event as a message to a streaming platform like Kinesis or Kafka. Then, as a data engineer, you could set up another service to ingest this message and send an update to an embedded analytics dashboard. In this case, you can think of the streaming platform as a source system from which you are extracting raw data. 
In later weeks of this course, you will see how these streaming systems can also cut across the data engineering lifecycle and be used in the ingestion and transformation stages to process data for various downstream use cases. In fact, you can see the same of all types of source systems, whether you're talking about databases, files or streaming systems. These could be systems from which you're ingesting raw data, or they could be systems you build into your data pipelines at another stage of the lifecycle. So, to recap, as a data engineer, you will extract raw data from different source systems. This raw data may be structured, semi-structured or unstructured, and the source systems may be databases, files or streaming systems. In the next series of videos, I will go into a bit more detail on the characteristics of each of these different types of source systems. We'll start with databases, then we'll look at how object storage works as a data source for files, and after that we'll dive into message queues, logs and streaming platforms in more detail. 
Join me in the next video to get started with a look at relational databases. 

#### Relational Databases
As a data engineer, the most common type of source system you'll interact with is a relational database, and this is because relational databases are everywhere. Many web and mobile applications use relational databases on the backend, and you will find them as well. In many corporate systems like customer relationship management, human resource, and enterprise resource planning systems, they are also commonly used for what are called online transaction processing or OLTP systems where you need to execute a high volume of transactions concurrently, like for banking or online bookings. The name relational database comes from the fact that this type of database is most often used to store data across different tables that are related to one another through a set of keys or common attributes. These tables are typically organized based on how information is structured in the business. So as a data engineer working at an e commerce company, for example, you might be working with a relational database where one table captures customer information, another table captures product information, and a third table captures order information. Structuring a database in this way reduces redundancy and makes the data easier to manage by not having a the same piece of information duplicated across multiple rows or tables in the database. 
To get a sense of what I mean by that, imagine for a moment that instead of multiple tables, you created one big table to store your data from each individual customer order. In this case, your table might contain a large number of columns, including everything about the customer, like their name, address, phone number, and so on. And then you would have all the information about the product they purchased, like the brand, SKU number, product description and other details, as well as the details of the order, things like the date and the time and the purchase amount and how much they paid. So in this scenario, if one customer placed an order for three different products, this would be recorded as three separate rows in your database and you would then have three rows in the table containing all the same customer data. Or if three different customers purchased the same product, then you would have three rows in the database containing identical information for that product. So in short, information would be duplicated across multiple rows in your table, and beyond that there may be inconsistencies with the data across rows. For example, if a customer changed their address, you would then have an inconsistency across rows unless you go back and update all the rows containing their previous address, or if the details of a particular product changed, you would need to go back and update all the rows containing the old information. 
When you instead separate the information for customers, products, and orders across multiple tables, then a row in the customers table represents a single customer and a row in the products table contains information about a single product. If a customer changes their address or the details of a product changes, then you just need to update the single row containing information about that customer or product. The way in which a database is organized into related tables like this is called the database schema. Relational databases represent these relationships across tables through the use of keys. A primary key is a special column or a collection of columns that uniquely identify each row in a table. For the customer's table, the primary key could be this column called Id. The relationship between the customer and the order could be established by having the customer_Id column and the order table reference the id column in the customers table. 
In this case, the customer id column in the orders table is called the foreign key and references the primary key or the id column of the customers table. Beyond the row structure in a relational database, each column has a unique name and a specified data type. For example, in the customers table you might have columns like id, first name and last name that contain strings, and another column called age that contains an integer. Each new row in a table then has to follow the same column structure, meaning the same sequence of columns and data types. This is part of the database schema as well. Now, in a database with schema like this one, to record information about a new order from an existing customer, you could create a new record in the orders table and indicate the customer's id from the customer's table and the product id from the products table and the details of the order, like date, time, and payment, and so on. And again, if that customer changes their address or the SKU number of the product they ordered changes, those changes would only affect a single row in the customers or product table, and the information would stay consistent. 
As you can imagine, there are many different ways you could potentially establish relationships between tables, and this is where the concept of data normalization comes into play. Data normalization is an approach that was developed in the 1970s to minimize redundancy and ensure data integrity by storing data across tables in a logical way. But now I think it's worth pausing for a minute to ask, why worry so much about redundancy or duplicated information in the first place? Like it might seem logical and orderly to structure your data like I've been describing. But is there any downside? Well, it turns out while a normalized relational database structure provides a high degree of integrity and minimize redundancy, it can actually be slow when it comes to querying the data. Nowadays, storage is relatively cheap and speed is often of the essence. 
Data integrity is so critical, of course, but the answer to exactly how you just store your tabular data could depend on what you're trying to optimize for. As a data engineer, you might be ingesting normalized data from a relational database system, but depending on the end use case you're serving, you might decide to organize the data according to a different model in your own storage systems. Today, there are even some use cases where data engineers are electing to take a so called one big table or OBT approach, where all the data is recorded in a single table for faster processing than will be possible if joining multiple tables in a traditional relational database. We'll get more into the details of data modeling in Course 4, specialization. When it comes to interacting with the database, you'll use a relational database management system, or RDBMS. That's a software layer that sits on top of a relational database. There are many popular RDBMs out there, including MySQL, PostgreSQL, Oracle, and SQL Server. 
Most RDBMS support the structured query language, also known as sequel or SQL for short. Some people say SQL, some say SQL. You can go with whatever you prefer. I sometimes say both myself. The important thing to know is that SQL provides a set of commands for performing various operations on the relational databases. And as a data engineer, SQL will be part of your everyday work. In the next video, I'll walk you through some of the SQL commands that you'll need for the lab. 
Then in the lab, you'll get a chance to practice querying data in a relational database using SQL queries. After that, join me in the next video, take a look at NoSQL databases. 


#### SQL Queries
In the lab, you'll work with a transactional database for a fictitious DVD rental company called Rentio. This database includes tables that contain information about Rentio's stores, staff, customers, DVD inventory, and rental transactions. You'll write SQL statements or queries in a Jupyter Notebook to retrieve information from this database in order to answer business questions. To query the data, you'll have to understand the database schema. In other words, you'll have to know the names of the tables, the columns they contain, and how the tables relate to one another through primary and foreign keys. This database is normalized, meaning that data, like addresses for the stores, staff, and customers are stored in separate tables to reduce redundancy and make it easier to update the data when it changes. Similarly, data about the films and rental transactions themselves are also stored in separate tables. 
You can refer to this entity relationship model that shows the relationships and attributes of the tables in Rentio's database. To walk you through the basics of SQL, I'll only be focusing on three tables. The film table that contains information such as the title and length for a list of films, the category table that contains a list of film categories, and the film_category table that shows the film_id's, along with their corresponding film category_id's. The most basic SQL statement starts with a SELECT clause, where you specify what data you want, followed by a FROM clause, where you specify which table you want to retrieve this data from. For example, let's say I want to explore the titles and release years from the film table. I can write SELECT title, release_year FROM film. Once I run this query, I'll get a list of all the titles and release years. 
It turns out that there are 1,000 films in this film table. If I don't want to see the whole list, I can limit the number of returned results using a LIMIT clause. For example, if I add LIMIT 10 to the end of this query, I'll only get the first 10 titles and release years from the film table. But what if you want to retrieve data from all the columns in a table? Well, you can list all the column names in the SELECT clause like this, or you can use a shortcut and type SELECT * FROM film. This will retrieve data from all the rows and columns from the film table. You'll learn how quarries are executed behind the scenes in the next course. 
But for now, just know that it can take a lot of processing resources to retrieve all data from all the columns, especially if your dataset is very large. I recommend only using SELECT * to retrieve all the data in a table, where you can filter the results with some Boolean condition. For example, let's say you're only interested in exploring films that are less than 60 minutes long. You can add a WHERE clause after the FROM clause to filter the results based on the length column. I'll write SELECT * FROM film WHERE length is less than 60. This will return a list of the 96 films that are less than one-hour long. You can also order the results by any column you want. 
For example, I can add ORDER BY length after the WHERE clause to get the results stored in ascending order by the film length. If I wanted the results in descending order by film length, I can add the D-E-S-C, or DESC, keyword to the end of the ORDER BY clause. If I wanted to limit the results to, say, 10 records, I can add a LIMIT 10 to the end of this query. You just saw how you can retrieve data from a single table using the SELECT, FROM, WHERE, ORDER BY, and LIMIT clauses. What if you want to explore data from more than one table? You can use the JOIN clause to combine records from two or more tables based on the shared columns between those tables. For example, let's say I want to get a list of film titles and their corresponding film_category for all films that are under 60 minutes long. 
Let's modify the query from earlier, where I selected star FROM film WHERE length is less than 60. I want to combine the rows from the film table with the rows from the film_category table based on matching film_id's. At the end of the FROM clause, I'll write JOIN film_category ON film.film_id = film_category.film_id. This way, the returned results will include all the columns from the film table, along with the columns from the film_category table for each matching pair of film_id's in both tables. Notice that the film_category table only includes the category_id, but not the category name. So we need to do another join to combine these records with rows in the category table based on the category_id's. I'll add JOIN category ON film_category.category _id = category.category_id. 
Now the results will include all the columns from the film table, all the columns from the film_category table, and all columns from the category table. Since I only want the film title and the corresponding film_category, I can modify the SELECT statement to specify that I only want the film.title column in the category.name column. Note that by default, the JOIN clause combines only the records from both tables that have a matching column value specified in the ON statement. It will not include any records from either table that don't have matching values. For example, if the film table has a row with film_id that doesn't appear in the film_category table, then that row will not be included in the results. This type of join is also known as the INNER JOIN, and you can think of the join results as the middle overlapping part of a Venn diagram. The other types of joins include the LEFT JOIN, which returns all records from the first table, along with any matching records from the second table, the RIGHT JOIN, which returns all the records from the second table, along with any matching records from the first table, and the FULL JOIN, which returns all the records from both tables and combines the ones with matching values. 
Going back to the results from the last query, I can see that quite a few of the shorter films belong to the children or documentary category. Let's say I want to know for sure which is the most popular category for short films. I can use the GROUP BY command to group the rows based on the film_category. Then use the COUNT command to count the number of records for each of the film categories. The GROUP BY command is written after the WHERE clause. Here I'll add GROUP BY category.name. Then I'll modify the SELECT statement to select category.name and COUNT(*), which counts all the rows for each category. 
I will also use the AS command to give the output of this count the film_count alias name. Finally, I'll order the results by the film count in descending order. As you can see, the most popular category for short films under an hour is documentary, followed by action, then children. Those are some of the most common SQL commands. Now, you're ready to get started with the lab. The lab also covers some data manipulation operations, including CREATE, INSERT INTO, UPDATE, and DELETE. Make sure you read the instructions carefully when trying each of the exercises. 
After you finish the lab, join me to take a look at NoSQL databases. 


#### NoSQL Databases
In the early 2000s, tech giants like Google and Amazon began outgrowing their relational databases. They needed to process large volumes of data from disparate sources that didn't fit neatly into the relational database model. Enforcing tabular structures would lead to data redundancy and performance issues at scale, and so these companies led the way in developing new distributed non-relational databases to scale their web platforms. And so in this way, NoSQL databases were developed to overcome the limitations of relational databases, trading certain RDBMS characteristics like strong consistency, joins, and a fixed schema for more schema flexibility, scalability and improved performance. Before we go any further, let's get one thing straight, NoSQL doesn't stand for No SQ, it means not only SQL. It's a category of databases that break away from the relational framework we saw in the previous video. But some non-relational databases still support SQL or SQL like query languages. 
Let's go over the basics of NoSQL databases. NoSQL databases have non-tabular structures. They can support various data formats including key-value, document wide-column, graph, and others. I will talk about key-value and document source later in this video and you will see some of these other types throughout the next few courses. Unlike relational databases, NoSQL databases don't require predefined schemas, so this means you have more flexibility when deciding how you want to store your data. NoSQL databases excel in horizontal scaling, which means automatically distributing data and workloads across multiple servers to meet increased traffic demands. When a user writes data to a NoSQL database that is distributed across multiple servers or nodes that write operation is first performed on a single node in this distributed system, which is a location where one version of the database is running. 
Then there might be a slight delay before those changes are propagated to all other nodes in the system. Unlike relational databases, NoSQL databases operate under the principle of eventual rather than strong consistency, meaning that the database will allow you to read from a node that has not received the latest write update and you may not get the most up to date data. But given enough time, the database will be consistent and reading data from any node will give you the same data. With a relational database that provides strong consistency, you would not be able to read data until all the nodes in the system have been updated. In this way, eventual consistency allows no single databases to prioritize speed. Which is perfect for applications where system availability and scalability is more important than real time consistency, such as social media platforms or content distribution networks. In terms of data integrity, not all NoSQL databases guarantee the principles of atomicity, consistency, isolation and durability, also known as ACID compliance, which we'll look at in the next video, but some do, for example, MongoDB. 
This means that if you're sourcing data from a NoSQL database, then you may need to take extra steps to ensure data integrity. Finally, NoSQL databases use specialized query languages tailored to their data model, which are often, but not always, different from SQL. Let's take a closer look at two common types of NoSQL databases, key-value databases and document databases. A key-value database stores data as a collection of key-value pairs, similar to what you might find in a JSON file or a Python dictionary structure. The key serves as unique identifier to retrieve the corresponding value. Both the keys and the values can be anything from simple to complex objects. This type of NoSQL database is perfect for scenarios where fast data lookup is needed, such as caching user session data in a web or mobile application. 
For example, when a user logs into an ecommerce application, actions like viewing different products, adding items to the shopping cart, and checking out can all be stored in a key-value database with the user session id as a unique identifier. Document stores are a special type of key-value database that store data in JSON like documents. Each document has a unique key that identifies a document and allows you to retrieve that document's data. Documents are organized into collections, so you can think of a collection sort of like a table in a relational database and a document like a row. In this example, data is stored in a collection called users. Each document represents a single user and the id is the key that uniquely identifies each user. This locality makes it easier to retrieve all the information about a particular user compared to a relational database, where the user information may be spread across multiple tables. 
However, document stores don't support joins, so it's harder and less efficient to combine information from multiple documents as compared to combining information across multiple tables in a relational database. The advantage, however, is this notion of a flexible schema. As you saw with relational databases, all records need to conform to a fixed schema, but with key-value databases and document stores, there's no fixed or predefined structure to data records. Document stores are commonly used for applications involving content management catalogs and sensor readings. Each interaction, product or sensor reading from an IoT device, for example, can be stored as a single document with a flexible schema. But be careful, this flexibility can have a downside. I've seen document databases become absolute nightmares to manage inquiry. 
And if you're ingesting data from a NoSQL document store as your source system, the flexibility of the schema makes it even easier for source system owners to change something that'll break your data pipelines. Both relational databases and NoSQL databases can be used as sort of a wide range of applications. When it comes to applications processing online transactions in areas like banking, finance and e commerce, among others, things are happening fast, money is changing hands, and products are on the move. And these types of online transaction processing or OLTP applications, any errors or inconsistencies in the data can cause major problems. In the next video we're going to have a look at the principles of atomicity, consistency, isolation, and durability, otherwise known as the ACID principles, which are critically important for your data sources and data pipelines when working with OLTP systems. I'll see you there. 


#### Database ACID Compliance
Both relational and non-relational databases can support very high transaction rates. They are commonly used in online transaction processing or OLTP systems. These systems typically need to store rapidly changing application states, such as the details of bank account balances or online orders. Most relational database systems are what's known as acid compliant, which means they support the principles of atomicity, consistency, isolation, and durability, which help ensure transactions are processed reliably and accurately in an OLTP system. By contrast. Many NoSQL databases are not asset compliant by default, but many offer you the ability to configure them to be asset compliant. In this video, I'm going to talk about what each asset principle is so you can get a better sense of when these principles will apply to the work you do as a data engineer. 
But first off, let's think for a minute about what happens if transactions, let's say, in a banking system or not process in a reliable or accurate manner. For example, what if you're trying to transfer money online from one account to another, and the banks website just goes down as you're trying to transfer money. When you eventually get back to your account, you'd be hoping to find that the transaction either went through or it didn't, but not that the money was deducted from one account, but not added to the other. The same thing applies to all systems from banking to online shopping and many more. The first asset principle is atomicity, which ensures that transactions are atomic, or in other words, treated as a single indivisible unit. A transaction might consist of multiple operations. But the atomicity principle ensures that either all of the operations within a transaction are executed successfully, or none of them are. 
For example, imagine the transaction of a customer placing an order for an item. Suppose this transaction involves two operations, deducting the total cost from the customer's account, and updating the inventory to reflect the purchased item. Let's say that the customer experiences a network connection error, right after the total cost was deducted from their account. But before the inventory update was completed. The atomicity principle guarantees that both of these operations happen as a single transaction. If the network connection error prevents the second operation from being completed, the first operation will be rolled back, so the customer is not charged, and the whole transaction fails. The second principle is consistency, which means that any changes to the data made within a transaction must follow the set of rules or constraints defined by the database schema. 
This ensures that the database will transition from one valid state to another. For example, suppose that the inventory database schema prevents any stock level from going below zero. Let's say that the stock level for a particular item is currently one. If a customer tries to place in order for two of those items, then the operation will fail, and the entire transaction will be ruled back to ensure that the database remains consistent with the predefined schema. As I already mentioned, this is the default in relational databases, but would need to be configured in most NoSQL systems. Just a note for clarity, the word consistency ends up getting a little overloaded here. In the previous video, I described the strong consistency property of relational databases, which refers to the idea that all nodes in a distributed system will provide the same up to date data. 
It turns out that strong consistency of a database system is a result of compliance with the acid principles, but a slightly different concept than the consistency represented by the C in ACID. The next principle is isolation, which ensures that when several clients try to execute transactions concurrently, each transaction is executed independently in sequential order. For example, let's say the inventory shows that there are 10 of one item remaining. Suppose that two customers each place in order for five of these items at the exact same time. The isolation principle guarantees that even though the time stamp on these two transactions may be the same, both transactions will happen independently in sequence, so that when the two transactions are completed, inventory level for that item will be zero and not five. Similarly, if one customer orders five and another orders 10 of the item at the same time, whichever order gets processed first, we'll go through, and the second one will fail, resulting in a sock level of either five or zero. The final principle is durability, which guarantees that once a transaction is completed, its effects are permanent and will survive any subsequent system failures, such as a power loss. 
This is essential for maintaining the reliability of the database, even when faced with an unexpected event like a natural disaster. In summary, the ACID principles guarantee that a database will maintain a consistent picture of the world. That might sound logical and relatively even straightforward. But in the real world, a database might be partitioned across multiple servers because of its size or replicated across multiple data centers for redundancy and speed. In these cases, it's especially critical to know that the data you're reading and writing remains consistent across the entire network of servers. This is the principle of what's called strong consistency, which is a key feature of ACID compliance that holds even for a distributed database system. Now, it's important to note that while relational databases are typically ACID compliant, not all databases are required to abide by all of the ACID principles in order to support application back ends. 
Some NoSQL databases only possess some degree of ACID compliance. But relaxing one or more of these constraints, you can improve certain aspects of the databases performance and make it more scalable. As a data engineer, understanding when your database needs to be ACID compliant can help you prevent disasters. In the next lab, you'll work with DynamoDB, a NoSQL key value database. Join me in the next video to take a quick look at the lab before you jump in. 


#### Object Storage
>> As I said earlier this week, files are one of the most common source systems that you will deal with every day as a data engineer. And you might be receiving or accessing these files from a file system like Google Drive or an object storage system like s three, or simply as an attachment to an email. While files may come at you from many different places, object storage is arguably the most important mechanism for file storage and retrieval in your work as a data engineer. Object Storage treats data, files in this case, as individual objects and stores them in a flat structure that doesn't adhere to a traditional file system hierarchy. This means that while you might be accustomed to storing files in a hierarchy of folders and subfolders on your local computer, Object Storage has no hierarchy. Now, just as a side note, this flat structure thing can be confusing. If you go into Amazon S3 for example, there is a Create Folder button, and you can go ahead and create folders and subfolders to your heart's content and happily store your files in what looks very much like a hierarchical file system. 
However, it turns out this is just a feature of the user interface to keep things looking organized in a way that looks familiar. The actual storage mechanism is flat, meaning that even though it might look like you have folders and subfolders in the UI, all all files are actually stored right at the top level. And this is by design because it allows quick and straightforward access to all objects without worrying about the overhead of a folder structure. Anyhow, I digress. So objects can be anything from CSV, JSON, text, video, image, or audio files, to machine-readable binary data. This versatility makes Object Storage the perfect repository for semi-structured and unstructured data, which can be useful when supporting applications like serving data for training machine learning models. Object storage plays a crucial role as a data source. 
In later weeks and courses, you'll see how object storage is also integrated throughout the entire data engineering lifecycle. But for now, let's take a look at some of the key components of object storage. In Object Storage, each object is assigned a Universal Unique Identifier, or UUID, which is a sort of a key. This key is required for accessing and managing the corresponding object. Each object also has associated metadata, which is additional information about the object, like the creation date, file type, or owner. It's worth noting that after the initial write, objects technically become immutable, and they don't support random write or append operations. In this sense, a file in Object Storage is not like a table in a relational database or a document in a non-relational database that you can update or append to. 
To change the data stored in an object, you must rewrite the full object and have the uuid point to this new object. With object storage, you can enable object versioning, which allows you to add metadata to an object to specify its version. So when you update an object, instead of overwriting the old object under the same uuid, you can keep multiple versions of that object. So why use object storage? Well, Object Storage allows you to store files of various data formats without a specific file system structure. This removes the complexity associated with hierarchical folder systems and databases. In a cloud environment, object storage can easily scale out to provide virtually limitless storage space for massive amounts of data. 
In terms of availability, the data in cloud object storage is typically replicated across several availability zones, meaning that data is replicated across multiple physical data centers that are isolated from each other. This makes the data highly durable and available even in the case of natural disasters. For example, as I mentioned in the previous course, Amazon S3 offers 11 nines of data durability, which means Object Storage on S3 can withstand concurrent device or data center failures. Also, Object Storage is often cheaper than other storage options, especially if you're storing data that you don't need access to on a regular basis. Cloud Object Storage is used in many applications and is the underlying storage for newer architecture designs such as data lakes and data lake houses because of its flexibility, high scalability, cost-effectiveness, and durability. Next, you'll get a chance to work with Amazon s three object storage. You'll create an s three bucket query data from the bucket, and work with object versioning. 
After the lab, join me in the next video to take a look at application logs as streaming system data sources. 



#### Logs
The simplest type of streaming system I can think of is a log. In fact, the log isn't even a system at all. It's just a record of information about events that can serve to track the activity of a system or an application. In the previous course, I mentioned that it used to be common for developers to regard the data coming from software applications as an exhaust or a byproduct, not necessarily having any intrinsic value on its own, but useful for monitoring or debugging a system. The specific data that is most commonly regarded as exhaust is the data contained in logs produced by software applications. When a developer deploys a product or a platform, like a website or mobile app, they'll set it up such that all activity that occurs within the application is recorded in a log. The log might include a user activity, like a user logging in, or navigating to a particular page. 
It might also include a record of events on the back end, like an update to a database or an error that was generated when trying to run a particular procedure. Logs are most commonly used in practice as a means of monitoring the health of systems. Engineers will use logs to trigger alerts or to debug, what went wrong when an error occurs. In this sense, logs can seem boring and the characterization of logs as application exhaust might seem appropriate. However, logs are a rich source of data that can be useful for much more than just monitoring the health of an application. As such, they can be an important source system. You'll ingest data from in your work as a data engineer. 
At its core, a log is an append-only sequence of records ordered by time capturing information about events that occur in systems. For example, if you're the data engineer for an e-commerce company, your web server logs can capture detailed user activity data that could be used to support downstream analysis of user behavior patterns. Many database systems will have logs that you could use to track changes in the database process known as change data capture or CDC, for short. You could use those changes to trigger your ingestion processes so that they run based on the arrival of new data in the database. Or you might ingest log data for use in certain machine learning applications, like anomaly detection, if, for example, you're ingesting log data from security systems. Logs play a crucial role in tracking what happened in many of the upstream software systems you'll work with. This makes them a rich data source that can support downstream use cases like data analysis, troubleshooting issues, monitoring performance, machine learning applications, and automation. 
As I mentioned before, log is a record of information about events. In general, the data you'll find recorded first for each event in a log is the person system or service account that's associated with the event, like a user ID and their IP address. Next, you'll find a record of the event that happened along with its metadata. For example, a user added a specific product to their cart, along with the status of that action. Finally, you'll find a record of the timestamp of the event. Log data may be recorded as simple unstructured text or in JSON or CSV format, or even as binary encoded data. In addition to the data describing the time and substance of an event, logs will also often include a tag to categorize the event by assigning what's known as a log level to each record. 
Log levels might include tags like debug, info, warn, error, or fatal that let you know what kind of information a particular record contains. For example, a record containing basic activity information would be assigned the info log level. Well, a record containing an error message might be assigned the error log level, or if something more serious happens, like major systems have failed and need urgent attention. This might carry the fatal log level as a tag. We'll talk more about log levels later on when you start building logs into your own data pipeline applications, instead of monitoring for your own systems. As a data engineer, it's important that you understand how to work with logs, their types, formats, and applications. Logs will be an important source of data for the work that you do and can help you troubleshoot issues, monitor performance, and serve lots of downstream use cases. 
Join me in the next video to take a look at some of the streaming systems. 


#### Streaming Systems
In the first course of the specialization, we looked at the difference between batch and stream processing as they appear across the different stages of the data engineering life cycle. Then the last lab of that course, you worked with an example of a data architecture where batch and streaming work together in the context of a recommendation system. In this video, we'll take a closer look at the details of event-driven architectures and how message queues and streaming platforms work as source systems for your data pipelines. First, let's define some terminology. Throughout these courses, so far, we've been talking about streaming data in terms of events, messages, and streams. Broadly speaking, an event is just something that happened in the world or a change to the state of the system. For example, a user clicking on a link or a sensor measuring a change in temperature are both examples of events. 
As I mentioned before, in some sense, you can think of all data as being streaming data at its source. This is because essentially all data consists of a record of events that happened out in the world or within some system. A message is a record of information about an event. A message might include details about the event, like which button a user clicked or what temperature the sensor recorded, as well as some metadata around the event and a timestamp of when the event happened. Messages can be generated continuously to form a stream. A stream is a sequence of messages that might be a series of sensor readings or website clicks over a period of time. Messages and streams collectively make up streaming data. 
If you want to handle chunks of this data all at once, like over a specific time interval, then that would be batch processing applied to a stream of messages. If you want to process each message as it's received, then you need a system that's set up to take action based on incoming messages. Then what you have is a system where messages record information about events and action is taken as messages are received, or in other words, a streaming system. Now, out in the real world, you'll often hear the words event and message used almost interchangeably when it comes to describing the various components of an event-driven architecture. But don't let that worry you. While it's technically accurate to say that events are the things that happen and messages or the information or data recorded about those events. For all intents and purposes in data engineering, it's not important to distinguish between the two. 
When we're talking about events or messages being produced, or consumed, or stored in a queue, it's all the same thing. Anyhow, there are three components of a streaming system: the event producer, the event consumer, and the event router, also known as the streaming broker that sits between the producer and the consumer. Just note that here, I could say message producer, message consumer, and message router, and it would mean the same thing. It's just that you'll often see these things described in terms of events, and so we'll go with that terminology here. The event producer is what generates the messages in a stream. The producer could be an IoT device, a mobile app, an API, or a website, to name a few examples. The event consumer, sometimes known as the subscriber, is what processes each individual message, and there can be more than one consumer in any given streaming system. 
For example, in any commerce scenario, when a user places an order, the ordering system might trigger an event that is passed along in a message to the payment service, through process payment, and then to the inventory service to update inventory. In this case, both the payment service and the inventory service would be the event consumers. The way that events find their way to the correct destination is through the event router or streaming broker, such as Apache Kafka, which acts as a buffer to filter and distribute the events from the producer to the consumer. It's this router that helps decouple the producer from the consumer, which enables asynchronous communication between them, so the producer doesn't have to wait for the event to be delivered to the consumer before it can send another one. This also prevents events from being lost even if the consumer is not immediately available. When you work with event systems as a source system, it could be that your upstream source is a simple event producer, like an IoT device, and your system comprises both the event router and consumer. Or it could be that your upstream source system is made up of multiple producers, routers, and consumers, and the systems you build are effectively just another downstream consumer of events. 
In your work building data pipelines to process stream data, you'll encounter two main types of streaming systems : message queues and streaming platforms. I often see these two types of systems confused with one another. While there are many similarities and potential overlaps and how they can work, there's one main difference between them, and that's in how the event router works. So I'd like to spend a minute talking about that now. In a message queue, the event router acts as a queue that accumulates the messages sent by the producer. The event consumer then reads the messages from the queue in a first-in-first-out order. Once the consumer reads the message from the queue and acknowledges this, the message is deleted from the queue. 
With a message queue, the event producer can push new messages to the queue at any time, and the event consumer can read them at any time. You can think of the queue itself as a sort of temporary storage solution that allows event producers to be decoupled from event consumers. An example of a message queue that you might encounter as a data engineer is Amazon Simple Queue Service or SQS. It's a fully managed message queue that's commonly used for microservices, distributed systems, and serverless applications. With a streaming platform like Apache Kafka or Amazon Kinesis Data Streams, the event producer streams events to a log, which, as we looked at in the previous video, is an append-only record of events. The event router then distributes messages in the log to appropriate event consumers. The consumer processes messages in the log sequentially as a read-only operation. 
This means that unlike a message queue, the messages do not get deleted from the log, and the data is persistent. Since the data is retained in this way in a streaming platform, it's possible to replay batch or reprocess events in the log from a past point in time. Streaming systems will be among the source systems you'll ingest data from as a data engineer. As you saw in the previous course and as you'll continue to see throughout these courses, you can also build streaming systems into your own data pipelines as part of the ingestion, transformation, and serving stages of the life cycle. So that's an overview of some common source systems you might encounter as a data engineer. In the next lesson, we'll discuss how you can connect to source systems. I'll see you there. 


#### Lesson Overview
In the previous lesson, we looked at some of the practical details, some common source systems you'll work with as a data engineer. In the labs, you got some practice manipulating data in relational and no SQL databases, as well as in object storage. When it comes to actually connecting to source systems and your work as a data engineer, it's relatively common to run into unforeseen issues that block you from accessing the data you're interested in. These issues can be due to things like having improper identity and access management or IM definitions, broken networking configurations, or even just having the wrong set of access credentials. These might at first sound like relatively trivial issues, but in my own experience, running into these problems happens all the time in data engineering, and they can be major blockers if you don't know how to debug and solve them properly. In fact, I think that solving issues like these are a core skill for data engineers that when I interview new data engineers, I like to take a working data pipeline that can connect to the intended source systems, and maybe break some of the IM or networking configurations, and then ask the candidate to solve the problems. This shows me how well prepared they are to troubleshoot these issues on the job. 
In this lesson, I'll start by going over some of the details of how you connect to different source systems. I'll demonstrate this in AWS. But the principles we'll be looking at also apply to other Cloud platforms. In the context of IAM roles and permissions, we'll look at the importance of security in the Cloud, where IAM is the key to controlling and managing access to Cloud based data sources and other components within your data pipelines. Finally, we'll get into networking. I'll start you off with a high level overview, and then Morgan will walk you through the details of networking on AWS, including VPCs and subnets, gateways, routing, security groups, and more. So in this lesson, you'll build on your knowledge of basic networking concepts that we looked at in the previous course. 
After this lesson, you'll put your skills to the test. Just like I do with data engineering candidates in the interviewing process, I've set up a challenge for you in the next lab exercise. There you'll find a data pipeline that should look familiar from one of the previous labs, but now I've broken it. In that lab exercise, you'll get a taste of what it's like to work as a data engineer tasked with connecting to a source system on the Cloud. I promise you, this won't be like your standard hello world exercise where everything works as expected. Set, you'll be thrown into a scenario where just like in the real world, things don't work the way you'd hoped. Your job will be to troubleshoot and figure out the cause of the problem and then solve it to connect to the data sources you need. 
Join me in the next video, where I'll give you a high level overview of ways to connect the source systems. 


#### Connecting to Source Systems
So far this week, we've looked at a number of examples of source systems. But before you can ingest data, you need to first establish a connection to your data source and verify that you're authorized to read data from it. In fact, you've already had some experience doing this in previous labs. For instance, in the DynamoDB lab, you used Boto3, which is the AWS software development kit or SDK for Python to create a client connection to a table within DynamoDB. And you also connected to an Amazon RDS MySQL instance by running this command with proper parameter values. The endpoint and port information you see here are what you use to locate the correct database instance. And the username and password credentials were used to authenticate you as someone with the proper permissions to access the database. 
And so, as you can see, there's more than one way to connect to a database. Or any resource for that matter. So let's take a closer look at this. If a source system is housed in a resource within your organization's AWS account, you can get the connection information from the management console. For example, if I'm trying to connect to an RDS database instance, I can navigate to the RDS console like this, locate the database I want to connect to, and find the connection information, including the endpoint and the port number. And just as a side note, AWS is always rearranging exactly how things appear in the console. So what I'm showing here might look a little different than the console when you're looking at it. 
But this basic set of steps will still be the same. And so the console can be pretty convenient for finding information like this or spinning up resources and connections. But keep in mind that doing your work from the console involves you navigating through and clicking on widgets and buttons to get things done. If you had to repeat this process in the future, it could be hard to remember exactly what steps you took. And like I said, by the time you want to do this again, AWS may have changed how things are arranged in the console, which could make things even harder. So in general, operating from the console is great to get something done quickly, maybe when you're prototyping something in your system. But the process is not very repeatable nor traceable. 
As a somewhat more programmatic way of finding the information you need and connecting to source systems, you can run code at the command line interface or CLI. In this way, you can get the database endpoint, then you can connect to the database using the command syntax specific to the DBMS you're using. And so issuing commands directly in the CLI like this is a common practice among data engineers in the connection and ingestion process, but it's still relatively manual. So it's typically better for simple workloads rather than complicated ones. To take another step towards repeatability and automation, you can connect to a source system using an SDK like boto3, running code in an IDE or, for example, from a Jupyter notebook. For certain source systems, you can also connect to them through an API connector. For example, you might use a Java Database Connectivity, or JDBC for short, or Open Database Connectivity, or ODBC for short, APIs to connect your application to a DBMS so you can query the database. 
And so you've already had some experience with some of these methods for connecting to source systems. And in the reading item that follows this video, you'll find materials that go over more of the details of each of these methods. After reviewing these materials, join me in the next video for an overview of identity and access management and permissions. 


#### Basics of IAM and Permissions
When you're building data pipelines in a Cloud-based architecture, identity and access management, also known as IAM, is central to your role as a data engineer. As I said in the previous course, as a data engineer, you're entrusted with sensitive data. Whether that's the personal and private information of your clients or proprietary business information. The owners of that data are trusting that their information is safe in your hands. Now, security on the cloud is a vast and complicated subject, but for the purposes of these courses, we're going to focus on the basics. And that's because by simply adhering to a basic set of best practices, you'll be able to successfully avoid the vast majority of data disasters. In fact, a study in 2023 found that more than half of all cloud data breaches were caused by simple human error, things like insecure storage of passwords or other credentials, or IAM misconfigurations. 
These data breaches can be extremely costly. So by ensuring the security of your data pipelines, you can help your company save money, sometimes millions of dollars or more, and protect its reputation and yours. In my own work with clients, I've been amazed at how many mistakes I see that could be corrected with a simple fix. I've seen people do things like storing confidential data in a public S3 bucket, putting access credentials up on GitHub, or providing essentially admin access to data resources for everyone at the company. These types of things are easy to fix, but present a major risk to your company and could cost you your job if disaster strikes on your watch. So my goal is to set you up for success so this doesn't happen to you down the road. Let's dive into the basics of how you'll be working with permissions in IAM as a data engineer. 
First off, IAM is a framework for managing permissions. Permissions define which actions an identity, like a person or an application, can perform on a specific set of resources, like a database or an ETL tool. IAM ensures that the appropriate set of identities have access to the right resources at the right time. Remember the principle of least privilege we talked about in the previous course? IAM is how you exercise this principle in practice, as it allows you to grant people and applications access to only the essential resources they need to do their jobs, and only for the duration that it's required. For example, a source system owner can grant a data ingestion system the ability to read data from only a specific table in a database for a limited period of time by not granting unnecessary access to your cloud resources when they're not needed. You're also preventing your team from incurring unnecessary cloud costs. 
Many cloud providers have built in IAM services that help users manage access and permissions to their cloud resources, for instance, AWS IAM is a web service that helps you manage and securely control access to AWS resources in your account. In fact, throughout these courses so far, we've been using AWS IAM to provide you with appropriate access to AWS resources in the labs. As a brand new data engineer, chances are that you won't be the person setting up IAM configurations at a high level across your company's cloud service account. But you will be interacting with various cloud resources, and you may be responsible for configuring IAM for the resources you deploy as part of your data pipelines. And so you need at least a basic understanding of the different IAM components so that you can operate securely and troubleshoot problems as they come up. And so we'll look at these iam components in the context of AWS Iam. But just know that these same types of components are common across other cloud providers like GCP or Azure, they just might go by different names in some cases. 
In AWS IAM, you'll use policies that grant identities permissions for actions on AWS resources. There are different types of AWS identities. There's the Root User who created the AWS cloud account and has unrestricted access to all resources in that account. Then there are IAM Users who are given specific permissions to certain resources. As a junior data engineer, it's most likely that you'll be assigned an IAM User account within your company's AWS account. You'll be given a set of long term credentials, like a username and password, or an access key that you can use to programmatically access AWS resources using code. Then there are IAM groups, which are collections of users that you can attach a policy to. 
A policy is just a JSON document that includes the details of what resources and permissions the group should have in this case. This streamlines the process of provisioning resources. For example, your company might have an IAM group for data engineers, where every user in that group has access to the resources needed to build and maintain data pipelines. And finally, there are IAM roles which are not associated with a specific person or application long term, but are briefly assumed by a user application or service to grant them temporary permissions to perform specified actions on your AWS resources for a limited time. For example, by default, an EC two instance does not have permission to read or write to s three storage, but you can create a rule that has read and write permissions to specific s three buckets and let the EC two instance assume this role when needed. This is a much more secure way to give the EC two instance permission to work with s three than, for example, storing your long term user credentials within the EC two configurations. If you ever get an access denied error message when making a request with temporary role credentials, it's good to check whether those credentials have expired. 
This is another issue that is very common when it comes to IAM configurations. To help you see how IAM policies work, let's take a look at a part of the AWS policy that gave you access to the resources you needed for the labs in the previous course. From the first statement here, you can see that the policy allows the user, in this case you, permission to access any s three buckets whose name begins with the DLAi data engineering. And that's because you have this little asterisk or star after the name, and then you are allowed to take these actions which include list or get. So again, these asterisks means you can take any action that begins with list or get in this case. These actions include listing the contents of a bucket or the names of buckets you have access to or in the case of get. These actions include getting objects from a bucket or information about a bucket like the region the bucket is located in. 
The second statement is similar and allows you permission to access the AWS Glue job associated with the lab. And this glue.star under action just means you can take any action with regard to that Glue job. And so during the lab, when you make a request to run the AWS Glue job, AWS first evaluates this policy to determine if you are allowed to perform this action with glue. So those are the basic components of IAM, you have the Root User, IAM Users, groups, roles, and policies. You'll find more details about these components in the reading materials that follow this video. After reviewing those materials, join me in the next video to look at networking in the cloud. 


#### Basics of AWS IAM
This reading item builds on the previous video and contains more information specific to AWS IAM.


What is IAM?
AWS IAM is a web service that helps you manage and securely control access to your AWS resources and services. With IAM, you can centrally manage who is authenticated in your Account and what resource permissions they have. Using IAM, you can share your resources without sharing your credentials, and you can select specific actions people can access at a granular level. It’s a global service available at no additional cost, meaning you can see and use your IAM configurations from any region in the AWS Management Console.



What is an IAM user?
When you create an account on AWS, you begin with the “root user” identity, which has full access to all AWS resources and services in the account. It is strongly recommended that you don’t perform daily operations using this account. Instead, create an admin user for everyday tasks. Whether you’re the root or admin user, you can create other users in your account to allow people in your organization to access AWS resources.

IAM users are created under your AWS account, so you don't need separate accounts for IAM users. Each user could be a person or service that interacts with AWS resources. When you create a user, you define what resources the IAM user can access, and what actions they can perform. AWS will then generate a set of credentials for that user. The credentials could be a username and password for accessing the AWS management console, or they it be an access keys for programmatic access to AWS resources. IAM user credentials are long-term credentials as they stay with the user until the admin rotates them. When you provide users with their own login credentials, you help prevent the sharing of credentials. You can add more users to your account, and all user activities are billed to your account.

By default, a new user does not have any permission to access any AWS resources. You can grant them access to AWS resources by attaching policies to them. A policy specifies what actions are allowed or denied for a given resource (read only, write only, full access). A policy can be attached to multiple users, and a user can have multiple policies. You can choose AWS-managed policies or create your own custom policies. Whenever a user makes a request, AWS evaluates their policies to determine if that request is allowed.

For example, suppose you’re working with a data scientist and want to grant them read-only access to an S3 bucket to extract the training data set. The figures below show the steps to create this user.

1. Create a new user


2. Create the credentials for this user


3. Set the permissions for this user by attaching an existing policy or creating a new one. (For more information, see the What is an IAM policy?  section below)


4. Review the user details and create the new IAM user


5. Share the login credentials with the user



What is an IAM group?
Now, what if you want to grant the same permissions to more than one user, maybe a team of data scientists who want to access the same resources with the same level of permissions. You could attach the same policy to each user, but it might be hard to manage as the team grows. In this case, you can create an IAM group, which is a collection of users, and then attach the policy to the group rather than individual users. Each user in the group inherits the group's permissions. Think of the IAM group as a way to organize permissions. Here are some features of IAM groups: 

Groups can have multiple users

A user can belong to no group, one group, or multiple groups (up to 10 groups)

Groups cannot be nested


What is an IAM role?
The third IAM identity is a role. An IAM role has specific permissions with short-term credentials. Roles can be assumed by entities, like people, applications, or trusted AWS resources. IAM roles don't have long-term credentials. Instead, they provide temporary security credentials for the duration of the role session. You first create an IAM role and attach a policy to it. Then you specify which resource can assume this role. This temporarily grants permissions to AWS resources.

Example 1: Let’s say you run a code on an EC2 instance that needs to read from S3. By default, the EC2 instance does not have permission to read from S3. You can transfer your credentials to EC2, but this is not secure. A better approach is to create a role, attach the required policy to read from S3, and allow the EC2 instance to assume this role. 

Example 2: Let’s say you run a Glue ETL job and want it to write the ingested and transformed data to S3. You can create a role with permissions to write to S3, then allow Glue ETL to assume this role.


What is an IAM Policy?
You can manage access in AWS by creating policies and attaching them to IAM users, groups, or roles.

“A policy is an object in AWS that defines the permissions of the attached user or role. AWS evaluates these policies when an IAM user or role makes a request. Permissions in the policies determine whether the request is allowed or denied. Most policies are stored in AWS as JSON documents.” - 
AWS IAM documentation

You can use an AWS-managed policy or create your own custom policy.

Example: AWS-managed policy (AmazonS3FullAccess)

Here's the AmazonS3FullAccess managed policy that grants full access to S3.


Version – Specify the version of the policy language that you want to use.

Statement – Use this element as a container for the details of some given permissions or denials. You can include more than one statement in a policy. If a policy includes multiple statements, AWS applies a logical OR across the statements when evaluating them.

Sid (Optional) – Include a statement ID to differentiate between your statements.

Effect – Use Allow or Deny to indicate whether the policy allows or denies access.

Action – Include a list of actions the policy allows or denies. In this example, the allowed actions on S3 are “*”, meaning all read and write actions on s3 are allowed).

Resource – An object or a list of objects to which the actions apply. For example, in the case of S3, you can specify which bucket is allowed or denied access. In this example, the resource element is "*", meaning all resources.

Example: Customer managed policy

Here’s another example of a policy that allows read and write access to all S3 buckets, except the bucket “confidential”, where deletion is denied.


#### Basics of Networking in the Cloud
When you're building a data pipeline in a cloud based architecture, what you're really building is a network of connected resources. And so the way you configure that network plays a key role in ensuring that the data flows properly throughout your data pipeline. In a previous course, you looked at the basics of networking and how networking is really just a collection of connected devices that can share data and communicate among themselves and over the internet. Here I'd like to revisit and expand on some of those basic networking principles to prepare you for the problems you might encounter when trying to connect to source systems. When it comes to networking in the cloud, the basic principles are roughly the same across all major cloud providers. Here I'll go over the principles in the context of AWS, since that's what you're working with in these courses. But just know that the underlying concepts apply to whatever cloud you're working on. 
The term cloud computing can sound a little abstract, like computation is somehow happening out in the ether. But make no mistake, the cloud and cloud computing is made up of very real physical data centers that are spread out around the world. As you learned in the previous course, the AWS cloud is a global network that is spread across different geographical areas known as regions. Each region contains clusters of availability zones, and each availability zone consists of one or more data centers with redundant power, networking, and connectivity. In many cloud computing applications, data and resources are replicated across regions and availability zones to ensure that systems keep working even if one or more data centers were to go down. As a data engineer spinning up new resources on the cloud, you'll need to decide which region to host your resources in. In doing so, you might need to consider things like legal compliance. 
For example, does storing your data in a specific region mean it needs to adhere to unique data privacy or regulatory requirements? You might also need to consider other factors like latency and availability. In general, the closer your end users are to the region where your resources are hosted, the lower the latency. And the more availability zones your resources are replicated across, the better you will be able to withstand or recover from a disaster. In addition to these considerations, the cost of resources can vary from one region to the next, which may be a factor in your decision. And so when you're actually working with resources on the cloud, it can be easy to lose sight of the fact that what you're really doing is interacting with a network of physical devices that are spread out around the globe. But as a data engineer, it's important to understand how this global infrastructure is set up and how it impacts things like latency, cost, reliability, and availability of the systems that you build. 
So moving back to important things you need to know about networking in the cloud. Within any given region, you can create custom virtual private clouds, or VPCs, which are smaller networks that span multiple availability zones within a region. Creating VPCs allows you to have more fine-grained control over who can access what resources. And so you can then divide the space in your VPC into subnetworks, or subnets for short, that house your actual data pipeline resources. Each subnet can then have its own security rules, known as a network access control list, or network ACL for short, as well as routing configurations through an internet gateway. This lets you create public subnets for internet-facing resources, like web servers, and private subnets for internal resources, like databases. In the real world, things can get complicated very quickly, especially when you start setting up multiple VPCs, subnets, gateways, and routing configurations between resources. 
And it's in this context that the simple act of connecting to a database depends on multiple layers of networking configurations, not to mention IAM permissions. And so understanding the details of network configuration is critical when it comes to connecting to your source systems. It's also required for successful orchestration and automation of your data pipelines, which we'll get into in the last week of this course. Up next, Morgan will walk you through the details of networking on AWS. And after that, I'll show you what you can expect in the upcoming lab, where you'll debug your connection to a database. 


#### AWS Networking Overview - VPCs & Subnets

en
​
Interactive Transcript - Enable basic transcript mode by pressing the escape key
You may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key
>> As Joe has already emphasized, networking will be a significant part of your job as a data engineer, and networking can feel overwhelming at first with all there is to know about setting up connections between resources, permissions, security and all the rest. But don't worry, networking in the cloud is something you can become proficient in. It just takes time and understanding a set of core concepts and how you can apply those concepts in building your data systems. Through this and the next several videos, I'll go over the details of these important core concepts including Amazon Virtual Private Clouds, or VPCs, subnets, gateways, route tables, network access control lists, and security groups. And I'll demonstrate how you can implement these elements of your data pipelines on AWS. So I'll go through a series of demos over the next few videos, where I'm going to show you how to build out the VPC and networking components for a scenario where you need to deploy an Amazon RDS database and an Amazon EC2 instance. With this example scenario, let's imagine there is a web application running on the EC2 instance that allows you to query the database running on RDS. 
This is a simple scenario for any kind of web application with a relational database on the backend. So the end state will look like this. One VPC with two public and two private subnets, EC2 and RDS database instances deployed into the private subnets, and NAT gateways in the public subnets, as well as an application load balancer to front the web application fronting on the EC2 instances. I'll walk through each setup step for this scenario that is related to networking and explain all of these components as we go along. We won't actually deploy the EC2 or RDS instances or the application load balancer, but we will build out all of the networking components needed for it. Having this scenario in mind will hopefully make the networking concepts easier to grasp. So to get started, we will need a VPC and subnets to place the EC2 and RDS instances in. 
Let's go ahead and complete that task. From the AWS console homepage, I will go to the search bar and type VPC, then I'm going to select VPC. This brings us to the VPC dashboard where I can then select the Create VPC button. Now on the create VPC page, I am only going to create one VPC. Then we will manually create these subnets next. Before moving ahead though, I want to point out that there is what's called a default VPC in each region. By default, in an AWS account, the default VPC includes a public subnet in each availability zone in that region, and an Internet gateway using the default VPC can allow you to quickly launch public Internet facing EC, two instances without additional setup. 
This can be convenient for experimenting with launching public resources like a simple website, but for most real world use cases, you likely don't actually want directly public facing resources, and instead you want network protected resources. So with that in mind, your default should not be to use the default VPC for any real work. Instead, you should create custom vpcs for your specific use cases, and that's what I'm going to show you how to do here. As you learned in the previous course, AWS operates in regions around the world, and these regions are made up of multiple availability zones, or azs for short. A VPC has the ability to span all of the azs within the region in which the VPC was created. Here were going to create one VPC, but a region can have more than one VPC in it. For example, you might create different vpcs for different projects, environments, or other organizational or technical considerations. 
Each VPC is isolated from other VPCs by default. Resources within the same VPC can communicate with each other, but communication between resources from two different vpcs is something you would need to design and configure. When you create a VPC, you need to give it a name, a private IP address range, and choose the region you want it to be placed into. It's useful to give vpcs a descriptive name so you can more easily identify which VPC is which. So I will go ahead and name this VPC project one and you can see here the region im creating this VPC in is Us east one, but I could choose another region from the dropdown if I needed to. Next, you need to define the IPv4 CIDR block. CIDR stands for Classless Inter-Domain Routing and it defines the range of private IP addresses that can be used within the VPC or how many private IP addresses are available in the VPC. 
Any resource that gets deployed into this VPC will be assigned a private IP address from this range. That being said, if you want to create a resource that is available via the Internet, you would also need it to have a public IP address. Any public IP address assigned to it would come from a pool of AWS managed public ips. So this CIDR range here is only for private IP addresses. So here I'm going to type 10.0.0.0/16 for the CIDR. And I'd like to pause now and go over this CIDR notation because it can seem complicated if you're not used to seeing it. I'm going to try to simplify it to make it easier to grasp if this is your first time getting into the details of networking. 
IP addresses have four numbers separated by dots. Each number can range from zero to 255. In other words, each number in the address is an eight bit integer value. In the case of this example, 10.0.0.0/16, the 10.0.0.0 is the starting address of our network. The 16 part is a prefix length which tells you how many bits are used for the network part of the address. So in this case, the 16 tells you that 16 bits, or in other words, the first two of these eight bit integers will be the network prefix. In binary form an IP address consists of 32 bits, which again, just means that each number between the dots in the address represents 8 bits, with 4 numbers of 8 bits each. 
There are 32 bits for the entire IP address. So as I said, the 16 means that the first 16 bits or the first two numbers are fixed and define the network, while the remaining 16 bits or the other two numbers can vary and are used for host addresses within that network. That means that any resource deployed into this network would have a private IP address that starts with 10.0. Then the other two numbers could be anything between zero and 255. If I wrote 24 instead of 16, that would mean that the first three numbers would be fixed and only the last number would be able to be used to assign host addresses. You'll see why you need to know this in the next step when we create subnets. So now back in the AWS console I have defined the different pieces I need to create a VPC and I can select create VPC from here I need to create the subnets. 
Subnets are sub networks within the VPC or smaller divisions of the private IP space for the VPC that you can use to group resources based on their network access and security requirements. Later, we'll use network access control lists and security groups to control what types of network traffic can come in and out of each subnet. Each subnet is associated with a specific AZ, meaning that when you create a subnet, you must specify which AZ it resides in. By strategically placing your resources in different subnets across multiple AZs, you can enhance the redundancy and availability of your applications. It's common to create at least one private and one public subnet per AZ that you intend to use here. I would like to deploy the EC2 instances and RDS databases into private subnets so that they aren't exposed to the Internet. And for redundancy I plan to create two public subnets and two private subnets across two azs in this VPC, which is a common pattern to follow that way. 
For example, if the primary database instance experiences degradation, or if the AZ itself has temporary availability issues, you have all of your data and an instance running in another AZ that can absorb the traffic after failover occurs. So to create these subnets I will select Subnets in the navigation and then select Create subnet. Now I need to select which VPC I want to create the subnets in. I will select the dropdown and choose the project-1 VPC. From this page I can create all four subnets. For the first subnet I will name it public subnet one and then select a specific AZ. So later I can ensure I am deploying additional subnets to different AZs. 
I will select us east one a for this one. Then I need to give it a CIDR range and subnets need to have an IP range that is a subset of the IP range of the VPC. The VPC is defined to use 10.0.0.0/16 so I will make the range for this first subnet 10.0.1.0/24. This means that any resource deployed into the subnet will be assigned an IP address that starts with 10.0.1. Then the last number will be assigned to identify the specific host. Now to create my first private subnet I will select add new subnet and then repeat the same steps. But this time, I will name the subnet private-subnet-1 and give it a CIDR of 10.0.2.0/24. 
Now let's do it two more times for the other public subnet and private subnet for the other AZ which I will use us-east-1b for. So the next thing to do is to create the public-subnet-2 which has a CIDR of 10.0.3.0/24 and then private-subnet-2 which has a CIDR of 10.0.4.0/24. >> Then I can finally select Create which will create all four subnets. All right, you now have a VPC with two public and two private subnets ready to use and it looks like this. The way this sits right now, anything deployed into this VPC would not have access to the Internet. You can add the EC2 and RDS instances to this diagram to show them sitting in the private subnet. Neither of these resources would be accessible from the Internet and they wouldn't be able to initiate connections with any resource on the Internet either. 
So at this point, it's a closed network and you will need to deploy and configure some more resources to make Internet connectivity possible for anything in this VPC. Up next, we will look at Internet connectivity and how Internet and NAT gateways work. 



#### AWS Networking - Internet Gateway & NAT Gateway
At the end of the last video, we had created a VPC with two public subnets and two private subnets. And like I said, if you deployed any resource like an EC2 Instance into one of the public subnets, it would not be accessible via the Internet, and it would not be able to establish connections to other resources on the Internet either. That is because VPCs and subnets alone create an isolated network. No traffic in or out. In this video, we will discuss how to make Internet connectivity possible using Internet gateways and network address translation or NAT gateways. Now, if you recall the scenario we are following, you would have an EC2 Instance and an RDS database in your VPC. As a best practice, both the application running on the EC2 Instance and the RDS database should be in private subnets and wouldn't need direct connectivity with the Internet. 
However, I want to call out two considerations we didn't discuss before. The first is that the applications running on the EC2 do need to occasionally download updates from resources on the Internet for things like application upgrades and patching. And the second consideration is that you would still need a way to submit requests to the application running on the EC2 Instance through the load balancer so that you can query your data on RDS. Both of these considerations mean that your VPC does, in fact, need Internet connectivity. So to do that, you need to first attach an Internet gateway to the VPC. To better understand what an Internet gateway is and what it does, I want you to think of your VPC in its current state like a house without a door. If you build a house around you without a door, you wouldn't be able to leave the house, and no one can enter the house from the outside. 
If you were in the house, you could move freely from room to room. But in order to get out, you would need to install a door to the outside world. So that's what we have so far here, a house without a door. And so what we will be doing next with our VPC is to install a door to the Internet. Or in other words, we will attach an Internet gateway to it. Internet gateways allow resources in your public subnets to connect with the Internet. They support both inbound and outbound traffic. 
Creating an Internet gateway and attaching it to the VPC is just one step in allowing Internet traffic to flow to and from the public subnets. You will also need to configure routes in the route tables and configure network security rules, and we will do those in the next couple videos. Now, I mentioned earlier that our EC2 Instance will be in private subnets. And here I am saying that we will be attaching an Internet gateway to allow traffic to flow to and from the public subnets. How will this help if our resources are in the private subnets? Well, let's recall our two considerations from earlier. First, the EC2 Instance needs to be able to download updates from resources on the Internet. 
This means that our EC2 Instance in the private subnet needs to be able to make outgoing connections from the VPC. And the second, you need to be able to submit requests to the application from the Internet. Let's talk about how NAT gateways and an application load balancer can help us meet these requirements. The NAT and NAT gateway stands for network address translation, and this is a service that allows resources in a private subnet to connect to the Internet or other AWS services, but prevents the Internet from initiating connections with those resources. Think of it as a controlled doorway that only allows outgoing traffic and protects the resources inside that are initiating that traffic. With a net gateway in place, your EC2 Instances in the private subnet can download updates and patches from the Internet without exposing them directly to the public Internet. Next, we need to address the second consideration, allowing external users to submit requests to our application. 
This is where the application load balancer or ALB comes in. The ALB distributes incoming application traffic across multiple back end targets, like our EC2 Instances, which are hosted across two availability zones. The ALB serves as the entry point for external users. Handling the load and ensuring the application remains responsive and available while also allowing us to keep those EC2 Instances private. We won't actually be building this part since I am focusing on the networking aspect of this architecture for these lessons. However, it's good to know when to use an ALB to allow for application connectivity without directly exposing your back end EC2 Instances. All right. 
So now let's create and attach an Internet gateway and deploy two NAT gateways to the VPC we built in the previous video. Here I am in the AWS Management Console on the Home page. And I will type in VPC into the Search bar. Then from the VPC dashboard, I will select Internet gateways from the Navigation panel. From the Internet gateways page, I will select the Create Internet gateway button. On the next page, I can give the Internet gateway a name. I'll name it Project 1 Gateway, and then select Create Internet Gateway. 
So now we have an Internet gateway, but we need to attach it to the VPC. Just as a note, one VPC can have one Internet gateway, and an Internet gateway can only be attached to one VPC at a time. This is a one to one relationship. From here, I'm going to select actions, then attach to VPC. On the next screen from the list of VPCs, I can select the Project 1 VPC, and then select attach Internet gateway. Now, the state of this gateway is attached. So we have installed the front door to this VPC. 
The next step is to create the NAT gateway. It's a best practice to create one NAT gateway in every AZ that you are operating in. So I will actually create two NAT gateways and place them in each public subnet. To do this from the navigation panel, I will select NAT gateways then create NAT gateway. From this screen, you'll configure the NAT gateway. So first, I'll give it a name. I'll call this one NAT Gateway 1, and then select Public subnet 1 from the dropdown. 
Then this needs to have an elastic IP address configured, which will provide a static IP address, and I will select Allocate Elastic IP to do this. Then I will select Create NAT Gateway. Then I will repeat the steps, but this time, place the NAT gateway into the other public subnet. I'll name this one NAT Gateway 2, and then select Public subnet 2 from the drop down. Create the Elastic IP address, and then finally, create the gateway. At this point, this is what our network looks like with the EC2 Instances, RDS database, and ALB included. There are a few more steps that we need to walk through before inbound and outbound Internet connectivity is working. 
In the next steps, we will walk through the process of setting up the necessary elements, which will include configuring route tables and defining security rules to secure our VPC. By the end of the series of videos, we'll have a VPC with both secure Internet connectivity and robust access control. 


#### AWS Networking - Route Tables
The next basic component of networking on AWS, you need to learn about our route tables. In the last video, we ended with an architecture diagram like this. There is a VPC with two public and two private subnets across two AZs and an Internet gateway attached to the VPC, with EC2 instances and RDS database instances in the private subnets, NAT gateways deployed in the public subnets in HAZ, and a public application load balancer, which fronts the private EC2 instances. The next step to enable Internet connectivity is to configure the route tables for the subnets. Route tables are essential for directing network traffic within your VPC. Each subnet can be associated with a route table, which contains a set of rules or routes that determine where network traffic is directed. When you create a VPC, AWS automatically creates a default route table. 
This default route table allows internal communication within the VPC, meaning resources in different subnets can communicate with each other. However, it does not include routes for Internet connectivity. This is where you need to customize your route tables to meet your specific requirements. Without these routes, your subnets won't know how to direct traffic either to the Internet or within the VPC itself. For public subnets, you'll configure the route table to direct all Internet-bound traffic to the Internet gateway. This allows resources in the public subnet to access the Internet. For private subnets, you'll direct Internet-bound traffic to the NAT gateway in the public subnet. 
This allows resources in the private subnet to make outbound connections to the Internet while preventing inbound connections from the Internet. Let's hop into the AWS Management Console and build out these route tables and routes. First, I will create and associate the route tables. Then we will actually go back in and create the actual routes. Starting from the VPC dashboard, I will select route tables from the Navigation panel and then select Create route table. I'll create one route table per subnet in our VPC. The first one I will name public-route-table-1. 
Then select Project 1 VPC, and then select Create route table. Then I can associate this one with the first public subnet by selecting actions. Edit subnet associations, selecting public-subnet-1, and then save Associations. Next, we will do the same for the other three subnets. Select Route tables, Create route table, name the second one public-route-table-2. Select the VPC, and create. Then associate this route table with public-subnet-2. 
Now it's time to create the private route tables. I can name this one private-route-table1. Select the VPC and create. Then associate this route table with private-subnet-1. Finally, to create the last one, I'll name it private-route-table-2, select the VPC and create. Then associate this route table with private-subnet-2. Now that we have the route tables, it's time to create the routes. 
For this step, we will want to create routes for the traffic that will flow between the public subnets and the Internet gateway and routes for the private subnets that will flow to the NAT gateway. From the route tables dashboard, I will select public-route-table-1, and then select Edit routes. There is already one route in the table, and this route is a default route added to every route table that you create for communication within the VPC. To add the route to allow Internet traffic to be directed to the Internet gateway, I will select Add route. Then for the destination, this will be 0.0.0.0/0. If you remember the lesson about CIDR notation, this has a slash zero prefix, which means it can match any IP address because zero bits are fixed. This represents any IP address or in other words, the entire Internet. 
The destination is 0.0.0.0/0 or any IP address. The route we will want to direct traffic to is the Internet gateway. Under target, I will select the dropdown and then select the Internet gateway attached to this VPC we created in the last video. I'll set up this route along with other network and configurations in the next video to allow instances in our public subnet to send and receive traffic from the Internet via the Internet gateway. I will now select Save changes and then navigate back to the route tables page. Let's move on to configuring the private subnets route table. I'll select a route table associated with one of our private subnets and add a new route. 
Again, the destination will be 0.0.0.0/0, but this time, the target will be the NAT gateway that we deployed to the public subnet. This ensures that any Internet-bound traffic from our private subnet instances is routed through the NAT gateway, allowing these instances to access the Internet for updates and patches while remaining isolated from direct inbound Internet traffic. To complete this, you would do the same steps for the other public and private subnets. Now, our route tables are configured to handle both internal and external traffic as required. The public subnets have routes directing Internet-bound traffic to the Internet gateway, enabling instances within these subnets to communicate with the Internet. The private subnets have routes directing Internet-bound traffic to the NAT gateway, allowing instances within these subnets to make outbound connections to the Internet securely. In the next video, we'll cover additional networking configurations, such as security groups and network ACLs. 
These configurations will further secure our VPC by defining the inbound and outbound traffic rules for instances and subnets. 


#### AWS Networking - Network ACLs & Security Groups

en
​
Interactive Transcript - Enable basic transcript mode by pressing the escape key
You may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key
As a reminder, in the previous video we left off having configured the route tables for these subnets which will direct traffic for the VPC here, well look at a few other networking configurations you're going to need to know about. In our scenario, the application load balancer will send Internet traffic to the EC two instances which would then create a connection with the RDS database instances. For queries, you can configure networking rules to allow only the network traffic you want to reach these instances. By default, no traffic is allowed to reach these instances even with the route tables in place. To change that, you first need to understand security groups and network access control lists or network acls. Let's start with security groups. Think of security groups as instance level virtual firewalls controlling both inbound and outbound traffic. 
By default, security groups deny all inbound traffic and allow all outbound traffic. So you need to define inbound rules which determine what types of traffic you want to allow and where you want to allow that traffic to come from. Security groups are also stateful, which means that if you allow inbound traffic to an instance, the return traffic is automatically allowed even if there are no outbound rules explicitly permitting it. For instance, if you permit an inbound HTTP traffic on Port 80, the responses to those HTTP requests are allowed to flow out without needing an explicit outbound rule. This stateful nature simplifies security group management because you don't have to create matching outbound rules for every inbound rule. Resources that are placed in VPCs use security groups like EC2 instances, load balancers, and RDS database instances. All can use security groups with different rules, and the rules for security groups can reference other security groups. 
In our example, we have an application load balancer that would need to accept traffic for HTTP on port 80 and HTTPs on port 443 from the Internet. Then we have an EC two instance hosting a web server. So you'll need to allow HTTP and HTTPS traffic coming from the application load balancer, so you could reference the security group used by the load balancer. We also have RDS database instances which would need to allow TCP on Port 3306, which is commonly used by databases like MySQL coming from the security group used by the EC2 instances. This is called security group chaining. Now let's walk through an example of creating a security group that could be used by the application load balancer to set up a security group, navigate to the VPC dashboard, and then select security groups. Then select create security group. 
First you give it a name like Albsg then we want to select which VPC this security group belongs to. Because security groups are associated with one VPC, we will select the project one VPC. Next we need to define the inbound rules. I'll select Add rule. Then for the type of traffic, I will select HTTP, which will auto populate the port range to be 80. Then I will select 0.0.0.0/0 for the source which will allow traffic from the Internet on Port 80. Then I will add another rule for HTTPs which uses port 443 and will allow traffic from the Internet. 
Then I will select create security group. This security group can then be associated with the load balancer. Now let's move on to network acls which provide an additional layer of security at the subnet level. Unlike security groups which are stateful, network ACLs are stateless. This means you need to define both inbound and outbound rules explicitly. They offer more granular control over traffic and can be particularly useful for implementing security policies at the subnet level. By default, network acls allow all inbound and outbound traffic, but you can modify these rules to meet your specific security requirements. 
We don't need to change this behavior for our simple use case, but it's still important to know as it's one of the main ways to control network traffic on AWS and you may need to tweak these rules if you are troubleshooting a network issue. So this is what our diagram looks like now, accounting for the places where we could apply security groups and network acls in the upcoming lab. You will need to troubleshoot connectivity issues with a database, which is a very common scenario you will run into as a data engineer. So what I want to do now is summarize everything you learned about networking on AWS in the last few videos so that you know where you need to look when trying to troubleshoot network connectivity issues on AWS. First, we covered the basics of VPCs and subnets, which give you a way to define a private isolated network on AWS. We discussed the importance of correctly configuring route tables to direct traffic within the VPC and to the Internet. You learned how to set up public subnets with routes pointing to the Internet gateway, allowing resources within these subnets to access the Internet. 
Similarly, we covered configuring private subnets with routes pointing to the NAT Gateway, enabling instances to initiate outbound connections securely. Notice how we call the subnets public subnets or private subnets, but it's actually the route table that determines that access. There is nothing magic about creating a public or private subnet. It's all about correctly configuring and managing the route tables. Next, we explored security groups, which act as virtual firewalls. At the instance level. They control both inbound and outbound traffic, and their stateful nature means that if you allow inbound traffic, the return traffic is automatically permitted. 
It's most common to configure inbound rules only on security groups, as they by default, allow all outbound traffic. We then moved on to network acls, which provide an additional layer of security at the subnet level. Unlike security groups, network ACLs are stateless, requiring explicit rules for both inbound and outbound traffic. This allows for more granular control and is useful for implementing specific security requirements. So if you encounter connectivity issues, start by verifying your VPC has an Internet gateway properly attached, that the route tables have appropriate rules to direct traffic correctly, and that the route table associations with the subnets are configured correctly. Next, check security groups to make sure they have the needed rules in place, and review network acls to confirm they allow the necessary traffic. Also, double check instance configurations to ensure they are associated with the correct security groups and subnets. 
In the next lab, you'll put these concepts into practice by troubleshooting database connectivity issues using the knowledge you've gained to identify and resolve network problems. Next up, Joe will walk you through the details of the last lab assignment for this week. 


#### AWS Networking Overview- VPC
In the series of videos that came before this reading item, Morgan gave you an overview of some key AWS networking concepts. This reading item summarizes these concepts for easy reference: VPC, internet gateway, NAT gateway, route table, network ACL, security group, and an optional section on endpoints.


VPC
A VPC (Virtual Private Cloud) is an isolated private network where you can launch your AWS resources. A VPC exists inside a region and can span multiple availability zones. A region can contain multiple VPCs: each region comes with a preconfigured VPC, known as the default VPC, that you can use to launch your resources, or you can create your own custom VPC within the same region.


Launching resources such as EC2 instances or RDS databases within a VPC enables them to interact with other resources in the VPC. However, a custom VPC, by default, does not allow communication with the public internet or other VPCs. You can configure the VPC settings to allow connectivity between VPCs or between a VPC and the internet. Note that AWS provides a default VPC, which is already set up for public internet access.

Learn more

For more information about the default VPC and how to create your custom VPC, see the 
AWS documentation.


Internet Gateway
An internet gateway enables resources created inside a public subnet to send and receive traffic from the public internet. It allows both inbound and outbound traffic, connecting resources within a public subnet to the internet and allowing outside resources to connect to your resources. You can attach only one internet gateway to each VPC. That's because behind the scenes, the internet gateway cloud service is supported by a distributed infrastructure, with built-in redundancy and high availability. AWS will also automatically scale the gateway's capacity up or down to handle varying loads of traffic. 

Note that resources that are created within a public subnet should have two types of IP addresses: one private IP address used to communicate with resources within the same VPC and another public IP address that allows outside resources to connect to them. You would need to enable each resource in a public subnet to have a public address.



NAT Gateway
Resources in a private subnet are hidden from the internet, preventing outside connections and protecting them from attacks and unauthorized access (unsolicited connection requests are not allowed). However, resources in a private subnet can establish a one-way connection to the internet for outgoing requests (e.g., to download an update or send an email). You can enable this one-way connection by using a NAT gateway (Network Address Translation service). You  launch a NAT gateway inside a public subnet and then the NAT gateway can work with the internet gateway to allow resources in a private subnet to connect to the internet. When a private resource sends an outgoing request, the NAT gateway replaces the resource's private address with its own address. To send the response traffic to the private resource, the NAT gateway translates the address back to the original source address. NAT gateways are not available in the free-tier AWS account.


Learn more

Internet Gateway
 (AWS Documentation)

NAT Devices
 (AWS Dcoumentation)


Route Table
Each time a resource within a subnet generates a request, that request will contain the IP address of the destination that it wants to reach. The request needs additional direction to know where to go. For example, does it need to stay in the local network or should it go through a gateway? All of this additional information can be expressed as a set of rules, called routes, that determine where network traffic from your subnet is directed. These routes are stored in a route table. So you can think of a route table as a collection of street signs that direct the traffic generated from a subnet to reach its destination. 

When you create a VPC, a route table called the main route table is automatically created. This table assumes that traffic should flow between the resources within the same VPC, so by default it contains a route that directs any traffic with a destination IP address within the VPC CIDR block to the local network. In addition to the main route table, you can create custom route tables for the same VPC and associate them with particular subnets. A common practice is to create at least two route tables: one that you associate with public subnets and another one that you associate with private subnets. By default, any custom table that you create will have the local route already inside it, and you can add additional routes to it. If a subnet is not associated with any custom route tables, it uses the main route table. Here's an example:


In this example, each subnet is associated with a different route table. The destination column represents the destination IP address to which a request is sent from, and the target column represents how or through what component the traffic should be routed. In route table 1 that's associated with the public subnet, the first row means that any traffic sent to an IP address within 10.0.0.0/16, which is the VPC CIDR block, is routed inside the local VPC. Otherwise, if the IP address does not match any address within 10.0.0.0/16, it is routed to the internet gateway.

Learn more

Route Table
 (AWS documentation).


Network ACL 
You can protect your resources from unauthorized access by choosing private subnets and ensuring proper routing. To add additional security to your subnets, you can also set up a firewall that filters traffic to and from your subnets; this is done by using network ACLs (access control lists) to explicitly mention what traffic is allowed to enter or leave a subnet. A network ACL consists of a list of rules that specify which inbound or outgoing traffic is allowed or denied. It is created at the VPC level and can be associated with specific subnets. When you create a VPC, a default network ACL is automatically created. You can add custom inbound and outbound rules to this network ACL or even create your own custom network ACLs and associate them with specific subnets. Network ACLs help control the flow of traffic within a VPC between subnets as well as to/from the public internet. 

Here's what the default network ACL looks like:

Inbound






Rule #

Type

Protocol

Port range

Source

Allow/Deny

100

All IPv4 traffic

All

All

0.0.0.0/0

ALLOW

*

All IPv4 traffic

All

All

0.0.0.0/0

DENY

Outbound






Rule #

Type

Protocol

Port range

Source

Allow/Deny

100

All IPv4 traffic

All

All

0.0.0.0/0

ALLOW

*

All IPv4 traffic

All

All

0.0.0.0/0

DENY

It is configured to allow all incoming traffic to flow into the subnets, and all outgoing traffic to flow out of the subnets. The last rule whose rule number is an asterisk (*) is always included in any network ACL: it ensures that if a request doesn't match any of the other numbered rules, it's denied. You can't modify or remove this rule.

Here's an example of a custom network ACL:


Rules are evaluated in ascending order according to the rule number until a match is found. If no match is found then the final rule (*) is applied.


Security Groups
While a network ACL acts as a firewall that filters traffic to and from your subnets, a security group acts as a firewall for a specific EC2 instance to control incoming and outgoing traffic for that specific instance. It adds an additional layer of security for any of your resources that run on your EC2 instances, and allows you to specify inbound rules (to control incoming traffic to your instance) and outbound rules (to control outbound traffic from your instance).


Reference:
https://docs.aws.amazon.com/vpc/latest/userguide/infrastructure-security.html#VPC_Security_Comparison


By default, all outbound traffic from your instance is allowed and all in-bound traffic to your instance is denied. When you create an EC2 instance or after you launch it, you can modify the in-bound rules to allow certain types of traffic from certain IP addresses at certain ports. Here's an example of some inbound rules that can be edited from the AWS management console.


When you create an inbound rule, you must specify the following:

The protocol (e.g. SSH, HTTP, HTTPs, etc.), which defines the type of traffic allowed. For more info about these protocols, you can check 
here.

The range of ports to allow (depending on the protocol, the port number can be automatically assigned). For more info about port numbers, you can check 
here
.

The traffic source to allow for inbound rules (the IP address of the source).

Learn more
Control traffic to subnets using network ACLs

Create a security group

Security group rules


Endpoints (optional)
  There are two types of AWS services: private zone services and public zone services. 

Private zone services offer resources that need to be launched within a VPC. These services include Amazon EC2, RDS (Relational Database Service), ELB (Elastic Load Balancer), and EFS (Elastic File System). 

Public zone services offer resources that do not need to be launched within a VPC. Instead, these resources are accessed using public endpoints (i.e. public IP addresses). These services include Amazon S3, Amazon DynamoDB, AWS Lambda, Amazon Kinesis, and Amazon Athena.

You can use an internet gateway and NAT gateway to allow resources in  the subnets to access public services, just as they connect to the public internet. But what if you want these resources to only connect to AWS public services? You can still use gateways, but AWS offers another option: endpoints.

Interface endpoints can be placed in a public subnet or private subnet to allow resources in these subnets to connect to AWS public resources.

Gateway endpoints can be attached to a VPC to allow the resources in the VPC to connect to S3 and DynamoDB. (S3 can also be reached using an interface endpoint, but DynamoDB can only be reached using gateway endpoint). 

Learn more

What are VPC endpoints?


Final Remark
In these reading materials, we mentioned how resources can connect to the public internet or public resources. But we did not mention how two VPCs can communicate with each other. To learn more about this topic, you can read about 
VPC peering
.


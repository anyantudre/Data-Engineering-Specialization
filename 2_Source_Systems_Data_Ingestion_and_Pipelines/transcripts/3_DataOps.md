## DataOps


#### Overview
Welcome to week three. This week is all about dataops, but really it includes elements of the software engineering and data management undercurrents as well. As you learned in the previous course, Dataops is a set of practices and cultural habits centered around building robust data systems and delivering high quality data products. And DataOps really emerge from DevOps, which is the set of practices and cultural habits that allow software engineers to efficiently deliver and maintain high quality software products. This week, we're going to get into the details of each of the three pillars of data ops, namely automation, observability and monitoring, and incident response. Well, actually, to be fair, we're going to spend more time looking at automation and observability and monitoring, and not as much time with incident response. But that's not because incident response is less important. 
It's just that incident response has more to do with the cultural habits side of data ops, and it's harder to build practical exercises around that in an online course. Anyhow, when it comes to automation, we'll be revisiting some of the ideas covered in the previous course, like continuous integration and continuous delivery or CI CD, and then zeroing in on the concept of infrastructure as code, which is to say, writing code that when you run it deploys the resources required to run your data pipeline. In the previous labs in these courses, you've gotten some experience spinning up certain resources for your data infrastructure using the AWS console on the job as a data engineer, however, it's becoming common practice to do this deployment through infrastructure as code frameworks rather than manually spinning up instances and installing software. And in fact, you already have some experience with infrastructure as code in many of the labs you've already completed. We use infrastructure as code tools like CloudFormation and terraform to set up the lab environment and deploy various resources in your data pipelines. In the first lab this week, you'll get hands on experience actually writing the code in terraform to deploy your infrastructure. And after that, in the labs that follow, you'll build on top of your infrastructure to include monitoring of your data quality and other important observability metrics. 
Another exciting thing we have in store for you this week is a set of interviews with industry experts from the field of data ops. In these interviews, you'll be hearing about the key aspects of data quality and data observability from people who are actually building products to serve data engineers in these areas. So it's going to be an action packed week of materials to kick off this week, we'll hear from one of my good friends, Chris Berg, who is the co author of the DataOps Manifesto and CEO at Data Kitchen, a popular data observability and DataOps tool. And so the next video is a conversation I had with Chris, where you'll hear how he defines dataops and why he thinks this undercurrent is so important when building data products. Then after that, we'll look at the first pillar of DataOps, which is automation. 


#### Conversation with Chris Bergh
Hi, Chris. It's good to chat with you about DataOps. It's a topic that I think of an expert at DataOps in the field. You're the first one that comes to mind. So it's awesome to see you. It's awesome to see you. Thank you. 
Yeah, of course. I guess, maybe give a quick intro for people who don't know who you are. So, I'm Chris Berg. I run a company called Data Kitchen here in Massachusetts. And so I'm an engineer by training, built software for a bunch of years at different companies like a MIT Lcn Laboratory NASA, some Internet startups. And then I got the bright idea, like in 2005 that I should do data full time. And so I started to manage a team of, you know, what we now call data engineers and data scientists and people who did data visualization, and my life really sucked. 
Things were breaking. We can never go fast enough for our customers. And I fired a bunch of people. I probably shouldn't. I blamed people for problems when In fact, it really was a management problem on how I managed that team. And so the question that I've been working on for the past almost 20 years is, how do you get a team of technical people doing data to deliver things quickly, new insight quickly, how to deliver data and dashboards with really high quality? And then like, how to live, how to not have a painful life. 
And so I think it's great that you're teaching a course on data engineering, but we did a survey with data dot world two years ago with 700 data engineers, and something like 78% wanted their job to come with a therapists. And I guess maybe I'm cynical, but I was like, Oh, that's a little low. And I think it is a very difficult career to have caught between the Rock and Hard place of sort of insane customer demands and data and infrastructure that's just hard to live with. And so I came to them sort of ideas of DataOps sort of from that background. Interesting. I guess, for the learners, what is DataOps? How would you describe it? 
So, it's think of it as a method to make yourself or your team deliver insight to your customers where they can trust the data and the insight. And then you can make changes to that insight very quickly with low risk. That's really the idea is, can you build a factory and push out new datasets and new insights, that's perfectly perfectly good and change things in the factory as fast as you want? It seems to take inspiration in lean, it sounds like. Yeah, I think that's one of the things that surprised me when I started to do data in analytics, I was a sophomore guy, I'm like, Oh, I'm great. This data thing will be easy. And then you really are running a manufacturing line in a lot of ways. 
And if you think about a data set coming in and being put in a bucket in a database in multiple levels and then being used in a model and a visualization and then governance. There's each step along the way. You could kind of think of it as a manufacturing station. And then, you know, you don't have one, right? You have lots of them. And so how do you get that manufacturing line to put out Toyotas and not Pacers from the '70s? And that's a hard problem. 
That wasn't really in my software background, like how to run a manufacturing line. And then the other weird part of it is that people are always going to ask you for new things. You know, your job's done. And the best way to deliver new insight to a customer is get it 80% done, get some feedback, and then iterate on it. And months building something when you can get some feedback early. That helps very opposite things. Build a really good assembly line, but change the assembly line really quickly. 
And those are hard to do. Right. But if you go back to places like Toyota, they were able to swap out manufacturing in their sues in days, not months, which is the old mass manufacturing way of doing stuff, right? Yeah, there is. And I think that's how did they do it? They didn't do it by like buying fancy robots. I mean, they have robots, right. 
But the solution to that is really about how do you think about what your job is? And what things optimize. And so a lot of teams try to optimize on getting things done. Like, I have to get my work. Yeah. And the definition of done is really interesting, right? I've done my work, I've got my SQL. I'm done. 
And does it matter if someone used it? Does it matter if it breaks next week? I don't know. Done my job. And so, really, it comes down to it's about Toyota. But Toyota also has focuses on the customer. And how do you make customer successful? 
And unfortunately, the customers are not happy with data and analytic teams. And that's the you know, the dirty little secret is that most projects in data and analytics fail. And to me, it's not because we don't have cool tech. And it's not because the people aren't smart. It's because the systems that people work in are flawed. And so these principles that come from software and manufacturing, I think really apply. And they're not rocket science. 
They're just ideas that are laying there that you can pick up and use. Absolutely. So if somebody's new to data, so how should they think about this in their job? Well, I think about the thing that you do. I'm building I do some s. Then think about the thing around it. So number one, as I build some SQL, how do I build a test or a validation to prove that that works, right? 
And I get some new data in it. How do I know? Now, why do you want to do that? Because tests are the gift that you give to your future self. If you don't do that, you end up owning your piece of code forever. And you have to and if something goes wrong, they call you maybe on your vacation, or you get stuck owning more and more code. And it's really tempting when you first start to be a hero. 
To like, I'm a hero. I wrote all this stuff. I keep writing it. I respond really fast to my customers, and you go into full hero mode. And what happens is over time, you get burnt out, and you get unhappy, and you want a therapist. And so in one lesson of DataOps is be a hero, maybe two, 3% of the time. But the rest of the time, build a system so you don't have to be a hero. 
Right? And that's what it's about building a system next to your code, testing observability deployment. It's all things that are the environment around that squishy little cool code that you're writing that can make your life tractable. And then let's talk about the DataOps manifesto. So what inspired that? Like, seven, eight years ago, we went to a conference, and no one knew what we were talking about. And I explained I DataOps and they're like, what's that? 
I was like, Oh, man, this sucks. No one really gets what we're talking about. And so we had to so I wrote on the plane, I wrote the first version of the manifesto. I mailed it around to some people, got some feedback, I wrote the Wikipedia article. And we've had to write a lot over the years to talk about what DataOps is. We have a free certification. You can listen to me for three hours on our website. 
But really, it's this, This idea that that building systems of work is really important and solving your team for kind of a couple problems. Like, if you look at it, you hire a new 23-year-old in your team? They're new to data engineering, maybe they have a CS degree. What do you want to do with that person in their first two weeks? I think a good organization says, number 1, if something goes wrong, they should be able to find the source of the problem. Is it data? Is it the integrated data? 
Is it the model? Is it the Iz? Where's the problem? Or number 2, like, could they write some small could they tweak something like anything? Fix a bug, and get that into production quickly. And those things are really hard in a lot of data engineering teams. You have to learn things and it almost becomes an apprentice. 
And so, like, I think of what DataOps is. You've got two 23-year-old problems, how fast the 23-year-old can find a problem, how fast the 23-year-old can fix and deploy a problem into production with low risk. And then there's also probably a 46-year-old problem, which is like, how do you measure your team? And to me, DataOps is about those set of problems, which really they don't really have to do with data engineering, they have to do with how you manage a group of data engineers working together. And that's where I come from. And there is a whole engineering task. I mean, writing a good data quality validation test is important. 
Deployment speed is important, doing DevOps, managing environments, test data. All those things are really important. So there is skill in that. One question I'm sure the learners will have is, how does DataOps differ from DevOps? So I think In a lot of ways, they're the same idea. They're all like, how do you get a group of technically nerdy people to deliver things quickly with high quality? And you certainly can do it badly both ways. 
And so what Devops idea is is that you should deliver your work in small chunks. Like, don't work for months on something. Because if you do that, you're most likely going to miss what your customer wants. And so the main idea that's similar between DevOps and DataOps is, if your customer asks you for 10 things, and it's going to take you three months, deliver one of those things in a week. Don't wait and get all 10 things done, because most likely what's going to happen is you're going to get four things that they want. They're not going to want six things, and then they're going to say, no, I want three more. And so you have this waste of doing all this work that didn't really matter. 
And so I think the idea of DevOps is, how do you manage short cycle times of delivery, and how do you automate that? And I think there is a observability error reduction and production in DevOps. But I think with DataOps or whatever word you want to put on it, it's the same idea. It's that, you know, run things with flow errors and change it quickly and measure your work. And they're very much the same ideas. So, whether you call Lean or whether you call it TQM or whether you call it DevOps or DataOps, they're all the same thing. How do you get nerdy people to be productive? 
And to do that, you actually have to do some things. It's not just It's not just words. It's actually work. You have to build some stuff. You have to build the thing next to your environment. You have to build the manufacturing line. You have to build the machine that makes the machine. 
Well, thanks. It's been very educational. I guess to cap it off, is there anything that you wish you would have known about DataOps that you could tell your younger self and maybe the learners for this course? Is hope and heroism is like number 1, what I said before, don't be a hero. You're not doing yourself any favors. And then the second part is, don't hope that things will work. Like, just don't maybe it sounds paranoid, but don't trust your data providers, don't trust your servers. 
Measure and prove that things work, because both of which I've had so many problems with in my career, just hoping things will work or trying to be a hero. And you can build systems around that. So you can do, you know, have a little hope, have a little heroism, but it's better to build a high functioning organization that can do that. And if you do, your just life's going to be a lot better. And you don't want to end up in a situation where You're like that 80% of the people we surveyed and said, My job sucks. You know, are you a therapist, I'm so stressed. Well, thanks for the class, Chris. 
I we're very helpful for the learners. So thank you. 


#### DataOps Automation
In this video, we're going to look at the automation pillar of DataOps. And as I said in the previous video, DataOps emerged from DevOps. And while there are significant differences between the requirements for delivering high quality software applications and those for delivering high quality data products, there's still a lot of overlaps. And so here, we'll be looking at some of the concepts that DataOps borrows from DevOps when it comes to automation. Within software engineering, one key aspect of automation is the practice of continuous integration and continuous delivery, or CI/CD for short. In the context of software, the CI/CD process involves setting up systems for automatic review and testing of new code. And then the automatic delivery or deployment into production of code that has been reviewed and tested. 
When it comes to DataOps, the practice of CI/CD can be applied directly to code and data within your data pipeline, much as it would be for a software product. And so whether that's code for applying a particular set of data transformations or populating a database or the data itself. You can maintain it just like you would any other piece of software application code. When it comes to the automation of actually running your data pipelines, as I talked about in the previous course, this is something you could do in a number of different ways. For example, have no automation and just run all the processes in your data pipeline manually. Or you could set up the stages of your pipeline to run according to a particular schedule. Or you could orchestrate your pipeline by defining it as a directed acyclic graph, or DAG, using an orchestration tool like Airflow. 
We'll look at DAGs and the automation of testing and deployment more closely next week when we get into orchestration. Now, one key underpinning of any CI/CD system, whether for a software product or a data product, is version control, where each new code version of the code is recorded. This makes it possible to easily revert back to a previous version if for some reason the current version isn't working as expected or other problems occur. You might already be familiar with version control in the context of your own code, maybe using a platform like GitHub. And so within DataOps, the concept of version control also applies to data. Just like you can track changes in your code and roll back to a previous version, with DataOps, you can track changes in the data moving through your pipelines. And be able to roll back to a previous version of the data if you run into problems. 
Another concept that DataOps borrows from DevOps in terms of automation is infrastructure as code. Whether you're building software applications or data pipelines with cloud platform resources, it's possible to maintain the design of your infrastructure as a code base, just like you would for any other application code. You can run that code to deploy your infrastructure, or modify the code to redefine your infrastructure, and then run it again to deploy the updated infrastructure. Now, by defining your infrastructure programmatically using code, you can then maintain version control over your entire infrastructure, just like you would for any other piece of code or for your data. And that way, if you need to roll back to a previous version of your infrastructure, it's as simple as rolling back to a previous version of your code. And so there are a number of ways in which DataOps automation practices will be part of your work as a data engineer. You can start to see how DataOps begins to overlap with the other undercurrents of the data engineering lifecycle. 
So for example, DataOps is closely related to software engineering in the sense that many DataOps practices are directly borrowed from DevOps. And when it comes to the DataOps practice of maintaining version control over your data, this ties directly with the undercurrent of data management. In the sense that it allows you to deliver control and enhance the value of data throughout its lifecycle. Next, I'd like to dig deeper into infrastructure as code in the context of DataOps automation. Then in the next lab, you'll get a chance to practice writing code to define your own infrastructure. So join me in the next video for a closer look at infrastructure as code. 


#### Infrastructure as Code
As I mentioned in the previous video, you can use Infrastructure as Code to programmatically define, deploy, and maintain your cloud infrastructure. This means you can automate the creation of all the resources you need for your cloud data pipelines, including networking, security, computing, storage, and other data management and analytics resources. But the concept of Infrastructure as Code actually precedes cloud computing and has much older roots in configuration management, dating back to the 1970s. Even back then, engineers struggled to efficiently configure and manage a series of physical machines. They would write BAS scripts to automate some configuration tasks, and looking back, you can think of this as the sort of primordial roots of Infrastructure as Code. With AWS's release of EC2 in 2006, anyone could easily spin up cloud computing resources whenever they needed them, so engineers were able to build more scalable applications with many components and complex dependencies. In the early 2010s, software engineers developed Infrastructure as Code tools like Terraform, CloudFormation, and Ansible that allowed them to provision and configure their infrastructure using code-based configuration files. 
Nowadays, you can use these tools to easily manage infrastructure resources on the cloud with lines of code, rather than manually clicking through resource setup windows or writing tedious BASH scripts. In fact, you already gain practice using Infrastructure as Code tools in some of the previous labs. For example, in the first lab in the previous course, you saw this architecture diagram for the data pipeline you're building. Behind the scenes, we use CloudFormation to automatically create and configure all the necessary networking resources, including the VPC and subnets, and the RDS database, and the S3 bucket that you needed for this lab. And then you also ran some Terraform scripts to deploy other parts of this infrastructure, including the Glue ETL job and the Glue crawler. In this week's materials, I'll focus mostly on Terraform because that's what you'll be using in the lab later this week, but it's worth noting that CloudFormation is another popular Infrastructure as Code tool, and it's native to AWS. Terraform and CloudFormation are both well-supported tools and have great documentation. 
And depending on the organization you work in, you might work with one tool over the other, and in some cases, you might even use both tools, just like we have used both tools to support the labs in these courses. Now let's take a look at some specific Terraform configuration files that you ran to create the Glue ETL and S3 bucket components of your data pipeline. Let's zoom in on a part of the S3 configuration file to take a closer look. With this first block of code, you set up the S3 bucket. This bucket has a unique name with the specified prefix based on the variables defined for you in this lab. You use a second block of code to configure that bucket and provide public access to it. Notice that the language used in this configuration file is relatively easy to interpret. 
It follows a simple pattern where you start with the keyword resource, then you specify the resource type and the name you want to give this resource. So in this example here, where it says AWS underscore S3 underscore bucket, you're telling Terraform you want to set AWS as the provider and S3 as a resource you want to provision. You'll refer to this whole string as the resource type, and data lake is the name you want to give this newly created resource. Then within the curly braces, you can specify these configuration options using key value pairs. This configuration file is written in a domain specific language called HCL or HashiCorp configuration language, which is named for HashiCorp, the company that created Terraform. You can use the HCL syntax with Terraform to manage infrastructure resources across many cloud vendors. For example, here's how you can use HCL to create or update a VPC and an EC2 instance in AWS. 
Notice this code follows the same pattern you saw in the S3 bucket configuration file. And here's how you could provision a GCP compute instance using Terraform. As you see, again, the code follows a similar pattern. But in this case, you specify GCP as a provider. HCL is what's called a declarative language, meaning that you just have to declare what you want the infrastructure to look like. For example, what resources you want to create, what values you want the configuration parameters to take. This is also known as the desired end state of the infrastructure. 
Terraform will then figure out the exact steps needed to achieve this desired end state. This makes Terraform highly idempotent, which means that if you repeatedly execute the same HCL commands, your infrastructure will maintain the same desired end state as the first time you ran the commands. For example, let's say you run a Terraform configuration file that creates five EC2 instances. Terraform will first check the existing infrastructure to see if those EC2 instances with these specific configurations already exist. If they don't exist, Terraform will create the missing EC2 instances. If there are already EC2 instances that exist but don't match the configurations you specified, Terraform will update the existing EC2 instances to match the desired state. And if these exact EC2 instances already exist, Terraform will do nothing and let you know that it didn't make any changes. 
This is in contrast to the imperative or procedural language used in bash scripts and some configuration management tools, where you need to specify the exact sequence of configuration tasks. Using the same example as before, if you repeatedly run a set of commands to provision five EC2 instances using an imperative language, then you will create five new EC2 instances each time regardless of whether they already exist. And so like I mentioned earlier, in these courses, we'll be mainly focusing on Terraform when talking about infrastructure as code, because it allows you to manage infrastructure across many cloud providers. And it's a very popular tool among software and data engineers. Just note that there are other infrastructure as code tools out there. In the following lesson, we'll take a closer look at data observability and monitoring. But before that, I'm going to walk you through the steps of creating your resources in Terraform and then you'll get some hands-on practice using Terraform in the first lab of this week. 


#### Terraform - Creating an EC2 Instance
In some of the previous labs, you provisioned and configured some resources by applying Terraform configuration files that were provided to you. To create your resources in Terraform, there's a consistent workflow that you'll always follow. You first write the configuration files to define your resources, then ask Terraform to prepare your workspace. Terraform then installs the necessary files that enable communication with the cloud APIs and creates an execution plan describing the resources it will create, update, or destroy. Once you approve the plan, Terraform applies the proposed steps and provisions your infrastructure. In the lab this week, you'll get a chance to write and run your own Terraform code. So I prepared a series of tutorials to get you ready for that. 
In this first tutorial, we'll go into the details of how you can prepare your configuration files to create an EC2 instance. In the process, you'll learn more about the inner workings of Terraform and be ready to create any other resources in Terraform. And just a little warning before we jump into the details here. This is probably going to feel like a firehose of information, but just bear with me. You don't need to master all the details the first time through, and you'll get a chance to practice all this yourself very soon. So let's say you want to create an EC2 instance and launch it in the default VPC of your selected region, as shown in this diagram. In this example, I'm going to go with the West Virginia region, or US East 1. 
I'm choosing to launch the EC2 instance in a default VPC here for simplicity. But recall that in week 1 of this course, Morgan said that you typically want to launch resources in your custom VPCs and reserve the default VPC for quick experimentation only. Let's start with creating the configuration file to define this EC2 instance. To write your configuration files, you can use any IDE of your choice. Install Terraform in your environment and use your AWS credentials to authenticate Terraform. You can find more information about those steps on the Terraform website, which is linked in the resources section at the end of this week. In the lab, this will all be set up for you. 
You will just have to follow the instructions and use Terraform right away. Here I'm using VS Code as the IDE. I already installed Terraform and created this folder that represents the root directory where I'm going to save the configuration files. So let's go ahead and create the first configuration file and call it main.tf. Any file that has .tf as its extension will be recognized by Terraform as a configuration file. You can structure your configuration files as 5 sections. You specify the Terraform settings and providers in the first two sections. 
Then you'll define all the resources you want to set up in the next section. And you can optionally define any input variables and output values in the final two sections. You'll then create blocks of code to represent entities within each section. In Terraform, a code block has a JSON-like structure that starts with a keyword that tells you what type of block it is. Then the block can include labels as strings if needed, depending on the type of the block. Each block has a body delimited by these curly braces. Within the body, you can define the block arguments or further blocks. 
Let's start with the first section. The Terraform block you see here specifies the Terraform settings, including the required providers that Terraform will use to create your resources. I'd like to pause for a moment to say a little bit more about providers because I feel this can be a bit confusing the first time you're using Terraform. So in the language of Terraform, a provider is a plugin file or a binary file that Terraform needs to install in order to interact with external resources. You can find all the available providers in the Terraform registry as shown here. Some providers allow Terraform to interact with the cloud platform, and others are utilities providers that allow you to use additional functionalities in Terraform. So just to be clear, in Terraform, the word provider is not a reference to the cloud provider where you're setting up resources. 
Instead, a provider in this context is that plugin or binary file that Terraform needs in order to interact with external resources. Let's click on the AWS provider. And again, AWS provider here is not referring to AWS as the cloud provider. Instead, this is the file that allows Terraform to interact with resources on AWS. So here you can see the version of the provider, the source code, and a link to its documentation. If you click on the documentation, you'll see a list of all the resources available by this provider and examples of their usage. When working with Terraform, this documentation can be really helpful because it shows you the arguments that you need to specify, in this case for any AWS resource. 
Now let's go back to the code. Since we're only using AWS resources in this example, you need to declare only the AWS provider as a required provider. For each provider, you need to specify a local name, the source location, and optionally a version constraint. The local name is the unique identifier that you can use everywhere in the configuration file to refer to the provider. For example, here I'm using AWS as a local name for the AWS provider, which is the preferred name that is used in the AWS provider documentation. The source, which is HashiCorp AWS, is the global identifier of the AWS provider. It specifies where Terraform can download this provider when you run this configuration file. 
You can also set a version constraint for Terraform itself by specifying the required version within the Terraform block. In the next section, you can create a provider block to configure the providers that you just declared. So in this provider block here, I've specified the AWS region. Note that the name that I used next to the provider keyword is AWS, which is the local name I assigned for this provider within the Terraform settings block. I know this feels like a lot of details, but don't worry. You can always go to the documentation for your specific provider and copy the code for the Terraform settings and providers blocks. Next let's define the EC2 instance. 
In the resource block, you start with the keyword resource. Then you specify what is called the resource type, which is a string that contains a provider and the resource separated by an underscore. So the prefix AWS refers to the AWS provider we specified earlier, and the resource type AWS underscore instance refers to the AWS EC2 instance that I want Terraform to manage. You can always search for the resource type in the Terraform documentation of the AWS provider to find the name of the desired resource type you want to configure with Terraform. The next string here represents the name you chose to give to this resource. Together these two strings form a unique ID, which you can use to reference your resource and other blocks of your configuration files. For example, you can refer to this EC2 instance using AWS underscore instance dot web server. 
Now inside the resource block, you need to specify the arguments of the resource. And again, you can find the list of the arguments for each resource in the documentation. So for example, for the EC2 instances, you have this long list of arguments. But just like when you're writing Python code, you don't need to specify every single argument as most of them are optional. The two required arguments for the EC2 instance are the AMI and the instance type. The AMI is a software template that contains information about the instances operating environment, such as the operating system and system architecture. AWS provides a long list of AMIs that you can find in the AMI catalog in the AWS console. 
In this example, I grabbed the ID of the most recent Linux based AMI. For the second argument, I'm using T2 dot micro as the instance type. I also set the optional tag to give the instance name example server. Note that I did not specify the subnet in which I want to launch the EC2 instance. And this is because I want to launch it in any subnet of the default VPC. What you see here in this configuration file is enough to create an EC2 instance. So let's actually create this EC2 instance. 
In the example here, I'm going to start with the Terraform init command. When you run this command, Terraform installs the providers defined in the configuration file. So in this example, Terraform downloads the AWS provider and stores it in a hidden subdirectory named dot Terraform. Because Terraform has been successfully initialized, you can run the Terraform plan command. When you run this command, Terraform creates an execution plan that details what Terraform is planning to create, update, or destroy based on your configuration files. So in this case, the plus signs mean that Terraform plans to create all of these components. In other cases, you might see a minus sign, which indicates things that will be destroyed, or a tilde symbol, which indicates things that will be updated. 
Finally, you can run the Terraform apply command. Terraform will show you the execution plan again, but then stops to wait for your approval. So here I'm going to type yes and wait for the creation of the EC2 instance. And there you go. Terraform just created the EC2 instance based on these configuration specifications. If you want, you can check the instance in the console. So in this first tutorial, you saw how you can set up your configuration file with the Terraform and provider blocks, then how you can create an EC2 instance using a resource block. 
Join me in the next video to learn more about other blocks in Terraform and how you can better organize your Terraform directory. 


#### Terraform - Defining Variables and Outputs
In this video, I'd like to continue working on the same Terraform configuration file we started with in the previous video. Here we'll look at how to create input variables for your configuration, how you can export your resource information using output values, and how you can organize your Terraform workspace. In the previous video, I incorporated some hard-coded values inside the configuration file, such as the region name and the EC2 instance name. Instead of hard-coded values inside your code blocks, you can create input variables to parametrize your configuration. The use of input variables allows you to customize your infrastructure and specify different values for your variables when you want to create your resources, instead of manually editing your configuration files. In this example, under the Input Variables section, I'm going to create one variable that represents the region name, and another one that represents the server's name. You can declare these variables like this using the variable keyword. 
Here the strings region and serverName represent the name of each variable, which acts as identifiers for the variables inside the Terraform configuration file. Each variable has three optional arguments, a description that documents the purpose of the variable, the type of the variable, and a default value assigned to the variable. If you don't assign a default value for the variable, then you'll be prompted to specify its value before Terraform applies the configuration. Now to use these variables in other blocks, you can refer to each variable using the syntax var.variable.name. So here in the Provider block, I replace the hard-coded value of the AWS region with var.region. And in the Resource block, I replace the hard-coded value of the instance name with var.server.name. To automatically assign values to the variables without being prompted, you can use the command line flag –var as shown here. 
Or more conveniently, you can define the values of the variables in a specific file that has .tfvars as its extension. So here in the same directory that contains the configuration file, I'm going to create the file terraform.tfvars, and inside this file, I'm going to assign a value to the variable server.name. Now to update the file, you can just use the command terraform apply. Terraform will use the file that ends with .tfvars to extract the values of the variables and update the configuration with the provided values. Since I assigned the same name for the EC2 instance, Terraform did not find any changes that needed to be updated. Now any resource that you create will contain a list of characteristics or attributes. For example, you can check the documentation of the EC2 instance. 
You can see a list of all of its attributes, such as the id instance, its ARN, which stands for Amazon Resource Name, and its public IP address. In some cases, you might want to export these attributes. For example, you could print them in the command line, use them in other parts of your infrastructure, or reference them in other Terraform workspaces. To do this, you need to declare them as output values. So I'm going to create two output values to export the id and ARN of the EC2 instance. The strings server underscore id and server underscore ARN that come after the output keyword represent the names or the identifiers for these output values. For each output value, you need to specify the value argument by assigning it to the attribute of the EC2 instance. 
You can access the attributes by specifying the identifier for the resource, which is the resource type dot resource name, then using the dot attribute syntax to get the value of the attribute. So in this example, to access the id attribute of the EC2 instance, I'm using AWS underscore instance dot web server dot id. I did the same for the ARN attribute. Now let's see how you can access the output values in the command line. First, let's apply the updates by running the Terraform apply command. Here you can see that Terraform detected the changes to outputs and it's telling you that it's going to create those output values. So here I'll type yes. 
You can see the outputs displayed in the Terraform message. After you create the outputs, you can use the Terraform output command to query all of them. Or you can query an individual output by specifying its name. Right now I have several blocks in this configuration file where each block has a different purpose. But what if you have more than one resource with several input variables and several output values? If you declare all the blocks in one file, then at some point managing the file will become very cumbersome. The better practice is to split this configuration file into several files like I'm showing here. 
You declare all your input variables in variables dot tf, all your output values in outputs dot tf, all of your providers in providers dot tf, and all of your resources in main dot tf. You can also further divide main dot tf where you declare each resource in a separate tf file. Terraform will automatically concatenate all the files that end with dot tf as if you've written all of them in one file. So here I'm going to create three new files, variables dot tf, providers dot tf, and outputs dot tf. Then I'm going to move the variables to the variable file. The output values to the output file. And finally the Terraform settings and provider blocks to the providers file. 
And so creating input variables and organizing your workspace in Terraform like this helps you maintain your infrastructure. And in the next video, I'll show you how to use modules to organize your workspace and how to declare data sources. I'll see you there. 


#### Terraform - Defining Data Sources and Modules
In addition to the resource variable and output blocks that you saw in the previous videos, you can also declare data blocks in Terraform. You can use data blocks in your configuration file to reference resources created outside of Terraform or in another Terraform workspace. Terraform refers to these resources as data sources. You can check the provider documentation in the Terraform registry to see how to declare these resources. You'll notice that for each resource available in the provider, you can declare it in Terraform either as a resource or as a data source depending on whether you want to create that resource or read from an external resource. Let's go over two examples that use the data block. In a previous video, I created a configuration file that launches an EC2 instance inside the default VPC. 
But now let's say you want to launch the EC2 instance inside a subnet of a VPC that you already created outside this current workspace. To access a subnet, I'm going to declare a data block as shown here. Similar to the resource block, next to the data keyword, I'm going to specify two strings. The first string represents the resource type, which you can get from the provider's documentation. And the second string is a name you choose to give to this data source. You can now refer to this name throughout the configuration file. Inside the data block, you need to specify the arguments that identify the subnet that you want to use. 
Make sure to check the documentation in order to know what arguments this data source expects. Here I am assuming that I know the ID of the subnet, and I'm assigning it to the id argument. Now that the data source has been declared, you can use its attributes. So for instance, to use the id attribute, you start with the keyword data, then specify the resource type and name for this data source, then give it the attribute you want to access. So in this case, you can use data.aws__subnet.selected__subnet.id to get the ID of this desired subnet. Inside the resource block of the EC2 instance, there's this subnet__id attribute that I can specify using this expression in order to launch the EC2 instance inside the desired subnet. Now, since you already know the subnet ID, you could have, of course, just referenced it directly in the resource block. 
But here I wanted to show an example of how a data block could be used instead, because in some cases you may not directly have the subnet ID, and so in the data block here, you may use other arguments to identify the desired subnet. As another example, you could also use a data source to automatically identify the AMI argument of the EC2 instance. In the first video, I explained that I grabbed this AMI from the AWS console. If I want to automate this process by asking Terraform to do the search on my behalf and retrieve the latest Linux AMI, I can use the following data block. So here I'm asking Terraform to find the most recent AMI, owned by Amazon, that has a specified system architecture, and that is Linux-based. And now, inside the EC2 instance resource block, you can specify the AMI as shown here. Again, you start with the keyword data, then you use the resource type and name of this data source. 
And finally, you specify that you want to access the ID attribute. Let's now update the configuration by running the terraform apply command. You see that Terraform is planning to destroy the previous instance to create a new one with the updated network settings. You can also see that it performed the search to find the latest AMI. So now I'm going to type yes and wait for the updates to take place. The last topic that I'd like to cover in this tutorial is the use of modules in Terraform. A module is a subdirectory inside your main directory that you can use to group resources that are used together. 
So you can think of it as a way to package those resources. In this example, I'm going to create this website module to group together all the resources that are used to create the website. So let's move the file main.tf that contains the definition of the web server into the website module. A module is like a regular root directory, so it also expects you to declare the providers, the input variables, and the output values inside of it. So I need to move those files into the website module. The module acts as a folder that contains the resource, the input variables, and output values. So you can't directly access this information in the root directory, meaning that if you run terraform apply now, it will give you errors because the root directory won't be able to see the values assigned to the input variables. 
To solve this, you need to declare a module block to call the module in the root directory, as shown here. I chose a website as the module name and specified the source argument, which is the module directory. Now inside this module block, I can include the input variables of that module with specific values. Here I need to specify a value for the input variable server underscore name. And for that, I'm going to create this variables.tf file in the main directory and declare the server underscore name underscore root variable. Then back over in the module block, I can assign to the module's input variable the value of this server underscore name underscore root variable. And then finally, in the tfvars file of the root directory, I need to specify the actual value of the server underscore name underscore root variable. 
And finally, if you want to export the module's outputs server ID and server ARN also as outputs in the root directory, you need to create an outputs file inside the root directory. In this file, you declare the outputs and assign them to the values of the module's outputs using this syntax. You start with the word module, then you specify the name of the module that contains these output values. And then you specify the name of the output values you want to access. Whenever you add, remove, or modify module blocks, you need to rerun Terraform init to allow Terraform to install new modules or adjust the installed modules. So here, I'm going to run Terraform init, then Terraform apply. We'll type yes and wait for the updates to take place. 
And that's it for this video series on Terraform. I know that was a lot, but don't worry, you can always refer back to these videos to review the details. I've also included some optional reading materials after this video that contain more Terraform configuration examples. I'll see you in the next video for a quick walkthrough of the upcoming lab. 



#### Data Observability

en
​
Interactive Transcript - Enable basic transcript mode by pressing the escape key
You may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key
In addition to the principles of automation that data engineers borrow and build on from DevOps, the principles and practices of observability and monitoring are a key pillar of DataOps that also has its origins in software development. Over the past decade or so, with the advent of the cloud and the move toward distributed systems, software engineers have developed observability tools help their teams gain visibility into the health of their systems. With these observability tools, teams are able to monitor metrics such as CPU and RAM usage and response time, which helps to quickly detect anomalies, identify problems, prevent downtime, and ensure reliable software products. When it comes to data observability and monitoring the health of your data systems, some of the same tools that software teams rely on can also be helpful to you as a data engineer. With that being said, in addition to monitoring for things like CPU usage or system response times, as a data engineer, you also need visibility into the health, or in other words, the quality of your data. As a reminder, in the previous course, we defined high quality data as data that is accurate, complete, discoverable, and available in a timely manner. And beyond that, high quality data represents exactly what stakeholders expect it to represent in terms of a well defined schema and data definitions. 
Low quality data is the opposite,, and it might be inaccurate, incomplete, or otherwise unusable. So when you are able to provide high quality data to stakeholders in your organization, you're providing value for those stakeholders. But believe it or not, providing low quality data is actually worse than providing no data at all. When business decisions get made based on low quality data, it can be extremely costly for an organization, and it can leave stakeholders second guessing the value of the data team. To complicate matters further, data systems that provide poor quality data might look perfectly healthy on the outside. For example, let's say you have a software application like a mobile app or a website. If the app crashes, or if you get a 404 error when trying to load a webpage, it's pretty obvious that something has gone wrong. 
And those are the kinds of things that can trigger an alert to a software engineer so they can go fix the problem. With the data system, if your system simply stops working and you're able to recognize immediately that it's not working, that's actually your best case scenario in terms of potential failure modes. In this case, you can debug the problem and get your system up and running again. If, on the other hand, your data suffers a breaking change such that your system still works but is no longer providing high quality output, that's when things can get really ugly. To better understand what I mean, let's look at a hypothetical scenario. So imagine, for example, that you're a data engineer at a us based company that sells certain products in Europe. And suppose that you're ingesting product sales data and serving data for downstream analytical use cases. 
Now, suppose for some reason, the team that manages the transactional database where product sales are recorded, they decide to start recording the sales price in euros instead of dollars. So your system still works and you're still serving data that's being used to populate analytics dashboards. And the numbers in those dashboards might even look plausible. But now it looks like there was a sudden drop in revenue of maybe like 10 or 20%, depending on the conversion rate from dollars to euros that day across all sales in Europe. This triggers an emergency meeting of the leadership team, which leads to an all hands on deck effort to resolve the issue. Okay, so maybe that sounds like a silly story, but I can tell you that some version of the story, in some cases a much uglier version, has played out across countless organizations worldwide. And so in terms of data quality, this would be an example of where, in the blink of an eye, your system went from serving high quality data to serving data that was no longer accurate, or at least was not what stakeholders expected it to be. 
Now, of course, you could argue that this was not your fault, that the source system owners shouldn't do such things, or at least that they should have alerted you to such a change that would affect your data. But as I said, throughout these courses, upstream changes that disrupt or break your data systems should be expected and mitigated. Really, no matter where it comes from, once the data is in your hands, it's your responsibility to ensure its quality. So when there's a disruption to your data system, how do you make sure you know what happened as soon as possible? Well, that's where data observability and monitoring comes in. Next up, you're going to hear from a true expert in this field, my friend Barr Moses. This next video is optional, so feel free to skip ahead. 
Otherwise, join me in the next video for a conversation with Barr Moses on the topic of data observability and monitoring. 


#### Monitoring Data Quality
By this point, I'm guessing you're feeling pretty well convinced that data observability and monitoring are important. But when it comes to actually doing it, where do you start? Well, that's what I'd like to talk about in this video. And like a lot of the other things we've been discussing so far in these courses, it all starts with what the stakeholders need. Practically speaking, there's a wide range of metrics or quality criteria you could decide to monitor when it comes to your data pipeline. For example, you can monitor something like the total number of records adjusted in each batch, or over some time interval, or whether the range of values in a particular column stays within some thresholds. Or you could count the total number of null values in a table, or the difference in time between now and the timestamp of the most recent record in your data. 
Or, well, you get the point. There are a whole bunch of things you could decide to monitor. However, rather than setting up monitoring and alerts for every conceivable thing you could measure or observe about your data, you'll want to identify the most important things and focus on those. If instead you try to monitor every imaginable aspect of your data, you can end up creating confusion and alert fatigue, and the really important stuff will get lost in the noise. And so, when it comes to deciding what metrics or aspects of your data to monitor, the first question should be what do stakeholders care about the most for this particular use case? For example, stakeholders might care most about looking at current data, maybe data that's no more than 24 hours old. And in this case, you'd want to monitor the so-called freshness of the data by measuring when the latest records were ingested and verifying that is within the expectations for the project. 
When it comes to the data being accurate and complete, it's probably fair to assume that these things are important for all projects. So to monitor these aspects of quality, you'll need to identify which components of the data are most important and which matter less or are safe to ignore. For example, if your stakeholders are interested in product sales revenue, it's critical that the purchase amounts recorded in the data are accurate. That you can verify that all sales records were successfully ingested, and that they don't contain null values. By contrast, it may be less important that all the product SKU numbers match the product descriptions, or that each product's postal code was recorded correctly. As you can imagine, the critically important aspects of your data will vary from one project to the next. Throughout these courses. 
I've been emphasizing that you should communicate with source system owners to be sure you understand what kinds of changes you'll need to anticipate or mitigate in the future. While there's no replacement for good communication, you should also take steps to build in checks or tests in your data monitoring to verify that things like the schema and types for the data you are ingesting stay consistent. If all goes well, these can just be nice sanity checks to ensure the data you're ingesting is still in the format you expect. But these checks can also help to identify problems early, before they are propagated further down your data pipelines. Like a lot of things in data engineering, there are many different ways to monitor your data quality. You could do some things manually, or maybe write some custom code to perform a set of tests or trigger alerts. Those approaches might make sense in certain scenarios, like when you're first setting up or prototyping your pipelines. 
But nowadays there are a number of tools you can use to ensure data quality and also spare you from any undifferentiated heavy lifting. And so, up next is a conversation with my friend Abe Gong, one of the creators of great expectations, which is an open source tool that you will use for testing data quality in the next lab. This is an optional video to give you more context about this tool. You can also jump ahead to get started using great expectations yourself. Otherwise, I'll see you in the next video for a conversation with Abe Gong. 


#### Conversation with Abe Gong

en
​
Interactive Transcript - Enable basic transcript mode by pressing the escape key
You may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key
I'm here with Abe Gong. You're the founder of Great Expectations? Co founder and CEO. Co founder and CEO. Awesome. Here's my dog, too. Cameo appearance. 
So walk me through this. We'll talk about data quality in a bit, but I'm also interested in Great Expectations as a project. Where did this inspiration come from? It came from a lot of long nights dealing with data quality issues. More specifically, I've been a part of data science, I don't even know what to call, like data science friends group with a few people, including James Campbell, who's the other co founder of Great Expectations. There was one month where I came and said, I've got this idea for a open source project. You should really be able to test data pipelines the same way you'd test software. 
James said, that's funny because I came to this meeting thinking I would pitch a project where you could test data pipelines the same. We came to this meeting with exactly the same idea. Oh, interesting. And actually, when we started off, I had, like, very similar sensibilities about, like, what is the right way to design this tool so that it'll be deployable in lots of places and easily integrated with data exploration. Interesting. Because I recall that you were running a healthcare data consulting company, is that right? That's right. 
Healthcare data for the learners in this course, healthcare data, I think is notoriously gnarly and hard to work with. I think all data data is gnarly to work with. In healthcare, the stakes are high. There's regulatory issues, and depending on the data, there might be actual health issues on the line. So for the learner, then, you created this tool and this approach sort of out of necessity. But I guess to back up, what is data quality? How do you know that your data is of quality? 
I think that's an awesome, like philosophical question. If you go out and look, you can find, like, 20 different things that all have the 10 dimensions of data quality and they're all different, and it's all over the place. Fundamentally, I think data quality it has to be grounded in some notion of fit for purpose. You're going to do something with the data. You're going to try ask certain questions of it or build a machine learning model or whatever it is. Data quality is whatever it takes to make it so that that thing works reliably consistently, and you know the results are trustable. Depending on the data system, it could be pretty different. 
If you have a dashboard that's reporting routine metrics, it's going to be important to make sure that the up stream data is being logged appropriately. It's being cascaded through and aggregated in the right way that you don't have too much time lag, like those are the concerns that will show up there. If you're training a machine learning model, you're going to think much more about distributions and bias and so on. I think you have to, there are these open ended discussions of data quality, and I think you can get good things from that, but ultimately, it's whatever makes it so that you can really build a reliable product or process on top of the data. That makes a lot of sense. I think what was it there was in the '90s or whatever, there was a paper that came out of there, what it 20 dimensions of data quality or something like that I can't remember. I think some people would like to think of that as sort of gospel truth, but I like what you're saying a lot more, which is just fit for purpose. 
Can I use the data in a believable, useful way or not. How does something like Great Expectations help I guess alleviate a lot of data problems? It's basically the same philosophy as software testing with a few twists that are specific to data. I don't think this is controversial at this point. If you went back 20 years, it might have been. But a big part of the DevOps movement is you need to have reliable tests for anything that you're building in software. If it's more than a prototype, you can't guarantee that it's going to continue to work unless you have the right infrastructure around to do automated testing. 
We did the same thing for data. Each expectation makes it so that you can assert something about the data that should be true at a specific moment in a specific place in a pipeline. For the learners in this course, we're going to be having a lab on Great Expectations. What are some of the main things you feel that a learner should know about how to properly use Great Expectations? I'm not totally sure where to begin there because it goes to the fit for purpose thing. It depends. Actually, maybe this is a useful thing to say. 
Great Expectations is super flexible. You can deploy it in lots of different places. It's actually a thing that has been challenging for us as the project has grown is providing documentation that will help people find their use case quickly and do the right thing with it. Interesting. Are there any anti-patterns, I suppose, maybe to fit the question, things that you would suggest people avoid or not do? Not so much an anti-pattern, but just like questions to ask yourself as you're getting started. We were just talking about different stakeholders who we're going to need to work with data requirements. 
That's actually a really useful segment in question. I would ask yourself, who is going to need to work with these expectations, these requirements? If it's just you and just your team, then very likely, you want to have that very tightly combined with the other tools that you have, store things and get and so on. On the other hand, if you're going to need to collaborate with other stakeholders, that they're going to need to give input and update things, and business process is going to change, and you don't want to be a middleman for keeping the expectations in sync with that, you need to find some other way to do that. Got it. Some teams have done that with Notebooks. I don't know how promotional I'm allowed to be here, but we're launching Great Expectations Cloud soon, and it'll comes with a lot of those things. 
That's cool. It'll make it a lot easier for teams to use then. Totally. Well, because asking somebody who might be fluent in Excel and really understand a business process, but who doesn't speak Python or SQL or JSON asking them to review things and get like a nonstarter. But they still need input to the data a lot of the time. Make sense. Cool, anything else that learners should know about data quality or Good Expectations? 
I'm just going to say if words starting off in the data world, and you haven't yet built a system and had to maintain it, just wait. Data quality is really important. It will bite you at some point if you don't get ahead of it. It's one of these things that you can neglect up front, but you don't want to? Yeah, so for the learners out there, data quality, take it seriously. Look forward to seeing how you like Great Expectations in the lab. Awesome. Well, thanks, Abe. It's good to chat with you, and thanks for everything. 
Thanks, Jack. This is awesome. Thank you. 


#### Great Expectations - Core Components
In the next lab, you'll use Great Expectations to apply data quality tests. But before we go there, I'd like to give you an overview of the core components of Great Expectations with an example workflow. When working with Great Expectations, you typically start your workflow by specifying the data that you wish to test. Then you define the expectations or the tests that you want to perform on the data, and finally, you validate your data against your expectations. To implement such a workflow, you need to interact with the core components of Great Expectations, which consist of data context, data sources, expectations, and checkpoints. You use these components to access, store, and manage the objects and processes that are needed in your workflow. To start your workflow, you first instantiate a data context object. 
A data context serves as the entry point for the Great Expectations API, which consists of classes and methods that allow you to create objects to connect to your data sources, create expectations, and validate your data. Using the data context, you can configure and access the properties such as objects, as well as the metadata of your Great Expectations project. After you instantiate your data context object, you need to declare your data source object, which tells great expectations from where to get the data that you want to validate. The source of data could be a SQL database, a local file system, an S3 bucket, or even a pandas data frame. After you connect to the data source, you need to tell great expectations which part of the data you need to focus on. You do that by declaring from the data source your data assets. A data asset is a collection of records within a data source. 
It could be a table in a SQL database or a file in a file system. It could also be a query asset that joins data from more than one table, or it could be a collection of files matching a particular regular expressions pattern. You can further partition the data in your asset to batches. For example, if your data asset represents the records that correspond to the sales of a given year in a table, you could partition the records into monthly batches and validate each batch. Or you could partition your data with respect to the store ids. You could also work with all the records of your data asset as one batch for you to retrieve the batches of your data asset. Whether it's one batch or multiple batches, you need to create from your asset a batch request object. 
Batch requests are the primary way to retrieve data from the data asset, and it's what you need to provide for the rest of Great Expectations components. Next, you need to define your expectations. An expectation is a statement that you can use to verify if your data meets a certain condition. For example, you can define an expectation to check if a column does not contain null values. You can define your own expectation or use one of the available statements from the expectation gallery. For example, expect column min to be between, expect column values to be unique, and expect column values to be null are all examples of tests that you can directly use. You'll see how you can call them in the workflow example that we'll work on. 
You can also define more than one expectation for your data asset and collect them in an expectation suite object. Now, to validate your data, you need to create a validator object which expects a batch request and its corresponding expectation suite. You can manually validate your data by interacting directly with the validator, or you can streamline the validation process by using a checkpoint object. A checkpoint takes a batch request and an expectation suite and automatically provides them to a validator which generates the validation results. Throughout this process, metadata about your project will be generated and great expectations will save it in some backend stores. Great Expectations supports different types of stores. The most common stores are the Expectation Store, where you can find your expectation suites. 
The Validation Store, where you can find information about the objects generated when you validate data against the expectation suite. The Checkpoint Store, where you can find your checkpoint configurations. And a data docs store, where you can find reports on expectations, checkpoints, and validation results. You can access these stores and their settings through the data context object. So here are the steps of a typical Great Expectations workflow. In the next video, we'll apply these steps on an example dataset. 


#### Great Expectations - Workflow Example
In the previous video, you've learned about the core components of grade expectations and what a typical validation workflow looks like. Let's now apply these steps on an example dataset, which is the DVD rental Database, you've seen in one of the labs of week one of this course. We'll just focus on checking the columns of the payment table. In particular, we'll check if the payment id column contains unique ids, the customer id column does not contain null values, and all the values in the amount column are non-negative. I already set up a postgresSQL database locally in my machine and loaded the data into the database. In the terminal I'll pip install great expectations. Then, to start the great expectations project, I'll type great expectations init. 
This command will initialize the data context object, set up the structure of your project folder as shown here, and create your backend stores such as the checkpoints, expectations, data docs, and validation stores as local directories. I'll type Y to proceed, you can always change the location of your backend stores. So for example, in the lab you will configure your stores as s three buckets. In this video ill keep these as local directories. Now, to interact with the components of great expectations, I'll launch a Jupyter notebook in the same root directory and create this notebook file, example. Here in the notebook file, I'll first import the great expectations package and then call the method get context, to get the context object of the project. Using this object, you can connect to the data source, define your expectations, create a validator, and then run your checkpoints. 
So let's use this context to first create the data source object. Create expectations provides different methods that allow you to connect to your different data sources. To connect to my local SQL database, I'll call the method context sources add sql, and then choose my datasource as the name for the data source. This method also expects a connection string that includes the information needed to connect to the database, such as the username, password, hostname, port number, and the name of the database. Here's the format of the string that you can use to connect to a postgresql database. Using this format, I'll create the connection string to my database using the information of my local database. Next, from the data source, I'll create the data asset by calling the method add tableasset. 
Since we're only focusing on the payment table, I'll choose the name for the asset as payment tb and then specify the name of the table within the source database, which is payment. In this case. Now, if you want to create batches on your asset, great expectations provides a set of methods that allows you to split your data asset based on the date or a column value. So for example, I'll call here the method add splitter datetime part on the asset that I just created, and then indicate the name of the column that contains the date, which is payment date, and specify month for the second argument datetime parts. And finally, I'll create the batch request object by calling the method build batch request on the asset object. Before we proceed with the workflow, let's take a quick look at the batches. Using the asset object, I'll call the method get batch list from batch requests and pass in the batch request object I just created and then iterate over the batches to check the specification of each batch. 
You can see the data contains four batches where each batch corresponds to one month. Now that we have the batch request object created, let's define the expectations. I'll do that interactively by directly working with a validator. First, I will create the expectation suite which will contain all the expectations that it will define. On the context object I'll call the method add or update expectation suite and choose the name mysuite. Now, to create the validator, I'll use the context object to call the method get validator and pass in the batch request object and the name of the expectation suite. Now, using the validator, you can call any of the expectation methods from the expectation gallery. 
So here I'll call expect column values to be unique to check the uniqueness of values within the column payment id. The results are shown here, correspond to the last batch. However, all batches were tested and since we reached the last batch, it means the test ran successfully on all the other batches. I'll now define two more expectations. Expect column values to not be null on the column customer id to check that the column does not contain any null values, and expect column min to be between on the column amount to check that all values in the column are null negative. You can see that both tests run successfully. Now these three expectations that I created interactively while working with the validator are just available for this session. 
In order to use these expectations in other sessions, you need to save them. Using the validator, I'll call the method save expectation suite and specify for great expectations to not discard failed expectations, this method saves the expectation suite, my suite containing the three expectations in the expectations store. Now, let's automate the process of the validation using the checkpoint object. To create the checkpoint object, I'll call the method add or update checkpoint on the context object and assign the name mycheckpoint to it. This method expects validations as a second argument which consists of a list of pairs of a data batch and its corresponding expectation suite of to specify these validations I iterated through the batches and for each batch I extracted the batch request and used the name of the expectation suite. I'll now run the checkpoint by calling the method run, the test on the four batches passed. To get more detailed information about each result, you can check the data docs by calling the method build data docs using the context object. 
This method returns a link that I'll open to check out the data docs. Here you can find the results of the validations performed on each batch. You can see that all tests were successful. If you click on any row, you can find statistics about the evaluated expectations as well as the successful and unsuccessful expectations. You can also find the expectations performed on each column and the corresponding result. So let's click on show more info. Here you'll find some metadata about the checkpoint execution and the batches used. 
Now, let's go back to the homepage and click on the expectation suites tab. Here you can find information about the expectation suite, my suite. And there you have it. You now know how to interact with the core components of grade expectations. Now, it's your turn to practice using great expectations and validate some data in the lab. 


#### Amazon CloudWatch
You've been learning from Joe and others this week about the importance of monitoring and observability when it comes to the health of your data systems. In the upcoming lab, you'll be using Amazon CloudWatch to monitor database on AWS. Here I want to show you some of the details of how CloudWatch works, so you'll be ready for the lab. Monitoring the health of your data systems means ensuring that the various components within your data system are operating as expected. If some component of your system is having issues, successful monitoring means that you are made aware of any problems as soon as they arise or even anticipate and correct issues before they happen. When you're working with resources on AWS, a lot of them automatically start posting metrics to CloudWatch, without you needing to do any sort of setup. Most of the time, these are system level metrics like CPU utilization, disk IO, network traffic, or memory usage. 
These metrics can provide a general understanding of how your resources are performing. Can help you identify potential issues before they become critical. Identifying issues before your end users do is important, and having robust monitoring in place can help you do that. While many AWS services automatically send metrics to CloudWatch, there are cases where you might want to send custom metrics. Custom metrics allow you to monitor specific aspects of your application that are not covered by default system metrics. For instance, you might want to track application specific data such as the number of transactions processed, the response time of an API endpoint, or the number of active users. You can use CloudWatch dashboards to visualize and monitor the metrics that you deem as the most important. 
You can aggregate metrics from multiple components of your system into one unified view. This view will help you identify and diagnose issues as they arise, but dashboards can show you data over time as well, so you can more easily identify patterns and anomalies. Now, you likely won't be sitting around watching your dashboards all of the time. Instead, you want a way to be made aware of problems as the metrics begin to reflect any issues. You can create CloudWatch alarms for specific metrics to do this, and you can define thresholds for these metrics so that when the threshold is breached, you can be alerted, or other actions can automatically be taken. Before you determine what reasonable thresholds are to set for your metrics, you need to establish a baseline. To do this, you would want to measure performance of your system under different loads and conditions at various times and determine what is normal. 
Cloud watch is configured to retain metric data for up to 15 months. You can collect metrics for a while before determining what the baseline is for your system. In general, acceptable values for metrics depend on what your application is doing relative to your baseline. In the upcoming lab, you will be monitoring the RDS instance that you've set up in the first lab of this week. Let's explore some of the common metrics that you might be monitoring for RDS. First is CPU utilization. A high value for CPU utilization might indicate that your RDS instance is under heavy load and needs to be scaled up or that queries need to be optimized. 
Consistently high CPU utilization, such as values over 80 to 90% can lead to performance bottlenecks, slowing down your database operations. Next, RAM consumption is another common metric to monitor closely for RDS. High RAM consumption can also slow down performance and may indicate that you need to scale up to an instance type with more RAM. Another metric to keep an eye on is disk space. If disk space is consistently above 85%, then you may need to look at deleting or archiving data to a different system to free up some space. Database connections are also important to monitor. This metric shows the number of active connections to your database. 
If the number of connections approaches the maximum limit, it can lead to connection errors and application failures. Keeping an eye on this metric helps you manage connection pooling and scale your database to handle more connections if needed. These are a few simple examples of metrics RDS posts to CloudWatch, and why you would want to monitor them. Apart from CloudWatch, there are other popular third party monitoring services as well, like Data Dog or Splunk, which you could also use to monitor your systems. When it comes to monitoring other AWS services, keep in mind that each AWS service posts different metrics, and which metrics are important to you is use case dependent. Have fun of the upcoming lab, and I'll see you again in the next week. 



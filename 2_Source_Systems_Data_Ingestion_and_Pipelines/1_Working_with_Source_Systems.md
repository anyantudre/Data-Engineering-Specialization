# Working with Source Systems  
*Understanding databases, files, streaming platforms, and the foundations of secure connections in cloud-based data pipelines.*  

---

## Introduction  
This chapter examines how data engineers interact with **source systems**, the starting point of all data pipelines. Whether data is structured in relational databases, stored in files, or continuously generated by streaming systems, effective ingestion is the backbone of analytics and machine learning workflows. The text emphasizes that if you cannot access or ingest data properly, no downstream transformations or modeling will be possible. Beyond ingestion, the chapter highlights **DataOps practices**, **Identity and Access Management (IAM)**, and **networking configurations**, which together ensure security, reliability, and repeatability in modern cloud environments. By mastering these concepts, data engineers can confidently design and troubleshoot end-to-end pipelines that integrate multiple heterogeneous data sources.  

---

## Overview  
The course module begins with the **first stages of the data engineering lifecycle**:  
- **Data generation and source systems**  
- **Data ingestion**  

Data engineers typically do not generate data themselves; instead, they interact with upstream systems that produce it. Thus, understanding **where the data originates, how it is stored, and its properties** is crucial. This section outlines the course structure:  
- Week 1: source systems (databases, object storage, streaming).  
- Week 2: ingestion patterns.  
- Week 3: DataOps and infrastructure as code.  
- Week 4: orchestration with tools such as **Airflow**.  

---

## Different Types of Source Systems  
Source systems are the origins of raw data, classified by both **data format** and **storage mechanism**:  

- **Structured data** – organized in rows and columns (e.g., relational databases, CSVs).  
- **Semi-structured data** – not tabular but with an internal structure, like **JSON** or **XML**.  
- **Unstructured data** – free form, such as text, images, audio, or video.  

The three common categories of source systems are:  

1. **Databases** – support CRUD operations (Create, Read, Update, Delete) via a DBMS.  
2. **Files** – universal data medium; may be stored locally, in object storage, or exchanged via email.  
3. **Streaming systems** – generate continuous flows of messages representing events (e.g., IoT device readings).  

These systems are foundational for ingestion pipelines and may serve in both **upstream** and **downstream** roles depending on the architecture.  

---

## Relational Databases  
Relational databases are the most common **OLTP (Online Transaction Processing)** source systems. They:  
- Store data across related tables using **schemas**.  
- Use **primary keys** (unique identifiers per table) and **foreign keys** (references to other tables).  
- Minimize redundancy through **data normalization**.  

However, while normalization ensures integrity, it may slow query performance. This leads to trade-offs:  
- **Normalized design** – less redundancy, but slower joins.  
- **Denormalized/One Big Table (OBT)** – faster queries, but more duplication.  

Interaction occurs via **Relational Database Management Systems (RDBMS)** such as MySQL, PostgreSQL, Oracle, and SQL Server. The common interface is **SQL (Structured Query Language)**.  

---

## SQL Queries  
SQL enables querying normalized data efficiently. A lab example uses **Rentio**, a fictional DVD rental database.  

Key SQL clauses:  
```sql
-- Select specific columns
SELECT title, release_year 
FROM film 
LIMIT 10;

-- Filter results
SELECT * 
FROM film 
WHERE length < 60;

-- Join multiple tables
SELECT film.title, category.name 
FROM film 
JOIN film_category ON film.film_id = film_category.film_id
JOIN category ON film_category.category_id = category.category_id
WHERE length < 60;
````

* **SELECT** specifies columns.
* **FROM** defines the source table.
* **WHERE** filters rows.
* **ORDER BY** sorts results.
* **JOIN** links tables using foreign keys.
* **GROUP BY** and **COUNT** aggregate results.

These operations enable answering business questions directly from relational schemas.

---

## NoSQL Databases

When relational systems cannot scale efficiently, **NoSQL (Not Only SQL)** databases are used. They:

* Support **non-tabular models**: key-value stores, document stores, wide-column databases, and graph databases.
* Offer **flexible schemas**, making them suitable for heterogeneous data.
* Scale horizontally by distributing data across nodes.
* Typically provide **eventual consistency** instead of strong consistency.

Examples:

* **Key-Value Stores** – store session data for fast lookups (e.g., Redis, DynamoDB).
* **Document Stores** – JSON-like records grouped in collections (e.g., MongoDB).

NoSQL is ideal for applications prioritizing **availability and scalability** over strict consistency, such as social media platforms.

---

## Database ACID Compliance

To ensure transactional reliability, databases are evaluated on **ACID principles**:

* **Atomicity** – transactions are indivisible (all succeed or none succeed).
* **Consistency** – data must respect schema constraints.
* **Isolation** – concurrent transactions must not interfere.
* **Durability** – committed transactions persist despite system failures.

Relational databases are typically ACID-compliant by default. NoSQL systems often relax one or more principles to improve scalability but may allow configuration for stronger compliance. For OLTP workloads (e.g., banking, e-commerce), ACID compliance is critical.

---

## Object Storage

Files are frequently ingested from **object storage** systems like **Amazon S3**. Unlike hierarchical file systems, object storage uses a **flat structure**:

* Each object is identified by a **UUID** and contains **metadata**.
* Objects are immutable; updates require rewriting with versioning.
* Highly scalable, durable, and cost-effective.
* Commonly used in **data lakes** and **machine learning pipelines**.

For example, S3 achieves **11 nines of durability** through replication across multiple availability zones, making it a reliable data repository.

---

## Logs

Logs are append-only records of events, originally considered “exhaust” data but now vital sources for monitoring and analytics.

* **Types of logs**: application activity, backend updates, error tracking.
* **Structure**: typically include user ID, event description, timestamp, and log level (`info`, `debug`, `error`, `fatal`).
* **Applications**:

  * Debugging failures.
  * Monitoring system health.
  * Feeding **change data capture (CDC)** pipelines.
  * Supporting ML tasks like anomaly detection.

Logs are simple yet rich datasets that enable real-time insights into system performance.

---

## Streaming Systems

Streaming systems handle **event-driven architectures**. Key concepts:

* **Events** – something that happens (e.g., user click).
* **Messages** – data describing an event.
* **Streams** – sequences of messages.

Three components:

1. **Producer** – generates events.
2. **Router (broker)** – buffers and routes events.
3. **Consumer** – processes messages.

Two common approaches:

* **Message queues (e.g., AWS SQS)** – FIFO processing; messages deleted once consumed.
* **Streaming platforms (e.g., Kafka, Kinesis)** – persistent logs allowing replay and reprocessing.

Streaming data enables **real-time analytics, monitoring, and recommendation engines**.

---

## Lesson Overview

After covering databases, object storage, logs, and streaming, the lesson transitions to **practical challenges**:

* Establishing reliable **connections** to source systems.
* Handling **IAM permissions** and **network configurations**.
* Debugging failed connections (a core data engineering skill).

Labs simulate realistic troubleshooting tasks, reinforcing the importance of problem-solving in ingestion pipelines.

---

## Connecting to Source Systems

Common connection methods include:

* **AWS Management Console** – manual but non-repeatable.
* **Command Line Interface (CLI)** – more efficient, but still manual.
* **SDKs (e.g., Boto3 in Python)** – programmatic, repeatable, and automatable.
* **APIs (JDBC/ODBC)** – standardized connections for querying databases.

Automation and repeatability are preferred, especially for production pipelines.

---

## Basics of IAM and Permissions

**Identity and Access Management (IAM)** ensures secure handling of sensitive data. Key components in AWS:

* **Users** – long-term credentials for individuals.
* **Groups** – collections of users with shared permissions.
* **Roles** – temporary credentials assumed by services or applications.
* **Policies** – JSON-based definitions of permissions.

Best practices include the **principle of least privilege**, temporary role-based access, and avoiding hardcoding credentials. Misconfigurations are among the top causes of **cloud breaches**, making IAM critical for security.

---

## Basics of AWS IAM

This section elaborates on AWS IAM specifics:

* **Root User** – full access; should not be used for daily tasks.
* **IAM User** – assigned login credentials and policies.
* **IAM Group** – manages permissions for teams.
* **IAM Role** – assumed temporarily by EC2, Glue, etc.
* **Policy Example** – `AmazonS3FullAccess` grants full S3 access.

These constructs enable fine-grained control of cloud access.

---

## Basics of Networking in the Cloud

Cloud networks consist of **physical data centers**, grouped into **regions** and **availability zones (AZs)**. Key considerations:

* **Latency** – minimized by hosting resources closer to users.
* **Availability** – improved by replication across AZs.
* **Compliance** – data residency laws may affect region selection.
* **Cost** – varies by region.

Networking configurations (e.g., VPCs, subnets, gateways, routing) directly impact pipeline reliability. Understanding these foundations prevents common ingestion roadblocks.

---

## Conclusion

This chapter underscores that **source systems and ingestion** form the cornerstone of data engineering. From relational and NoSQL databases to object storage, logs, and streaming platforms, engineers must master diverse data origins. Secure connections through **IAM** and robust **cloud networking** are essential for building pipelines that are scalable, reliable, and compliant. Ultimately, successful data engineering depends not just on choosing the right tools, but on understanding the **characteristics of data sources** and applying best practices for ingestion, security, and orchestration.

---

## TL;DR

Data engineering begins with **source systems**: relational/NoSQL databases, files, logs, and streams. Effective ingestion requires secure connections (IAM, networking) and thoughtful pipeline design. Without mastering ingestion, no downstream analytics or machine learning system can succeed.

## Keywords / Tags

* Source Systems
* Data Ingestion
* Relational Databases
* NoSQL Databases
* ACID Compliance
* Object Storage
* Logs
* Streaming Systems
* AWS IAM
* Cloud Networking

## SEO Meta-Description

Learn how data engineers work with source systems—databases, files, and streams—plus IAM and networking for secure, scalable ingestion.

## Estimated Reading Time

20 minutes
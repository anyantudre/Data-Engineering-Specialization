## The Data Engineering Lifecycle & Undercurrents


#### Overview
Welcome to Week 2. This week, you'll dive deep into the data engineering life cycle and undercurrents, which we introduced in the previous week. As you saw in Week 1, the life cycle begins with data generation from source systems over here on the left, which is really a stage that happens before your job as a data engineer begins. Then we'll look at ingestion, transformation, storage, and serving of data for multiple use cases like analytics and machine learning. Or in simpler terms, we'll be looking at how in your job as a data engineer you'll work to get raw data from somewhere, turn it into something useful and then make it available for downstream use cases. In the second lesson of this week, we'll look at the undercurrents of the data engineering life cycle, security, data management, data ops, data architecture, orchestration, and software engineering. Now, just to be clear, I want to emphasize that this week, just like the previous week of materials, is more focused on a high level mental framework for data engineering than it is about actually building data infrastructure. 
This is because I believe it's really important to get that mental framework in place before you start building. This will make you more successful in all aspects of your work as a data engineer. But with that being said, this week isn't all about theory. At the end of the week, we'll look at how this mental framework comes together in practice on the AWS Cloud. In the lab activity, you'll be working with your first end to end Cloud data pipeline. Join me in the next video to begin by looking at data generation and source systems. 

#### Data Generation in Source Systems
The first stage of the data engineering life cycle is the generation of data and source systems. Your role as a data engineer, you'll be consuming data from various sources. For example, if you're working at an e-commerce company, you might build a data pipeline that consumes sales transaction data from an internal sales database and product catalog data from files. The marketing team might need you to get social media data from an API and market research datasets from a data sharing platform. You might even find yourself working with data from Internet of things or IoT devices, such as GPS trackers to gather location and movement data for product delivery updates. These systems are typically created and maintained by other teams, including software engineers from other internal departments, external vendors or third party platforms. As a data engineer, they're largely out of your control. 
But it's important to understand how these systems work because data pipelines you build will rely on the data generated from these sources. Let's start by taking a closer look at a few of the common source systems you'll be working with as a data engineer. The most common source systems are databases, which could be relational databases, represented as tables of related data, like you see here, or other types of NoSQL systems, including things like key value databases, document stores, and more. These databases might be part of the back end of a web or mobile application, or you could be storing data from some other system, and don't worry if you're not already familiar with a different types of databases. We'll get into the details of databases later on. Apart from database source systems, you might also be consuming data in the form of files, like text files, audio files like Mp3s, or even video or other types of files. While individual files like these might not sound like something you could call a source system, you'll find that in many cases, in your work as a data engineer, you need to download or be given access to files to begin your work. 
Another common source system that you'll encounter as a data engineer is an API, which is short for application programming interface. Simply put, an API allows you to issue a web request for data and then get that data back in a certain format, such as XML or JSON. You might also source data through a sharing platform, which is something that organizations may set up to share data internally or with a third party. IoT devices, as I mentioned earlier, represent yet another type of source system that is becoming increasingly common. With this type of source system, you might have many individual devices, what's known as a swarm of IoT devices, streaming data in real time. This streaming data is typically sent to a database, and the source system owner might make that data accessible via an API or a data sharing platform. In other cases, you might need to ingest and combine all those individual streams of data for use in your own downstream workflows. 
In a perfect world, the source systems you depend on would deliver the data you need in a consistent and timely matter, such that you could build downstream systems that rely on the predictability of that source system. In the real world, however, source systems are unpredictable. These systems sometimes go down or the team managing the system changes the format or schema of the data, or maybe the schema stays the same, but the data itself changes. When I first started working as a data engineer, I remember working with a database that was maintained by an internal team of software engineers. One day, that team decided to rearrange the columns in their application database, and they didn't tell me about these changes. I came to find out that the columns that my data pipelines relied on were changed, renamed, and some of them were even deleted. This completely halted some of the downstream data workflows, and I had to answer to some pretty upset stakeholders, so that wasn't good. 
When you're accessing data from source systems is essential to understand how those systems are set up and what changes in the data or system you can expect. This means that as a data engineer, you'll be more successful if you work directly with source system owners to understand how those systems work, how they generate data, how that data may be subject to change over time, and ultimately how those changes will impact the downstream systems that you build. In my experience, developing good working relationships with the stakeholders of source systems is an underrated yet crucial part of successful data engineering. Depending on your source systems and your goals, the next major phase of the data engineering life cycle, ingestion can look quite different from one project to the next. Join me in the next video to take a look at ingestion from source systems. 


#### Ingestion
As a data engineer, you'll build architectures that begin with the step of ingesting data from source systems, which means moving raw data from source systems into your data pipeline for further processing. In my experience, source systems and data ingestion represent the biggest bottlenecks of the data engineering life cycle. If you started, as I recommended in the previous video by working directly with source system owners to understand how those systems work, how they generate data, how that data may be subject to change over time, and ultimately how those changes will impact the downstream systems you build. Then you'll be well positioned to avoid common pitfalls in the ingestion phase. One of the critical decisions that you need to make when designing data ingestion processes is how frequently you need to ingest the data, meaning how often you need to move data from these source systems shown here on the left, into your data pipeline for further processing. You might choose to ingest data every so often in batches, like once every hour or once a day. Another approaches becoming increasingly common is to ingest data as a constant stream of events in near real-time. 
In this video, I'll focus on introducing these two major data ingestion patterns, batch versus streaming. You can think of data being produced as a continuous series of events. Those events might be clicks on a website or sensor measurements or something else. Out in the world, events like these are happening continuously. In a sense, you could think of nearly all data as being streamed at its source. Batch ingestion is just a convenient way to process the stream of events in large chunks, either on a predetermined time interval or as data reaches a pre set size threshold. For example, you could handle an entire day's worth of data in one single batch. 
For a long time, batch processing was the default way to ingest data. There are more options for ingestion today. Batch processing remains a practical and popular way to ingest data, particularly in cases where the data is used in analytics and machine learning. With streaming ingestion, on the other hand, you're ingesting and providing data to downstream systems in a continuous near real time fashion. Now, when I say you're ingesting data and near real time, I mean you're making it available to downstream systems a short time after it's produced. Possibly less than one second later. In this case, you need to use specific tools, such as an event streaming platform or a message queue to continuously ingest streams of events. 
With these tools becoming more ubiquitous, streaming ingestion has become more accessible and popular. However, streaming ingestion is not necessarily the best choice for all use cases and there are significant trade offs to consider when deciding whether streaming ingestion is an appropriate choice over what might be considered the simpler approach of batch ingestion. For working on a stream processing solution, you should be asking yourself, things like what actions can you take on real time data that would be an improvement over batch data? Would streaming ingestion costs more than batch in terms of time, money, maintenance, and potential downtime? How will choosing stream ingestion over batch ingestion or vice versa influence the rest of your data pipeline? Batch ingestion is an excellent approach for many common use cases, like model trading and weekly reporting. My advice is to adopt a streaming ingestion system only after you've taken the time to identify a business use case that justifies a trade offs of using it over batch. 
It's also important to note that streaming ingestion generally coexists with batch processing. For example, machine learning models are usually trained on a batch basis. But that same training data might have been originally ingested via streaming because of some separate goal of your architecture like real time anomaly detection. Rarely do data engineers have the option to build a purely streaming data pipeline with no batch components. Instead, you'll choose where the boundaries between batch and streaming will occur. Beyond just deciding between a batch versus streaming approach, there are other important nuances to the ingestion sage, like how you might use change data capture or CDC, for short to trigger certain ingestion processes based on data changes in the source system, and whether you'll be adopting a push or pull approach, which is to say whether a source system will be pushing data to you or you'll be actively pulling it from the source. We'll get into all these details and more throughout the specialization. 
But for now, join me in the next video to look at data storage, which is really part of every stage in the data engineering life cycle.


#### Storage
I'd like you to think for a moment about all the different ways you interact with data storage systems on a daily basis. On your laptop, for example, you might create or delete files, or move files between different folders. Doing this, you're changing how things are stored on your hard disk or solid state drive. When you open applications, you're loading them into random access memory or RAM, which is simply another type of storage that allows for faster access, or you might download new files or applications from the Internet or have some of your files automatically backed up to cloud storage. With your smartphone as well, you might be sending or receiving messages or interacting with apps, effectively moving data around between different storage components on your device as well as in the Cloud. With almost any action you take on a digital device or online, you're interacting with data storage systems, and you might run into the limits of the storage systems like running out of space to store pictures on your phone or attempting to send files that are too large. The function, performance, and limitations you encounter will have a lot to do with how those systems are set up. 
Similarly, in your work as a data engineer, the function, performance, and limitations of the systems you build will have a lot to do with the storage solutions you choose to support those systems. To begin with, let's take a look at some of the raw hardware ingredients of storage. In your day to day life, you've probably become familiar with various forms of solid sate storage, like flash memory cards or solid Sate drives, your laptop or smartphone. By contrast, you probably haven't read data from a floppy disk recently. You could be forgiven for thinking that the world has moved on from magnetic disks as a data storage solution. However, the truth is that magnetic disks still form the backbone of modern day storage systems, and that's primarily because of the low cost of disk storage. At the time of this recording, disc storage is about 2-3 times cheaper than solid state storage. 
RAM, commonly referred to as memory is another form of physical storage you might be familiar with. RAM offers much faster read and write speeds and solid state drives or discs, making it a critical component of many applications and architectures. However, RAM storage could be 30-50 times more expensive than solid state storage. It's also volatile in the sense that if your system loses power, most of the time, depending on the type of ram you use, memory is lost instantly. In most modern architectures, data will pass through magnetic storage, solid state drives and memory as it works its way through the various processing phases of [inaudible] pipeline. However, the physical components of storage are just one aspect of how data storage is implemented throughout your architecture. Modern and Cloud data storage systems are commonly distributed across multiple clusters and data centers. 
This means that things like networking, CPU, serialization, compression, and caching are all critical raw ingredients for storing data in modern data systems. Don't worry about it if you aren't familiar with all the things I just mentioned. We'll get more into the details of each of these elements of storage and the third course of the specialization. As a data engineer, you typically won't be responsible for managing the granular details of how your data moves and is stored across a network of data centers and physical storage devices. Instead, you'll work with storage systems like database management systems or object storage platforms like Amazon 3. Depending on the requirements for your architecture, you might also be working with systems like Apache Iceberg or Hoody, cache and memory based storage systems. Or streaming storage. 
All of these data storage systems are built on top of the physical and other wrong ingredients of storage that exist inside servers and clusters, allowing these systems to ingest and retrieve data using different access protocols. Finally, in your work as a data engineer, it's likely that you will not only work with individual storage systems, but combinations of storage systems arranged into storage abstractions, like a data warehouse, a data lake, or the more recent combination of these concepts, the data lake house. With the storage abstraction tools, rather than worrying about the details of how the underlying components are arranged, you'll choose various configuration parameters that allow you to meet your system requirements in terms of latency, scalability, and cost. If you were to think about storage as a hierarchy, how would you arrange the different aspects of storage? Well, at the bottom, you have the raw ingredients of data storage, including the various physical components like disc, RAM, and solid state storage, as well as the various non physical raw ingredients of storage, like networking and serialization. Then on top of that, you have storage systems that are built from these raw ingredients, and these include database systems, object storage, and much more. Then finally, at the top of the hierarchy, you have storage abstractions, which are combinations of storage systems that allow you to achieve your high level data storage needs without worrying about as many of the low level details. 
As a data engineer, it's entirely possible that you will spend much of your time operating at or near the top of this hierarchy, meaning that you won't be required to think about the details of exactly how your data is moving between different storage components and systems. However, you will be most effective in your work. If you take the time to understand the inner workings capabilities and limitations of your entire storage solutions right down to the wrong ingredients. The truth is that many practicing data engineers today do not deeply understand the details of the storage systems they build. This leads to unfortunate consequences when it comes to things like performance and cost. As it happens, I was consulting for a team once that failed to consider these details. They needed to move a large data set into their data warehouse, and they unwittingly chose to use an approach of doing direct row inserts for each individual row of data, meaning they were ingesting and writing one row at a time into the data warehouse. 
This turned out to not only be very slow, but also very expensive, since direct row inserts are generally charged on a per usage basis. In the end, they discovered their error and moved to a bulk ingestion approach, but only after they had wasted significant time and burned through half their annual data warehousing budget in the span of a week. Anyhow, it'll be to your advantage to be storage savvy as a data engineer. That's why we'll be exploring the details and implications of various storage solutions throughout these courses. Next up, we'll look at the next stage of the data engineering life cycle, data transformation. 


#### Queries, Modeling and Transformation
The transformation stage of the data engineering life cycle is really where you, as a data engineer, start to add value. This is because the stage that comes before transformation, namely ingesting and storing raw data from source systems, doesn't directly add any value from downstream stakeholders. As I said before, in your role as a data engineer, the big picture is that you get raw data from somewhere, turn it into something useful, and then make it available to end users. Transformation is the turn it into something useful stage. In terms of what useful looks like, imagine, for example, a business analyst as your downstream user. Let's say they're tasked with reporting on daily sales across a range of products. They might need information like customer IDs, product names, prices, quantities, times of sale, and so on. 
While data analysts are often fluent in SQL, they'll be relying on you to transform the raw data and provide it to them in a format that is quick and easy to query. As another example, imagine a data scientist or machine learning engineer as your downstream user. In addition to SQL, they might even be fluent in many potential approaches to data transformation, but their core function is really in using the data for predictive analytics, and you can provide them with tremendous value by handling the transformation of data into structures and features that can be used directly for their model training or analysis. We call this part of the data engineering life cycle transformation. But in reality, the stage is made up of three parts : queries, modeling, and transformation. I include queries and modeling as separate from transformation here because these are critical components of any data pipeline that really add value when done well and present risks when done poorly. To illustrate this point, let's start with queries. 
When you query data, you're issuing a request to read records from a database or other storage systems. For example, you may need to query tabular and semi-structured data that's sitting in a cloud data warehouse. There are many languages you can use to query data, but in these courses, we'll focus on structured query language or SQL for short, which remains a popular and universal query language. Your query may involve cleaning, joining, and aggregating data across many data sets. You may use SQL expressions to filter the data so that only specific records are retrieved. Don't worry if you're not already familiar with the SQL commands you see here on the slide. In later courses, you'll have a chance to learn the basics of SQL through hands-on labs. 
There's more than one way to write a query, and poorly written queries could have negative consequences, like impacting the performance of a source database or cause a situation known as row explosion, where a query that includes what's known as a join between tables produces many more records than you anticipated, which can bring down your storage infrastructure. In other circumstances, poorly written queries may just be slowed or broad and cause downstream delays in reporting or analytics. In practice, it turns out that most data engineers can read and write SQL, but are unfamiliar with how queries work under the hood. This can have unforeseen consequences in the architectures they build. We'll get into the details of exactly how queries work in the third course of the specialization. The next thing I want to talk about is data modeling. A data model represents the way data relates to the real world. 
Data modeling involves deliberately choosing a coherent structure for your data and is a crucial step in making data useful for the business. For example, looking again at the case of the business analyst that needs to create product sales reports, you might have ingested so-called normalized data from an upstream relational source database that contains separate tables for things like product information, order details, customer information, and so on. This data often has complex relationships between them. To serve this analyst's needs, you might need to do what's called denormalization of this data to model the data in a way that allows the analysts to quickly and efficiently query and get the data they need for the reports. A good data model is designed to best reflect your organization's processes, definitions, workflows, and logic. For example, the term customer might mean different things to different departments in your company. To be successful with data modeling, you need to work with stakeholders to understand their terminology, like what the word customer means to them, as well as the business goals for the data. 
You'll learn more about data modeling and normalization in the fourth course of the specialization. Apart from querying and modeling the data, data must also be transformed, which is to say, manipulated, enhanced, and safe for downstream use. As I mentioned before, you will be typically transforming data multiple times throughout the data engineering life cycle. For example, data may be transformed before you even touch it, like having a time stamp added to a record while it's still on a source system, or you might apply transformations while your data is in flight during ingestion, then immediately after ingestion, it may set up basic transformations to map data to correct types, and put records into standardized formats. You might enrich a record within a streaming pipeline with additional fields and calculations before it's sent to a data warehouse. Even further downstream, you might transform the data schema and apply denormalization, large scale aggregation for reporting or featurized data for machine learning model training. Throughout these courses, you'll be engaged in plenty of hands-on exercises involving querying, modeling, and transforming data. 
For now, though, let's move on to the next video, where we'll take a look at the final stage of the data engineering life cycle, serving data for downstream use cases. 


#### Serving data
Once you've ingested, transformed, and stored your data, you're ready for the final stage of the data engineering lifecycle. Serving data this stage is about more than just making data available. It's really when you give stakeholders the opportunity to extract business value from data. And like I mentioned last week, value means different things to different stakeholders. Generally, data has value when it's used for practical end use cases like analytics, machine learning, reverse ETL, or other use cases. In this video, rather than discussing specific mechanisms for serving data, we'll instead take a brief look at each of these end use cases to provide more context on how serving data might look different for different scenarios later. In course four, we'll get into the specifics of how exactly you will serve data for each of these use cases. 
So let's start with analytics, which is the process of identifying key insights and patterns within data. As a data engineer, you're almost guaranteed to serve data that powers one or more of the three most common forms of analytics, business intelligence, operational analytics, and embedded analytics. Business intelligence, or BI for short, is where analysts explore historical and current business data to discover insights. As a data engineer, you'll serve data that ultimately gets presented in the form of reports or dashboards that help users make strategic and actionable decisions. For example, analysts on sales and marketing teams will use BI reports and dashboards to spot patterns and trends and to monitor things like marketing campaign engagement, regional sales, and customer experience metrics. Or imagine a scenario where an analyst observes a sudden spike in product returns. They might then investigate existing reports or dashboards to compare this phenomena with historical trends. 
They might also pull more data by running SQL queries against the database you provided or ask you to serve up additional data for ad hoc analytics. In contrast to the reflective or insight driven nature of BI, operational analytics is about monitoring real time data for immediate action. For example, an e-commerce platform team might need to monitor a dashboard with real time performance metrics for the website. If the website goes down for some reason, whether that's because of a spike in user traffic or a data center going offline, they need to know immediately so they can triage the situation and get the site up and running again. And in this case, your role as a data engineer would be to ingest, transform, and serve event data from the website's application logs to populate the platform team's dashboard. While BI and operational analytics are internally focused data practices that have been in use for many decades, a somewhat newer trend is external or customer facing embedded analytics. You've most likely interacted with a variety of embedded analytics applications yourself. 
For example, your bank might provide you with dashboards that show you historical trends in your spending or how your purchases break down across different categories like food, retail and utilities. Or maybe you have a smart thermostat in your home that's connected to a mobile application. The application might show the current temperature inside your home, as well as historical metrics like temperature over time. When it comes to embedded analytics, as a data engineer, your job would be serving real time and historical data for use in user facing applications. With the rise of machine learning in recent decades, it's quite likely that your role as a data engineer will involve serving data for machine learning use cases. And these courses will treat machine learning as separate from other serving use cases simply because it can involve additional complexities that we'll want to look at in detail. For example, with a machine learning use case, you may be responsible for serving feature stores of data that facilitate model training, and you might also need to serve data for real time inference, or support metadata and cataloging systems that track data history and lineage. 
We'll take a closer look at all these scenarios later in these courses. Apart from analytics and machine learning, another common use case for serving data is what's at least today called reverse ETL. With reverse ETL, you'll take transformed data as well as analytics and perhaps machine learning model output and feed it back into source systems. For example, let's say you've ingested data from a customer relationship management or CRM system, and this might include information like names, contact info, form data, or other relevant client information, and you've then transformed the data into the appropriate format and stored it in a data warehouse. Analysts can then retrieve the data to train a lead scoring model, which is a model that attempts to say which clients are the most promising for various engagements or product offerings. The model results could then be returned to the data warehouse and then pushed back into the CRM as an enhancement of the client data already stored there. Now, I'll just say that the name reverse ETL for this process is not so much an attempt to describe what's going on as it is for just a lack of a better name to describe this pretty longstanding process. 
In any case, this practice is increasingly common, and in your work as a data engineer, you're likely to be engaged in reverse ETL, or whatever it happens to be called in the future as part of your role. And with that, we've taken a quick look at all the phases of the data engineering lifecycle, including source systems, ingestion, transformation, storage, and serving. In the next lesson, we'll shift our focus to the undercurrents of the lifecycle to touch all of these phases. I'll see you in the next lesson. 



#### Intro to the Undercurrents
We've been looking at the data engineering life cycle and how you'll ingest data from source systems, transform it, store it, and serve it to end users. As a field, data engineering is rapidly maturing, whereas just a decade ago, the role of a data engineer was primarily focused on the technology layer. The continued abstraction and simplification of tools has expanded the scope of the role. Data engineering now encompasses far more than just tools and technologies. In other words, the field is moving up the value chain, which is great news for you as a data engineer. Modern data engineering incorporates traditional enterprise practices such as data management, and cost optimization, as well as newer practices like DataOps. When it comes to work as a data engineer, there's a set of such practices that will apply to your work across the entire life cycle. 
In the fundamentals of Data Engineering book, we describe these practices as the undercurrents of the data engineering life cycle. These undercurrents include security, data management, DataOps, data architecture, orchestration, and software engineering. The next several videos, we'll take a closer look at each of these undercurrents. After that, you'll begin exploring how the data entering life cycle and undercurrents take shape in real life on the AWS Cloud. Let's get started. 


##### Security
Before getting into how security applies to your role as a data engineer, I'd like you to think for a moment about how security concerns apply to your own personal data every day. For example, you probably don't give out your bank account information to just anyone, and you don't publish the passwords to all your online accounts where others can see them. Similarly, you wouldn't give the keys to your house to someone you don't know in trust. In your role as a data engineer, you're entrusted with sensitive data, whether that's the personal and private information of your clients or proprietary business information. The owners of that data are trusting that'll keep their information safe. It's important to recognize your role in bringing together the right set of principles, protocols, and cultural practices to ensure security in the systems you build. In this video, I'll cover some of those foundational principles and best practices for data security. 
When you're managing a data pipeline and serving data to end users, you'll need to give access to the data and system resources to other users or applications. Key security principle to keep in mind when doing so is the principle of lease privilege. This means you give users or applications access to only the essential data and resources they need to do their jobs and only for the duration that's required. You need to apply the principle of lease privilege, not only to others in your organization but also to yourself. That means, for example, just like you don't give everyone a team administrator access, when it comes to your own work, don't operate from the root shell when it's not necessary and don't use administrator or super user permissions unless it's absolutely essential. When you think about how to best secure data in your work, you need to be thinking not only about data access but also data sensitivity. Adhering to the principle of lease privilege means making sensitive information visible to users only when it's absolutely necessary. 
Beyond that, the best way to protect sensitive data is not to ingest it into your system in the first place. If you don't have a clear purpose for ingesting and storing sensitive data, simply don't do it, and you'll have entirely removed the risk of accidentally leaking that data. In today's Cloud centric world, security takes on yet another dimension, one that requires you to understand things like identity and access management, or IM Rals, encryption methods, and networking protocols. You'll see more on these topics throughout the specialization as we'll delve deeper into ensuring the security of your data pipelines. Beyond that, security is not just about principles and protocols, it's also about people. Security starts and ends with you, the individual, as well as other individuals across your organization. When it comes to security, you should adopt a defensive mindset. 
Always be cautious when asked for credentials or for sensitive and confidential data. Imagine potential attack and leak scenarios and design your data pipelines and storage systems with them in mind. When it comes to real world data leaks, individual people have been the source of many of the biggest security breaches. People ignoring basic precautions, like sharing their passwords insecurely with others, people falling victim to phishing attacks, where an attacker tries to steal of sensitive information by impersonating an authority figure or so many trust, or people otherwise acting irresponsibly while working in the company's systems and data. When it comes to security and data engineering, it never ceases to amaze me how often I see data engineers leap save in AWS, S three bucket, a server, or database exposed to the public Internet when that was not the intention. There are some simple fixes to prevent this from happening. But I often see this happen because the data engineer simply doesn't know security best practices, or they accidentally forgot to apply these practices. 
When it comes to security at the organizational level, I've seen that all too often, organizations are set up with a facade of security. Maybe a set of checklist to show they're compliance with regulations or compliance standards, but a deeper spirit of security is lacking in the organization's culture. This approach of adhering to the letter of security without a culture and spirit of security is what I call security theater. Security emerges from a culture where every team member recognizes the role in protecting data and everyone from top to bottom embraces security as a priority and a habit. On your journey as a data engineer, remember that while principles and protocols, as well as tools and technologies, are your allies when securing data. A true security culture emanates from a shared understanding of responsibilities and vulnerabilities across the organization. As we progress in the specialization, you'll get hands on experience with security considerations as they apply to all aspects of your data architecture. 
In the upcoming video, we'll take a look at the next under current in the data engineering life cycle, Data Management. 



##### Data Management
Throughout these courses, I'll be emphasizing that whatever you're working on as a data engineer, you need to be thinking about how you work adds value for stakeholders in your organization. Data can be an incredibly valuable business asset, but only when it's managed properly. Data management is so important, in fact, that there's an international organization, the Data Management Association International, or DAMA dedicated entirely to providing resources for companies and individuals to get data management right. DAMA provides this neat little publication, the Data Management Book of Knowledge or DMBOK for short. As you can see, there's a lot to know when it comes to data management. But before you start freaking out, let me just say that as a data engineer, you don't need to memorize everything in this book. In fact, this is a great reference volume. 
In your work as a data engineer, you'll focus on a subset of data management tasks, and you'll share the full responsibility of data management with teams, including software engineering, IT, and others. In this video, I'll quickly highlight the key aspects of data management that you need to be concerned with as a data engineer. First of all, the DMBOK defines data management as the development, execution, and supervision of plans, programs, and practices that deliver, control, protect, and enhance the value of data and information assets throughout their life cycles. Now, that's a mouthful, and perhaps even sounds a little vague. But let's break it down a little bit. As a field, data management consists of many facets and disciplines, each with their own set of responsibilities. This can make the data management environment confusing. 
The DMBOK breaks down the different facets of data management into 11 data knowledge areas. These include data governance, data modeling, data integration, and interoperability, metadata, security, and more. They arrange all that in a diagram like this. You'll be seeing more on all of these topics throughout these courses. But as you can see in this diagram, data governance touches all other areas of data management. As it happens, many of the other knowledge areas interact with each other through data governance practices. Here in this video, I'll just focus on data governance as it relates to a lot of the areas that will be important in your role as a data engineer. 
According to another book called data governance, the definitive guide. Data governance is first and foremost a data management function to ensure the quality, integrity, security, and usability of the data collected from an organization. From this definition, you can start to see that data governance covers a lot of ground, everything from data security and privacy to data quality and usability. We touched on security and privacy a bit in the previous video. For the purposes of this video, I'd like to zero in on data quality, which is closely related to some of the other key terms you've seen here like integrity, usability, and reliability. Data quality is a deep and nuanced topic, but at its core, this concept is relatively straightforward. High quality data is accurate, complete, discoverable, and available in a timely manner. 
Beyond that, high quality data represents exactly what stakeholders expect it to represent in terms of well defined schema and data definitions. Data that conforms to the standards of quality is a powerful tool in decision making and adds great value in our organization. By contrast, low quality data might be inaccurate, incomplete, or otherwise unusable. Low quality data can cause stakeholders to waste their time, make poor decisions, or even fire their entire data team. You'll learn more about how to monitor and ensure data quality in your data pipelines in the next course. But for now, we'll move on and take a look at the next undercurrent. Join me in the next video to explore data architecture as it applies to the data ensuring life cycle. 


##### Data Architecture
You can think of data architecture as a roadmap or blueprint for your data systems. In the first week of this course, we talked about requirements gathering and how to think about taking stakeholder needs and turning those into specific requirements that you can use to make design and technology choices. In order to map requirements to a successful design for your system, you need to think like an architect. Now, just to be clear, in your role as a data engineer, depending where you work, you may not actually be directly responsible for making architecture and design choices. Your organization may have someone in the role of data architect. Who is responsible for establishing the design and passing that on for you to implement. However, in my experience, being able to think like an architect will make you more successful in your role as a data engineer. 
In some circumstances, like if you're working at a small startup, you may in fact be both the architect and the engineer. In any case, I wish someone had taught me how to think like an architect when I was getting started with data engineering. In this video, I'll briefly introduce some of the key principles that are part of thinking like an architect. Throughout these courses, we'll be revisiting these principles so you can feel confident in your abilities to design and build robust data systems. In our book fundamentals of Data Engineering, Matt Housley and I defined data architecture as the design of systems to support the evolving data needs of an enterprise, achieved by flexible and reversible decisions reached through a careful evaluation of trade offs. Let's take a minute to unpack that definition. First off, you'll notice that data architecture needs to support the evolving data needs of the organization. 
This means that a good design supports not only the data needs of today but also tomorrow. In practice, this means that data architecture is an ongoing effort, rather than something you just do once and be done with. The next part of this definition says that a good design is achieved through flexible and reversible decisions. This is calling out the fact that the data needs of your enterprise may evolve in ways you hadn't anticipated and that you'll need to update your architecture over time. If your initial design choices were flexible and reversible to begin with, then you'll have a much easier time evolving your architecture to meet the needs of the organization. Finally, you'll see that in this last part of the definition. All this is achieved through a careful evaluation of trade offs, these might include trade offs in performance or cost or scalability or other parameters. 
Now, I think it's worth mentioning at this point that back when essentially all data architectures were build as on premises systems, making flexible and reversible decisions was much harder, in some cases impossible. For example, if you decided to purchase and install millions of dollars worth of server hardware. You're likely to be committed to that system for some number of years, whether you like it or not. Nowadays, with most data architectures being built in the Cloud. You can in some sense, change your mind as often as you want about the technology choices you've made for your architecture. Provided you've made flexible and reversible decisions to begin with. To expand on these ideas a little more, let's take a look at a set of principles of good data architecture that we'll be revisiting throughout these courses. 
Before I get into these, I'll just say that you don't need to worry about memorizing any of the stuff right now. I just want to give you a preview of what's to come in these courses and get you started thinking like an architect. Principle number one, choose common components wisely. Common components are the parts of your architecture that will be used by multiple individuals and teams across your organization. A good choice of common components is one that provides the right set of features for individual projects and simultaneously facilitates collaboration between teams. Principle number two, plan for failure. This means exactly what it says. 
A good architecture is designed not only for the case where everything is working as expected, but also for when things break. Principle number three, architect for scalability. Scalable systems can scale up to meet demand as needed and scale down to minimize costs when demand receds. When you build scalability into your architecture, you can be responsive to a changing demand and optimize for cost at the same time. Principle number four. Architecture is leadership. Now, while the principle of architecture as leadership may not directly apply to you in your role as a data engineer, if you work towards thinking like an architect and seek mentorship from data architects, you will be better able to lead and mentor other team members as your skills develop and you become more senior. 
Eventually, you may occupy the data architect role yourself. Principle number five, always be architecting. As I said before, architecture design is not something that happens only once. Instead, you'll be constantly evaluating your systems against the evolving needs of your organization and re architecting on an ongoing basis. Principle six and seven, build loosely coupled systems and make reversible decisions. A loosely coupled system is one that is built from individual components that can be easily swapped out for other ones without having to re architect the whole system. Choosing to build with easily interchangeable components like this. 
You're making a set of reversible decisions, which is to say if you change your mind or the needs of your organization evolve, you can easily reverse your prior set of decisions and swap out components of your architecture to meet your new design specs. Principle number eight, prioritize security. We've already looked at some security principles like the principle of lease privilege, and later in our discussions of architecture, we'll get into others like the zero trust principle. The main takeaway with all these principles is that security is central to your role as a data engineer. Principle number nine, embrace FinOps. The cost structure of data has evolved dramatically in the Cloud era. FinOps is a movement to bring together the business priorities of finance and DevOps, or in this case, DataOps. 
On the Cloud, most data systems are pay as you go and readily scalable. By embracing FinOps, you can design your systems to be simultaneously optimized for cost and potential revenue generation. That's a quick look at the key principles of good data architecture. Next week in this course, we'll look more closely at these principles and good data architecture in general. But for now, let's move on to the next undercurrent of the data engineering life cycle. Join me in the next video to take a look at DataOps. 


##### DataOps
Around 2007, a framework called DevOps emerged in software development to break the silos between software development teams who write and test code and the software deployment teams who deploy and maintain code. DevOps borrows from other well-known methodologies including lean and agile, to accomplish things like the removal of bottlenecks, the reduction of waste, and the quick identification of problems, as well as rapid iteration. The DevOps movement has resulted in increased release cycles and enhanced quality for software products. As the data field has matured, we've adopted a similar approach, known as DataOps, to the development of data products. Similar to how DevOps improve the development process and quality or software products, DataOps aims to improve the development process and quality of data products. DataOps is first and foremost a set of cultural habits and practices that you can adopt. These include things like prioritizing communication and collaboration with other business stakeholders, continuously learning from your successes and failures, and taking an approach of rapid iteration to work toward improvements to your systems and processes. 
These are also the cultural habits and practices of DevOps, and they're borrowed directly from the agile methodology, which is a project management framework. Focus on delivering work in incremental and iterative steps. In terms of the technical elements of dataOps, there are three key pillars. The first pillar you see here on the left is automation. Then the second pillar is observability and monitoring. And finally the last pillar is incident response. These are similar to the core components of DevOps, where the end goal is to provide specific functionality and features in a software product. 
But in data Ops, the goal is to provide high quality data products, where you can think of a data product as any data or data system that you're providing to end users. And so let's take a closer look at each of these three pillars of data ops. In terms of automation, one of the DevOps practices that accelerated the software build lifecycle is what's known as continuous integration and continuous delivery, or CI/CD for short. With CI/CD, developers are able to automate many of the manual processes required to build, test, and deploy code. This automation results not only in faster review and deployment cycles, but also fewer errors, ultimately making software teams more efficient and effective in building high quality software products. And DataOps employs a similar automation framework to data processing as DevOps applies to software development. Within dataops, the high level goal of automated change management remains the same, for example, when it comes to managing changes in code, configuration or environment. 
In addition, DataOps is focused on change management when it comes to data processing pipelines and the data itself. To get a sense of how automation applies to data processing, let's imagine that you just got started at a small organization. And you've been tasked with building a data pipeline that starts with ingesting data from multiple source systems. So maybe you're ingesting from a database as well as some files and an API or a data sharing platform. Then perhaps you're performing some in-flight transformations during the ingestion process, then storing the ingested data in a storage system, perhaps a database. And then let's say you've got two end use cases you're serving, one for analytics and one for machine learning. So next, let's suppose you're performing some further transformations, maybe modeling and aggregating the data before pushing it to another storage system and making it available to end users. 
And so you've got essentially two pipelines for the transformation and serving stages here. If you're the first data engineer at this organization and you're in the early stages of developing your data systems, you might choose to manually execute the various tasks in this data pipeline, like manually starting each of these ingestion processes. Then once those are complete, manually executing each of the subsequent steps in the transformation, storage and serving stages. This could be a reasonable approach to get started quickly and prototype some aspects of your data pipeline in the long run. However, this sort of multistage manual execution will be prone to errors and be inefficient because it requires you to manually run each task with a minimal level of automation. You might choose to take a so called pure scheduling approach, meaning that you would set each task in your pipeline to start at a particular time of day. So maybe you start all these ingestion tasks at midnight every night. 
Then you would estimate how long it takes for all the data to be ingested and loaded into your storage system. Then you could schedule the downstream transformation tasks to start after that, and so on through all the tasks in your pipeline. So this is called scheduling because you create a schedule to automatically start each of the tasks in your data pipeline. To take things to the next level of DataOps automation, you could adopt an orchestration framework like Airflow. Orchestration frameworks check the dependencies between tasks within your data pipeline before each task is run. So you can decide the time and frequency you want the first task of your pipeline to start. Then the orchestration framework will automatically start subsequent tasks once the previous ones have been completed successfully. 
The orchestration framework can also notify you when there's an error with any of the tasks. So that the downstream tasks that are dependent on the previous ones don't start when they're not supposed to. Many orchestration frameworks not only automate the execution of tasks in your data pipelines, but they also enhance the development of these pipelines by enabling automatic verification and deployment of new aspects of your data pipeline. Similar to the CI/CD process for software deployment. When it comes to the next pillar of observability and monitoring, the main thing you need to keep in mind is that any data pipeline you set up is bound to fail eventually. To quote Werner Vogels, the CTO of Amazon Web Services, everything fails all the time. This means that if you're not closely observing and monitoring your data systems, you'll be caught unaware when they fail. 
In a worst case scenario, you might only become aware of these system failures when your downstream stakeholders discover these problems on their own, for example, in their reports or analytics dashboards. In my own work with clients, I've seen countless cases of bad data lingering in reports for months or even years due to undiscovered failures in data processing systems. These sorts of failures can be a waste of time and money, lead to ill informed decisions, and ultimately could cost you your job if stakeholders lose trust in your work. So observability and monitoring are crucial aspects of the data systems you build. The third pillar of DataOps is incident response, which is about using the observability and monitoring capabilities you set up to rapidly identify the root causes of an incident and resolve it as quickly and reliably as possible. As I said before, things will break and it's only a matter of time before they break. With incident response, it's not just about the technology and tools you use to identify and respond to an issue. 
It's also about open and blameless communication and coordination of the efforts of members of the data team responding to the incident, as well as others across the organization. As a data engineer, you should be proactively finding issues before they are reported to you by other stakeholders in your organization. DataOps is a relatively new set of ideas that is still a work in progress, and not all organizations have adopted DataOps best practices. In your work as a data engineer, you might find yourself in an organization where DataOps is fairly mature or somewhere that has not yet embraced DataOps. Next up, we're going to take a closer look at orchestration, which is a key component of DataOps. And such a critical component of modern data architectures and pipelines that we consider it as a separate undercurrent of the data engineering lifecycle. 


##### Orchestration
When you think of the word orchestration, what comes to mind? Maybe a conductor, guiding an orchestra or a choir, the signaling when various instruments or voices should be featured and shifts and things like tempo and intensity, all in an effort to make good music. Like an orchestra, a data pipeline has a lot of moving parts that must be coordinated to get a good result. As a data engineer, you are the conductor in charge of coordinating and managing the tasks in your data pipelines. We touched on orchestration briefly in the previous video as its a central component of data ops. Here, I'll just say a few more words about how orchestration plays a key role as an undercurrent of the data engineering life cycle. As I mentioned in the last video, if you're just getting started, maybe as the first data engineer at a small start up, or in the prototyping stages of a new project at any size organization, you might initially set up a data pipeline where you're manually executing each of the tasks at each stage. 
We looked at a pipeline like this where you're ingesting data from multiple sources into your storage systems, while applying some in flight transformations. Then downstream, you have more processes to transform, store, and serve the data to downstream users for machine learning and analytics use cases. Mannual execution of all these steps in your data pipelines, meaning manually triggering each step to run when you need it to, might be something that can help in prototyping various aspects of your system. But in the long run, this is not a sustainable method for data processing. Once you know what tasks you need to run in your data pipeline, you might take a pure scheduling approach to have those tasks run automatically at particular times of day or at particular frequencies. For example, you might schedule your ingestion and initial storage test to start at the same time each day. Then you might schedule the transformation steps to kick off an hour after you expect all the ingested data to be present in your storage system. 
Pure scheduling is an approach that has been widely used historically for various data processing tasks. However, you can run into problems with this approach. For example, if the scheduled ingestion task failed to execute, this could cause downstream transformation tasks to fail as well. Or if this transformation task simply takes longer than expected and the next transformation task kicks off before the previous one finishes, you can end up with incomplete or stale or otherwise problematic data being propagated through your pipeline. Not that long ago, orchestration frameworks that were more sophisticated than pure schedulers were really only available to large enterprises where they had the resources to build their own custom solutions. But nowadays, there are a number of open source orchestration frameworks that make it possible for you to build sophisticated orchestration into your own data pipelines, no matter what size team or company you're working with. The most popular framework right now is Apache airflow, but several other newer open source frameworks such as Dagster, Prefect, and Mage, are gaining traction as well. 
What these frameworks allow you to do is automate your data pipelines and build in complex dependencies and monitoring capabilities. You could do time based scheduling, if you like, but you could for example, build in a dependency that verifies that this first transformation task has been completed before starting the next transformation task. Or instead of predefining a particular time of day or frequency, you want to start a task, you could have tasks that are triggered by events. For instance, you can trigger this suggestion step to start when there's a certain amount of new data available in the source database. You could set up monitoring within your orchestration framework and trigger alerts to let you know if, for example, this transformation task fails to execute or hasn't been completed by a certain time. Many orchestration frameworks require you to set up your data pipeline, as what's known as a directed acyclic graph, which is really just an overly complicated term to describe how data flows through your data pipeline. Let's take a look at what a directed acyclic graph or DAG, for short would look like for this pipeline we've been talking about. 
In some sense, you could say this is already a DAG where each of these icons represents a different task in your pipeline, and the arrows show how data moves from one task to the next. But now I'm going to modify the visual here just to make it work explicitly like a DAG, as you'll see them represented elsewhere. It's called the source systems, source 1, source 2, source 3, and source 4, and we're ingesting or extracting data from all of these sources, and there's a transformation step happening in flight from source 4 here. Then you store all of the extracted data into storage. After that, your pipeline splits, where along this top branch, you have two more transformation steps followed by storage that will serve the machine learning end use case. On the bottom branch, you have a transformation step and then storage to serve the analytics end use case. The word directed end directed acyclic graph indicates that the flow of data goes only in one direction. 
Acyclic indicates that there are no loops. Data doesn't flow back to a previous step, and it can be described as a graph because it's composed of nodes and edges. You can think of a DAG as a flow chart for how data moves through your pipelines. You'll construct and deploy DAG within your orchestration framework of choice. As I mentioned, you'll be able to specify criteria and dependencies for how each task should be triggered and what monitoring and alerts you want to set up. You'll get plenty of practice setting up and running DAGs for some data pipelines and some popular frameworks later on in these courses. For now though, the main takeaway is that orchestration is an undercurrent that spans the entire data engineering life cycle, as well as key aspects of data ops. 
Join me in the next video, take a look at how software engineering, the last undercurrent relates to your role as a data engineer. 



##### Software Engineering

In this lesson so far, we've been talking about some pretty complex stuff when it comes to the undercurrents of the data engineering lifecycle, things like security, data architecture, operations, and management, as well as the orchestration of data pipelines. Of all the undercurrents, perhaps the most straightforward one to wrap your head around is the last one, software engineering. What I mean by that is, as a data engineer, you need to know how to read and write code. It's as simple as that. So I don't just mean hacking together some code that does whatever you need it to do right now, but instead to write production-grade code that's clean, readable, testable, and deployable. And so software engineering is the design, development, deployment, and maintenance of software applications. And they're not too distant. 
Past there was no such thing as data engineering as an official profession. There were just software engineers who occasionally dealt with data in their own work. Eventually, as businesses recognized the value of data, software engineers began taking on various aspects of data engineering as part of their work. With a growing diversity and volume of data in the recent decades, the data-oriented component of software engineering became much more intensive and eventually emerged as its own field. Over the years, those software engineers doing data engineering built a variety of fantastic solutions, such that nowadays, as a data engineer, you have access to a wide range of managed services and applications that allow you to more efficiently get the job done. And this is a good thing. It allows you to spend more of your time focused on the most important aspects of adding real value to your organization. 
In a sense, these existing services and applications allow you to move up the value chain, so to speak. This also means that as a data engineer today, you are often required to write much less code than your software engineering oriented ancestors of a decade or two ago. This doesn't mean, however, that coding is not important in your work as a data engineer. In fact, it's more important than ever that you can write great code and that the code you'd write is of top quality. For example, you'll be required to write core data processing code at all stages of the data engineering lifecycle. From ingestion to transformation and serving, you'll need to be proficient in frameworks and languages such as SQL, Spark, or Kafka. You're also likely to encounter Python or maybe Java virtual machine languages like Java or Scala, as well as Bash, for operating at the command line. 
You may be required to work in other languages as well, like rust or go. But if you focus on building strong foundational software engineering skills, you won't have much trouble moving between languages. In this specialization. We'll focus mostly on SQL, Python, and bash in the lab exercises, since these are the most common ways you'll interact with data as a data engineer also as a data engineer, you are likely to get involved in the development of open source frameworks. The way this tends to happen is that you adopt a open source framework to solve a particular problem, and you end up further developing the framework for your specific use case. As long as you write good code, you can make a pull request and add your contributions to the open source project to help others solve similar problems. Other efforts you'll be involved in or in the development of so called infrastructure as code, or pipeline as code solutions, which we'll talk about later in these courses. 
Apart from these specific instances of where you'll write code in your role as a data engineer, you'll also need to write code for everyday general-purpose problem-solving at all stages of the data engineering lifecycle. So, as I said right at the top of this video, as a data engineer, you'll need to know how to read and write code. Coding will be part of your everyday work, and your ability to write clean, readable, testable, and deployable code will translate into value for your organization. It's well worth your time to make friends with the software engineers in your organization and learn from them how to write great code. And with that, we finish this lesson on the undercurrents of the data engineering lifecycle. At this point, I'm sure you've had quite enough of all this theory stuff, and you're ready to roll up your sleeves and dive into applying some of these concepts in some practical exercises. Join me in the next lesson to take a look at how the data engineering lifecycle and undercurrents come to life on the AWS cloud. 


#### The Data Engineering Lifecycle on AWS
You may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key
Hi again. I hope you've been enjoying the lesson so far on the data engineering lifecycle and undercurrents. I'm excited to share with you how these concepts come to life with tools and technologies on the AWS cloud. In this video, I'll step through each of the stages of the data engineering lifecycle, as well as the undercurrents that Joe has presented so far, and connect them to these specific cloud tools you'll be using to build your data systems. When it comes to source systems on AWS, as Joe mentioned previously, the most common source systems you'll interact with are databases. And so in several labs in these courses, including the one this week, you'll be working with Amazon Relational Database Service, or RDS as its called. RDS is a service that provisions database instances with the relational database engine of your choice, like MySQL or PostgresSQL. 
RDS simplifies the operational overhead involved with provisioning and hosting a relational database, as well as taking care of tasks like patching and upgrading. In the next course, you'll also be working with Amazon DynamoDB, which is a serverless NoSQL database option. With DynamoDB, you create standalone tables, and these tables are virtually unlimited in their total size across all items in the table. DynamoDB has a flexible schema and is best suited for applications that require low-latency access to large volumes of data, such as gaming, IoT, mobile apps, and real-time analytics, where the data model can evolve over time without the need for complex migrations. In terms of streaming sources, in the last week of this course, you'll get a chance to work with Amazon Kinesis Data Streams that set up as a source system streaming real-time user activities from a sales platform blog. And although you won't be using message queues as sources in the labs, you might find yourself using something like Amazon Simple Queue Service, or SQS, to handle messages when building your own data pipelines outside of these courses. Another popular option for streaming source systems is Apache Kafka, which is an open-source streaming platform that you can either implement on your own or use Amazon's Managed Streaming for Kafka, or MSK, service, which makes it easier to run Kafka workloads on AWS. 
Because the underlying infrastructure is managed for you. You won't be using Kafka in the labs, but you will learn the details of Kafka in course two. When it comes to ingestion, if you're ingesting from a database, you might use Amazon's Database Migration Service, otherwise known as DMS. With DMS, you can migrate and replicate data from a source to a target in an automated way. But for the labs in these courses, you will mostly use the AWS Glue ETL service, which offers features that support data integration processes. And when it comes to ingesting data from a streaming source, you'll use Amazon Kinesis Data Streams and Amazon Data Firehose in the labs. But out in the wild, you could use one of the other streaming ingestion tools that I mentioned earlier, like SQS, Kafka, or others. 
With cloud storage, you'll get practice using traditional data warehouse options, including Amazon Redshift as well as object storage for a data lake on Amazon Simple Storage Service, which is known as S3 for short. We'll also look at how you can combine services into what's known as a lakehouse arrangement for seamless access to both the structured data in your data warehouse and unstructured data in an object storage data lake. For the transformation stage in these courses, you'll be working with AWS Glue as well as Apache Spark and DBT, which are tools that you can use in combination with Glue or as alternatives, depending on your needs. When it comes to serving data, we'll be looking at the two main potential use cases, which are, first, the business intelligence or analytics use case, and second, an AI or machine learning use case. For analytics, you'll use tools like Amazon, Athena or Redshift for querying structured and unstructured data. You'll also get some experience working with a dashboard in a Jupyter Notebook in this week's lab. And depending on the company and team you work with, you might also use dashboard tools like Amazon QuickSight as well as Apache Superset and Metabase, both of which are open-source options. 
For the AI and machine learning use cases, you'll serve batch data for model training, and work with some vector database options for serving data for product recommenders, and use with large language models. It's important to keep in mind that for each stage of the data engineering lifecycle, there are numerous other open source and managed service options. But I wanted to name a few specific ones here so you can start to connect some of the concepts you've been learning about to the tools and technologies you'll be practicing with in these courses. In the next video, I'll relate each of the undercurrents of the data engineering lifecycle to concepts and technologies on AWS. I'll see you there. 


#### The Undercurrents on AWS
The undercurrents of the data engineering life cycle that you've been looking at this week are security, data management, data ops, data architecture, orchestration, and software engineering. When it comes to how these undercurrents appear on AWS, there are some aspects that are more conceptual and others that are more tool oriented. For example, security as a concept will appear in many forms in your work on AWS. While something like orchestration is more about choosing the right tool or service to meet your needs. So in this video, I'll say a bit about each undercurrent as it will relate to your work on AWS so you can start to connect these ideas. So let's start with security. Last week, I mentioned the shared responsibility model as it relates to security of data systems or other applications built in the AWS cloud. 
In short, the shared responsibility model says that AWS is responsible for security of the data centers and services they provide. And you are responsible for the security of the systems you build with those resources. For example, if you store your data on Amazon S3, it is AWS's responsibility to make sure the S3 service itself is secure. And then it's your responsibility to make sure the access permissions are set correctly so your data is only available to people and applications that should have access to it. Speaking of access permissions, one key security concept on AWS is what's known as identity and access management or IAM. Through IAM, you can set up roles and permissions which control access to AWS resources, ensuring that users and services have the necessary access to perform their tasks securely. Within your data pipelines, you'll use IAM roles, which give users or applications access to temporary credentials that automatically rotate and provide appropriate AWS API permissions to various tools or data storage areas. 
Network security is also crucial for security on AWS. And you'll need to be familiar with services and features like Amazon Virtual Private Cloud or VPC, as well as security groups, which are instance level firewalls that are another key aspect of implementing secure data pipelines. With the undercurrent of data management in these courses, you'll use AWS Glue crawlers and Glue data catalogs, which allow you to discover, create, and manage metadata for data stored in Amazon S3 or other storage and database systems. You will also get familiar with Lake Formation, which helps you centrally manage and scale fine-grained data access permissions. All of these are also related to security, but I'm listing them here under data management because they have to do specifically with data privacy and discovery. For data ops, in the next course, you'll work with a service called Amazon CloudWatch that collects metrics and provides monitoring features for cloud resources, applications, and even on-premises resources. Then there is also Amazon CloudWatch Logs, which can help you store and analyze operational logs. 
In course two, you will also work with Amazon Simple Notification Service, or SNS, which provides a means of setting up notifications, either between applications or via text or email that are triggered by events within your system. There are also many open source observability tools you might use in your own work, like Monte Carlo or BigEye. With orchestration in these courses, you'll work with Airflow, which is an orchestration tool that you can implement as an open source tool or use a managed version from AWS. Airflow is currently the industry standard, but you should be aware of newer orchestration tools like Dagster, Prefect, and Mage that aim to solve some of the issues that Airflow isn't designed for. When it comes to architecture, we'll take a look at the AWS Well-Architected Framework next week, which is a set of principles and practices developed by AWS that can help you build systems with an eye towards operational efficiency, security, scalability, and sustainability. And with regard to software engineering, you may use tools like Amazon's CodeDeploy, which allows you to automate code deployment as well as various CI-CD tools. You'll also handle version control with Git and GitHub. 
And so that's a quick overview of some of the tools you'll be using as they relate to the undercurrents of the data engineering lifecycle. Next up, Joe's going to walk you through the first live exercise where you'll spin up an end-to-end data pipeline on AWS. 


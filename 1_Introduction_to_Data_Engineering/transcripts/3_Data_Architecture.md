## Data Architecture

#### Overview
Last week, we took a brief look at data architecture and how, in your work as a data engineer, you'll be most successful if you can think like an architect, even if data architect is not technically your role. This week, we're going to dive deeper into what it means to build good data architecture. We'll first take a look at how data architecture fits within the broader context of enterprise architecture, which is to say, the architecture for your entire organization. After that, we'll look at some specific architecture examples and how you can start to think about translating stakeholder needs into technology choices for your data systems. I'll also share with you some guiding principles you can keep in mind as you're learning to think like an architect. In the lab this week, you'll be evaluating the trade-offs when it comes to things like cost, performance, scalability and security for an actual data architecture on the AWS cloud. You'll also get a chance to explore the AWS well architected framework, which is a complimentary set of principles that can help you design robust and effective data systems. 
By the end of this week, you'll be armed with a set of tools that will serve you at all stages of your data engineering journey. Join me in the next video to get started. 


#### What is Data Architecture?
Before getting into the details of data architecture, I'd like to zoom out for a moment and look at how data architecture fits within the broader context of what's called enterprise architecture. Which can admittedly be a somewhat vague and abstract concept. In terms of a definition for enterprise architecture, there's no clear consensus out there and different groups have defined it differently. But here's a definition we settled on in the fundamentals of data engineering book. Enterprise architecture is the design of systems to support change in an enterprise, achieved by flexible and reversible decisions reached through a careful evaluation of trade-offs. Now, you might be thinking, hey, wait a minute, haven't we already seen this definition in the context of data architecture last week? And yeah, you'd be right. 
And in fact, the definition of data architecture that you saw last week was data architecture is the design of systems to support the evolving data needs of an enterprise. Achieved by flexible and reversible decisions reached through a careful evaluation of trade-offs. Given the similarity between these two definitions, you can see that data architecture is very well aligned with and fits within the context of enterprise architecture. In fact, enterprise architecture encompasses various domains, and you can think of these domains as including four main areas. The first is business architecture, which applies to the product or service strategy and business model of an enterprise. The second is application architecture, which describes the structure and interaction of key applications that serve business needs. Third, you have technical architecture, which relates to the software and hardware technology components that are required to support the deployment of business systems and applications. 
And finally, there's data architecture, which, as you've already seen, is all about supporting the evolving data needs of the enterprise. So you can think of data architecture as an element of enterprise architecture, and in this way you can start to see how your work as a data engineer ties in directly with the highest level goals and the structure of your organization. As the structure of an organization may change over time, this leads to another important concept, change management, which is right at the heart of enterprise and data architecture. You can expect the needs of your organization to be constantly evolving in your data architecture will need to adapt to those changes. Jeff Bezos, the former CEO of Amazon, is credited with the idea of one way and two way doors as it applies to any decision that is made in an organization. A one way door is a decision that is almost impossible to reverse. The door shuts and locks behind you, and there's no way to get back. 
As a simple example of this concept, you can think of it maybe in this way, say if you crack an egg to cook it, you can't change your mind later and uncrack the egg. So breaking an egg is a one way door decision. Or as this concept relates to an organization. Amazon, for example, could have decided at some point to sell AWS and all of its cloud services, or just shut it down. And after taking such an action, it would be nearly impossible for Amazon to rebuild a public cloud with the same market position. On the other hand, a two way door is an easily reversible decision, so you can walk through the door, and if you like what you see, you can proceed. Or if you don't like what you see, you can walk back through the door. 
For example, when it comes to choosing how you store your data in an object storage system like S three, you can choose from a range of storage classes based on your performance, data access, and cost requirements. An example of a two way door decision would be if you chose to store your data in the standard storage class in S three. Then later, if your storage needs change, you can transition to any other storage class for a fee. So this decision is reversible. Since the stakes attached to each two way door decision are typically low, organizations can easily make more of these types of reversible decisions, rapidly iterating and improving on how they collect, use, and store data. Flexible and reversible decision making is central to enterprise and data architecture. In aiming for two way door decisions wherever possible in your architecture, you'll be better able to handle change as it comes to your organization. 
If you find yourself faced with what looks like a one way door decision, see if you can break it down into a series of smaller decisions where each individual decision is a two way door. And so architecture is not simply about mapping out business IT or data processes and vaguely looking toward a distant utopian future. Architects need to be actively solving business problems and creating new opportunities. If you're thinking like an architect in your role as a data engineer, you'll build technical solutions that exist not just for their own sake, but in direct support of business goals. Next up, we'll be looking at the principles of good data architecture. But before we do that, I want to highlight a very interesting phenomenon known as Conway's law that affect the architecture of all data systems. The next video is optional, and I'm not going to test you on Conway's law, so if you want to skip ahead, that's perfectly fine. 
But I feel that I'm going to do you a disservice if I don't at least mention Conway's law in these courses, because I can guarantee that this will influence the types of systems that you build. 



#### Conway's Law
Throughout these courses, I want you to keep in mind that the various principles and architecture patterns that we discussed will apply to different scenarios. Depending on the type of architecture and systems that you're building to support your organization's objectives. But there is one very important guiding principle that has affected every architecture and system I've ever encountered. I want to introduce you to what's known as Conways law. Conway's law was best described by its author Melvin Conway, who stated that any organization that designs a system will produce a design who structure is a copy of the organization's communication structure. Now, that might sound like a very strained assertion to make, but here's how it works in practice. Imagine a company with four different departments, sales, marketing, finance, and operations. 
Now, if these departments operate in relatively isolated silos, their communication patterns, will be isolated and siloed as well. When it comes to building data architectures and systems, they will inevitably build four relatively isolated and siloed systems. In other words, sales will develop one data system, marketing, will build another finance third and operations, yet another system. You get the idea. If instead, the four departments in the same organization communicate cross functionally, share ideas and collaborate between departments, then the data systems they build will reflect the culture of cross functional collaboration and communication. Again, I realize this might sound strange, but as odd as it may seem, Conways law is remarkably consistent across all types of organizations. The main takeaway for you as a data engineer is that when it comes to understanding, what kind of data architecture is going to work for your organization? 
You first need to pay attention to and understand the communication structure of the organization. Even say, you attempt to build a data architecture that clashes with your company's communication structure, you're bound for trouble. If you're interested in learning more about Conways law, I have included a link in the resources section at the end of the week. Join me in the next video to have a closer look at the key principles of good data architecture. 


#### Principles of Good Data Architecture
Last week, I briefly mentioned nine principles to keep in mind in your approach to data architecture. Here they are again to jog your memory. We'll be revisiting these principles throughout the specialization. This week is all about data architecture. And so I'd like to spend a little more time on the details of each of these principles before we get into some specific architectures. In some sense, these principles are all related to one another. But for the purposes of this discussion, I'd like to break them up into three groups, which you can see here. 
To me, what these two principles in the first group have in common is that they both relate to how data architecture impacts other teams and individuals in your organization. The second group of principles are all about the fact that data architecture is an ongoing process and that you can expect your architecture to evolve over time. This third group is a set of unspoken but understood priorities that underlie any data architecture, namely that you need to be thinking about things like cost, security, scalability, and failure modes for any system you build. Of course, there are other ways you could think about combining or relating these principles. But here, I'll discuss them in these three groups. In this video, I'll talk about the first pair of principles that relate to how data architecture impacts other teams and individuals in your organization. In the next couple of videos, we'll take a look at the other two groups. 
Here I have choose common components wisely, and architecture is leadership. One of the primary jobs of a data architect, and potentially part of your job as a data engineer is to choose common components and practices that can be used widely across an organization. Common components can be anything that has broad applicability within an organization, including things like object storage, version-control systems, observability, monitoring and orchestration systems, and data-processing engines. Cloud platforms are an ideal place to adopt common components. For example, the separation of computed storage in Cloud data systems means you can serve data to different teams in organization with a shared storage layer that allows users to query the data for their particular use case. When you choose common components wisely, they become part of the fabric of the organization. Facilitating team collaboration and breaking down silos. 
This is not to say that there will always be common components that can provide the right solution for every team. Making a wise choice of common components means identifying use cases where teams can benefit from using the same data tools and practices while not creating obstacles to productivity by blindly applying a one-size-fits-all approach in your data systems. As a data engineer, you can practice architecture leadership by identifying the right common components in consultation with members in your organization. As you become more senior and take on more responsibilities, you might be the one to mentor others and provide proper training around these components as well. As I said before, as a data engineer, I also recommend you seek mentorship from data architects in your organization or elsewhere because eventually, you may well occupy the role of architect yourself. Join me in the next video to take a look at the second group of principles, which are all about building flexibility into your architecture. 


#### Always Architecting
In a previous video, I mentioned the idea of one-way and two-way doors, where one-way doors represent those decisions that you make that are difficult or impossible to reverse, while two-way doors represent reversible decisions. Systems built around reversible decisions allow you to always be architecting. To mention another concept that came out of Amazon, the so-called API mandate that came in the form of an email from Jeff Bezos to all Amazon employees in 2002. You can read more about the details of this mandate by following the link in the resources section at the end of this week. But the gist of this mandate was this. All teams must use service interfaces, otherwise known as application programming interfaces or APIs to communicate as well as to serve data and functionality. The mandate also ended by Jeff Bezos saying that those who don't follow this mandate will be fired. 
You can tell how seriously he took this notion of APIs at Amazon. What this meant was that no matter what kind of complicated mess any particular team had going on in their own systems, they would be required to provide any data, functionality, and communications to other teams, through a stable and predictable service interface. This allowed teams at Amazon to function together as a loosely coupled system where individual teams plugged into one another through these service interfaces, and any reconfiguration or retooling within given team did not affect the other teams. The other part of the API mandate was that all of the service interfaces or APIs had to be built from the ground up to eventually be public facing to developers in the outside world. This reorientation toward service interfaces laid the foundation for what would eventually become Amazon web services or AWS, which much of the world now uses as a cloud platform. In this next group of principles, I have make reversible decisions, build loosely coupled systems and always be architecting. As you already know, those reversible decisions are your two-way doors, The decisions that you can easily undo if you don't like the outcome. 
One key way to ensure your decisions are reversible is to build your data system from loosely coupled components. By loosely coupled in the context of architecture. I mean components that can be swapped out relatively easily without having to redesign your whole system. With a system built from loosely coupled components and reversible decisions, you will always have the ability to always be architecting. As we've already touched on, data architecture need to support the evolving data needs of your organization, and that means data architecture itself must also be able to evolve. As a data engineer, your job is not only to build systems that serve the data needs of your organization today, but also to have an eye toward the future so that you can constantly be adapting to the changes and business requirements, as well as available technology. Join me in the next video to look at the last group of principles, focus on best practices when it comes to understanding the cost, security, scalability, and failure modes for the systems you build. 


#### When your Systems Fail
Apart from building data systems that serve the needs of stakeholders, break down silos across teams, and evolve with the changing needs of your organization. If that's not already enough, you also need to anticipate what happens when things go wrong. Believe me, things will go wrong. In the next group of principles, I have planned for failure, architect for scalability, prioritized security, and embrace FinOps. When it comes to failure modes in your systems, it's best to take a practical and quantitative approach, just like you would with any other aspect of the performance for your system. Now we get to define more explicitly what is meant by terms that describe your system metrics, like availability, reliability and durability of your data systems. Availability often called uptime is the percentage of time a service or component is expected to be in an operable state. 
If you take a look at the different storage classes and Amazon S3 object storage, for example, you'll see that they range in availability from 99.5% up to 99.99% over the course of the year. Now, 99.5% and 99.99% might sound like high availability and even very similar numbers, but keep in mind that a 99.5% annual availability means that you can expect your storage system to be unavailable for around 44 total hours each year. While 99.99% availability means you can expect just about one hour of downtime per year. Unfortunately, 100% availability is never possible to guarantee, since failure scenarios can include unexpected power outages or impairment of network devices. But depending on the needs of your system, you can choose a storage class with the availability that you require. Reliability is similar to availability, but is instead the probability of a particular service or a component to perform its intended function within a given time interval, within well defined performance standards. Durability refers to the ability of a storage system to withstand data loss due to hardware failure, software errors, or natural disasters. 
In the Cloud, durability is crucial as businesses rely on Cloud services to store and access to critical data. Example, Amazon S3 boasts very high durability at 99.99999999. That's probably too many 9s to say out loud, but that's a total of 11 9s of data durability. Meaning the loss of objects in Amazon S3 is extremely rare. Related to the concepts of availability, reliability and durability, are the recovery time objective or RTO, for sure and the recovery point objective or RPO, for sure. The RTO is your maximum acceptable time for a service or system outage. To establish an RTO for your application, you want to consider the impact to internal and external customers if this application is unavailable. 
Then you can, for example, decide on the S3 storage class to meet the subjective. RPO, on the other hand, is a definition of the acceptable state after recovery. For example, when talking about a data storage system, the RPO might refer to the maximum acceptable data loss your system can tolerate due to an outrage. Being deliberate about your RTO and RPO for these systems you build will help you choose components with the right availability, reliability, and durability specs to meet your needs. That's one aspect of what it means to plan for failure. Another way your system can fail is through security breaches. That's why the principle of planning for failure and the principle of prioritizing security go hand in hand. 
We already talked a little bit about cultivating a culture of security and the principle of least privilege. Here, I'd also like to introduce what's called zero trust security. To understand what zero trust is all about, it's useful to take a look at what you might call a more traditional approach to security, which is known as hardened perimeter security. Taking a hardened perimeter approach is equivalent to building a big wall, like this around your systems where everyone and everything outside the wall is untrusted, while everything and every one inside the wall is trusted in the sense of having access to sensitive data and systems. The problem with a hardened perimeter approach is that would be attackers only need to get past the wall, to gain unfettered access to all of your data and your systems. In the Cloud era, the idea of building a hardened perimeter has the additional problem of there really being no physical perimeter with data and systems connecting across the Internet. Zero trust by contrast, means that every action requires authentication, and you build your system such that no people or applications, internal or external are trusted by default. 
Instead, access is granted only as needed. Your system can also fail when you do things like incur large unforeseen costs or miss opportunities for revenue. For example, when it comes to unforeseen costs, I'm talking about things like accidentally running expensive Cloud services such that your entire annual budget is consumed in a month or less. Believe it or not, I've seen this happen a lot? Or conversely, a missed opportunity might be getting a sudden spike in demand for your products and having your whole system crash because you weren't well prepared to scale up quickly. In this way, the principles of embracing FinOps and architecting for scalability, are connecting to the principle of planning for failure. As a data engineer, you need to think about the cost structures of Cloud systems. 
For example, when running a distributed cluster, what is the appropriate mix of AWS on demand, EC2 instances versus spot instances? By the way, spot instances or the unused EC2 instances that are available at a sharp discount on AWS. What is the most appropriate approach for running a sizable daily job in terms of cost effectiveness and performance? In the Cloud era, most data systems are pay as you go and readily scalable. Systems can run at a cost per query model, cost per processing capacity model, or another variant of a pay as you go model. It's now possible to scale up for high performance and then scale down to save money. However, the pay as you go approach makes spending far more dynamic. 
The new challenge for data engineers is to manage budgets, priorities, and efficiency when building and maintaining their systems. The main takeaways here are that if you plan for failure, or architect for scalability, prioritized security, and embrace FinOps, you'll be better positioned to serve the needs of your organization, not only when the systems you're building are functioning as expected, but also when failures happen. Next up, we'll dig into the details of some specific architecture approaches for different types of data systems. Join me in the next video to take a closer look at batch architectures. 


#### Batch Architectures
Last week we looked briefly at the concepts of batch versus streaming in the context of data ingestion. Now I'd like to focus on how some of these concepts appear in some well-established architecture patterns that you'll run into as a data engineer. The goal here is to get you thinking about some of the trade offs and implications of the different choices you can make with your architectures. In this video, we'll take a closer look at batch architectures, and in the next video, we'll take a look at streaming architectures. Batch data architecture is what you might call the traditional approach to data processing, where you ingest, transform and store data in batches or chunks. Batch processing is most practical when real-time analysis is not critical. Usually a batch of data consists of data collected over some fixed period of time, maybe over the course of a day. 
And for instance, in an e-commerce company, a data analyst might be interested in analyzing the history of sales of a particular product on a daily basis. And so you might set up a batch architecture where you ingest and process this data once a day. So what might this look like? This could be the beginning of a so called extract, transform, load, or ETL pipeline, where you first ingest or extract batches of data from one or multiple sources, perhaps into a staging area. And then apply some transformations to clean up, standardize, and model the data, and then load it into a data warehouse for storage and serving. There's also a variation of this pattern, known as extract, load, and transform, or ELT. With ELT, the idea is that after you ingest the data, you load it into your data warehouse and then perform transformations directly inside of the data warehouse. 
This ELT architecture or pattern is becoming more popular nowadays, given the expanded computational power in many modern cloud data warehouses and other storage abstractions. What happens next, whether you're working with an ETL or ELT architecture, is that you'll serve data for downstream use cases, which I'll represent on the right side of the data warehouse over here. These might typically be analytics or machine learning, but another possibility, as I mentioned before, is that your end use case is so called reverse ETL, where some analysis is performed and then the process data is actually sent back to the source systems that are at the start of your data pipeline. You could also have an additional layer here between your data warehouse and your end use case for something called a data mart. A data mart is a more refined subset of a data warehouse that is focused on a specific department, function, or business area. A data mart is designed to serve analytics and reporting. So you might have a data mart focus on sales, another for marketing, and yet another for operations. 
This kind of setup can make data more easily accessible to analysts and those who need to create reports. With data marts, you can also provide an additional stage of transformation beyond that provided by the initial ETL or ELT pipelines. These additional transformations might be things like additional joins between tables or aggregations that can help improve the performance of live queries. So these are some examples of typical batch processing architectures. If you are setting up an architecture like this for your organization, then there are a number of things you could be considering in terms of the principles of good data architecture. For example, if you're serving multiple end use cases for different teams or departments, how might you choose common components for your data warehouse and data pipelines that help facilitate collaboration and interoperability between teams? When it comes to planning for failure, you want to be thinking about what happens, for example, if a source system goes offline or the upstream data scheme changes. 
Connecting with source system owners would be a great first step toward building a system that can handle changes in the source. You'd also be looking for the availability and reliability specs for each of the components in your pipeline, and you'd be figuring out how to build flexibility into your system. For example, if you decide later to change the cadence of ingestion, or if you expect the volume in each batch of data to be dramatically different over time to embrace finops, you'd also be doing some cost benefit analysis to understand what kind of trade offs you might need to consider when it comes to performance of different components of your system, as well as what kind of value you can provide for the business under different scenarios. We'll be keeping these principles in mind throughout these courses, and remember that the technologies you choose when building your data systems will always present a certain set of risks, as well as opportunities that can add value to your organization. Next up, we'll take a look at some common streaming architectures. I'll see you in the next video. 


#### Streaming Architectures
As I mentioned last week, you can think of data as being produced in a series of events, and these events might be clicks on a website or sensor measurements or something else. In this sense, at its source, nearly all data could be characterized as a continuous stream of such events. That is, data is generally produced and updated in a continuous fashion with batch data pipelines, which we looked at in the last video. You're waiting for data to accumulate, then at a predefined time interval or once the data reaches a certain size threshold, you could process a batch of data, so you're just processing a stream of data in a series of chunks. Or a streaming data pipeline on the other hand. You're ingesting and providing data to downstream systems in a continuous near real-time fashion. When I say near real time, I mean, you're making data available to downstream systems a short time after it's produced. 
Possibly less than a second later. In the simplest sense, you can think of a streaming system as being composed of a producer, a consumer, and a streaming broker. The producer is the data source, and this might be click stream data coming from an application or measurements coming from an IOT device, and then there's a consumer. This could be, for example, a service or an application that will process the data, or it could be a data lake or a warehouse, and between the producer and consumer is your streaming broker that coordinates data between producers and consumers, and then downstream of the consumer might be some real-time analytics or machine learning end use case. The early to mid 2010s, the popularity of working with streaming data exploded with the emergence of Kafka as a highly scalable event streaming platform and other stream processing frameworks such as Apache Storm and Samza for streaming and real-time analytics. These technologies allowed companies to perform new types of analytics and modeling on large amounts of data, like user aggregation and ranking and product recommendations. This new demand for streaming data solutions didn't mean that batch processing went away. 
Instead, it meant that data engineers needed to figure out how to reconcile batch and streaming data into a single architecture. What's known as the Lambda architecture was one of the popular early responses to this problem. In a Lambda architecture, you have batch streaming and serving systems operating independently of each other, and the source system is simultaneously streaming data to two destinations. I say one for stream processing where the process data might be stored in a no SQL database, and one for batch processing, which might use a data warehouse to transform and store the processed and aggregated data for analytical purposes. The serving layer in this architecture then provides a combined view by aggregating query results from the batch and streaming layers. I wanted to mention the Lambda architecture here just so you're aware of it, but this architecture comes with various challenges and issues, like managing parallel systems with different code bases among other things. In many ways, technology and practices have moved beyond the Lambda architecture. 
But the Lambda architecture is still a good reference point for the streaming architecture designs and tools that came later. As a response to the shortcomings of the Lambda architecture. Actually, one of the original authors of Apache Kafka, J Kreps propose an alternative called Kappa architecture. The central idea for Kappa architecture is to use a stream processing platform as the backbone for all data handling, ingestion, storage, and serving. This facilitates a true event based architecture, meaning that rather than waiting for systems to periodically check for updates when things happen and when data is produced, information is automatically sent to relevant consumers that need the update so that these consumers can react more immediately to this information. With the stream processing platform as the backbone, you can apply real time processing by reading the live event stream. At the same time, you can configure the stream processor to retain a certain amount of historical data as it reads from the live stream. 
This effectively allows you to apply batch processing when you want to by replaying large chunks of the retained data from the same data stream. Well, Lambda has fallen out of favor and Kappa was never widely adopted. Both of these architectures provided inspiration and groundwork for overcoming the central challenge of unifying batch and streaming data processing. One of the central problems of managing batch and streaming processing is unifying multiple code paths. Today, engineers seek to solve this in several ways. Google developed the data flow model and the Apache beam framework that implements this model. The core idea in the data flow model is to view all data as events. 
Ongoing real-time event streams contain unbounded data. Data batches are simply bounded event streams, and the boundaries provide a natural window, and so real-time and batch processing can happen in the same system using nearly identical code. Apache Flink and other stream processing tools are widely used these days, and we'll take a look at these and similar tools in this course. In data engineering today, the philosophy of batch as a special case of streaming is now more pervasive than ever. In your work as a data engineer, you can expect to be confronted with the challenges of managing both batch and streaming pipelines. As you face these challenges, you'll need to keep the principles of good data architecture top of mind as you choose the components of your systems, build for flexibility and scalability, and anticipate potential failure modes. One thing you need to be thinking about, no matter what system you're building is compliance. 
In short, compliance means ensuring your data systems are in compliance with laws, regulations, and your own organizations privacy agreements, and terms of service policies. Join me in the next video to talk about architecting for compliance. 



#### Architecting for Compliance
Okay, before we get started here, I just want to come out and say it. Regulatory compliance is probably the most boring topic in these courses. I don't think anyone really wants to talk about laws and regulations, especially when we could be talking about working with data and working with cool technologies, right? So I agree, and I also feel I'd be letting you down if I didn't spend at least some time talking about how regulatory compliance fits into your role as a data engineer. And this is primarily because one of the most spectacular ways your data systems can fail is to run afoul of regulations and cause your organization to get sued and incur large fines. This does happen. So what kind of regulations am I talking about with regard to data? 
One big regulation is the General Data Protection regulation, or GDPR, that was enacted in the European Union in 2018. In short, the GDPR is all about the protection of privacy and personal data for individuals. But what constitutes personal data under the GDPR is relatively broad, including not only personal identifiable information, or PII, but also other information that could collectively be used to identify an individual. To be in compliance with a GDPR, you need to ensure that you have the appropriate consent from the individuals you're collecting data from, as well as the ability to delete the data in a timely manner. If an individual wants to have their data removed from your systems now, you might be thinking, what if my company isn't based in the EU? Or what if we don't serve customers in the EU? Well, technically, yes, the location of your company and your customers will at least play some part in determining whether the regulations apply to you. 
However, since the enactment of the GDPR, dozens of countries around the world, as well as individual states within the US, have adopted similar regulations. As a data engineer, you'll be responsible for building systems that not only comply with today's regulations, but also those of tomorrow. And those could be new laws that are enacted where you currently operate or regulations already in place in the areas your company expands to. In the future. It'll be your responsibility to keep your systems up to date, to stay in compliance with the set of regulations that apply to your business. The smart approach is to build systems that are in compliance with modern data protection regulations, like GDPR, even if your local regulations are less stringent. And to build flexible, loosely coupled systems that allow you to adapt to with regulatory changes. 
Apart from the location of your company and your customers, the industry you operate in might also come with its own set of regulations. If you work with healthcare data in the US, for example, you'll be required to adhere to the Health Insurance Portability and Accountability Act, or the HIPAA law, regarding sensitive patient data. Similar laws have been enacted regarding medical data in many countries around the world. Or if you work with financial data in your organization, you would need to adhere to the Sarbanes Oxley act in the US or similar laws elsewhere that mandate certain financial reporting and record keeping practices. So the main takeaway here is that no matter where you're located in the world or what industry you're in, there are laws and regulations that will apply to the systems that you build. As a data engineer, one way you can deliver value for your organization is to avoid the lawsuits and fines that come with failing to adhere to the necessary regulations. We won't be spending much time on the details of regulatory compliance in the rest of these courses, but I wanted to at least make you aware of this aspect in your role as a data engineer so you can keep it in mind along with the other principles of good data architecture. 
In the next lesson, we'll look at choosing the right technologies for your architecture. I'll see you there. 


#### Choosing Tools and Technologies
In the previous lesson, we looked at what it means to design good data architecture and why it matters. I emphasize that you need to consider the trade offs between different design choices and build loosely coupled flexible systems that will be able to accommodate the evolving data needs of your organization. In this lesson, I'll focus on how to choose the right tools and technologies to build this kind of architecture. In the field of data engineering, there's no shortage of options when it comes to tools and technologies to get the job done. Quite the opposite, in fact. As data engineers, we suffer instead from an embarrassment of riches. Whether you're considering solutions for ingestion, storage, transformation, or serving, you'll be faced with options, including open source, managed open source, proprietary software services, and much more. 
As you face these decisions, it's important to keep your end goal in mind, namely delivering high quality data products that meet the requirements of your end users. In other words, your data architecture is the what, the why, and the when of achieving the data needs of the business. The tools and technologies you choose to make that architecture, a reality or the how. At this point, you might be thinking, well, sure, that sounds logical. Choose the tools that will make for a successful outcome. That's great. Thanks, Joe. 
Well, unfortunately, there are a number of ways that this can go wrong, and that's what we're going to talk about in this lesson so that you're set up for success. We'll be looking first at location, namely the trade-offs between building your systems on-premises, on the cloud or some hybrid of the two. Then we'll look at cost optimization and how you can think about whether to build your own tools for certain things or purchase off the shelf solutions, considering things like your team's size and capabilities and what sorts of activities that really drive value in your business. We'll talk about building for the needs of today, while also having an eye towards the potential future needs of your organization. We'll be discussing all these things in the context of the principles of good data architecture that you saw in the previous lesson and the undercurrents of the data engineering life cycle that you saw last week. Join me in the next video to get started. 


#### Location
Not that long ago, maybe just two decades or so in the past. Building on premise data systems was really your only choice for any kind of data storage or processing needs. This was simply because modern Cloud data platforms didn't exist yet. On-premises system is one where the company owns and maintains the hardware and software for the entire data stack. This means that a company is operationally responsible for provisioning, maintaining, and updating and scaling their hardware and the software that runs on it. Nowadays, many companies build their entire data systems on the Cloud. With Cloud data systems, the Cloud provider, like aws, for example, is responsible for building and maintaining the hardware and data centers to serve client needs. 
If you are building your data systems on the Cloud, you are essentially renting the compute and storage resources needed for your system. The nice thing about Cloud computing and storage is that you can easily scale to meet demand or scale back down to save on costs when you don't need it. You don't need to maintain or provision any hardware, and you can change your mind relatively easily about exactly what kind of tools or technologies you want to use in your system. Well, many companies are now choosing to build data systems entirely on the Cloud. Other still maintain on premises systems, or some kind of hybrid system with some components on premises and others on the Cloud. The momentum in the industry is definitely in the direction of more companies choosing Cloud over on premises data systems. Or migrating from on premises to the Cloud. 
This is because of all the obvious advantages the Cloud offers in terms of flexibility and scalability. However, there are some companies that either choose to or are required to keep some or all their data systems on premises due to the nature of their business or regulations or security and client privacy concerns. As a data engineer today, it's possible that you'll work at a company that has some on premises systems or one that is in the process of migrating from on premises to the Cloud. In these courses, we'll be focused exclusively on building data systems on the Cloud. This is because for the vast majority of business use cases today, building your data systems on the Cloud is the best choice. The industry is moving in the direction of more Cloud and less on premises. As an aspiring data engineer, I believe your time will be best spent learning about building data systems on the Cloud. 
Join me in the next video to learn more about another trend in software and data engineering, the move from monolithic to modular systems. 

#### Monolith vs Modular Systems
Another concept in data engineering that I want to touch on briefly here, while we're talking about systems that contain hard dependencies and limited flexibility versus those that are closely coupled and flexible, is the idea of monolithic versus modular architecture. Monolithic systems are self-contained systems that are made up of tightly coupled components. In software development, monolithic architectures have been a technology mainstay for decades, where large teams work together to deliver a single working code base. All the components of that software product code base would be built and deployed as a single application. One advantage of a monolithic system is simplicity. It has all the functionality in one place. This means that it's easy to understand a monolithic system. 
Instead of dealing with dozens or hundreds of technologies, you have just one technology and typically one principal programming language. Monoliths are an excellent option if you want simplicity and reasoning about your architecture and your processes. However, monoliths are also very hard to maintain as they grow, since a monolith consists of tightly coupled components, if you need to update one component, you may have to update other components as well. Oftentimes a whole application has to be rewritten. For example, I once worked at a company that had a monolithic ETL pipeline that took at least 48 hours to run. Now, if anything broke anywhere in this pipeline, the entire pipeline process had to be restarted. Meanwhile, anxious business users downstream were waiting for the reports, which were already two days late by default, and often arrived much later. 
The team responsible for this pipeline desperately wanted to replace this pipeline, but it would have required weeks of downtime for the entire system. They limped along and accepted sub optimal performance because updating the system was just too daunting and expensive of a task. By contrast, modular systems consists of loosely coupled components. Instead of relying on a massive monolith that combines all the functionalities of an application, modular systems rely on breaking the application into self contained areas of concern. In software development, truly modular systems emerge with the rise of micro services. With micro services instead of combining the components that correspond to multiple services into a single deployable entity, each service is deployed as a single unit. Modern data engineering. 
Data processing technologies have shifted toward a modular model by providing strong support for interoperability. This means that most data processing tools available today can be easily integrated with tools that support the other stages of the data engineering life cycle. For example, data stored in object storage in a standard format such as Parquet can be paired with any processing tool that supports the Parquet format. The ability to swap out tools as technology changes is invaluable. It helps you build good data architecture by enabling flexible and reversible decisions and continuous improvement. Through the rest of this lesson, we'll talk through the details of the various choices you'll make to implement your architecture. Things like cost optimization, whether to go with open source or mana solutions and everything else. 
Again, I'll be presenting these ideas from a Cloud first and modular perspective because these are the directions that data engineering as a field is going. Join me in the next video to take a look at cost optimization. 


#### Cost Optimization and Business Value
As I said before, at every stage of the data engineering life cycle, you'll have multiple tools and technologies to choose from in order to get the job done. Each of these choices comes at a cost, and I'm not just talking about the price tag and the software or service you're subscribing to. There's also a cost associated with implementation, namely paying a team to spend the time it takes to stand up and maintain the system, and there's also opportunity costs, meaning in choosing one tool, you are at least for a time for going the opportunity to choose another. And so your tool and technology choices will significantly impact your budget. As a data engineer, your job is to provide a positive return on the investment your organization makes in its data systems. In this video, we'll look at costs through three main lenses. I'll first focus on the total cost of ownership, or TCO for short. 
After that, we'll take a quick look at the total opportunity cost of ownership or TOCO or TOCO for short. Finally, we'll revisit FinOps, which we touched on briefly last week. The total cost of ownership, or TCO, is the total estimated cost of a solution project or initiative over its entire lifecycle. The term TCO is not specific to data engineering, but rather it's a general business term used to describe the total investment in some project, including the direct and indirect costs of the products and services you're using. This includes things like the acquisition of hardware and software, the cost of management, maintenance and repairs, as well as any required training. And so when it comes to data systems, direct costs or the tangible, easy to identify costs that are directly attributed to the development of a data product. For example, your direct costs include the salaries of the team working on the initiative or the AWS bill for all the services used, as well as any fees or licensing costs of software subscriptions. 
Your indirect costs, almost known as overhead or expenses that are not directly attributed to the development of a data product. For example, this could be the costs incurred due to network downtime, ongoing IT support, or the loss of productivity of certain team members. It's important to include indirect costs when estimating TCO because these costs can be significant. When it comes to the cost of hardware and software, these expenses generally fall into two big groups. The first group is capital expenses or CapEx for short. CapEx is the payment made to purchase long term fixed assets. This type of expense for data systems was common before the existence of Cloud platforms. 
Companies would make an upfront payment to purchase hardware and software, and then install them in data centers. These upfront investments, commonly hundreds of thousands to millions of dollars or more would be treated as CapEx assets that would slowly depreciate over time. Nowadays, with the shift to the Cloud, many companies are building data systems with essentially zero CapEx. The other major type of costs is operational expenses or OpEx for short. OpEx is an expense associated with running the day to day operations, so it spread out over time. In data systems, OpEx often appears as a pay as you go expense in the form of recurring subscription fees or the cost of using a particular Cloud service. In the context of building on premises versus Cloud based data systems, building on premises generally incurs a large CapEx cost. 
Well, Cloud based systems can be almost entirely OpEx. Now, before Cloud platforms existed, an OpEx first approach wasn't really an option for large data projects. This is now changed with the advent of the Cloud as data platform services allow you to pay on a consumption based model. Long term hardware investments for data projects will inevitably become obsolete, so I suggest that you take a Cloud centric OpEx first approach by choosing flexible pay as go technologies for your data pipelines. By contrast to TCO, what I call the total opportunity cost of ownership or TOCO, TOCO for short, is the cost of lost opportunities that you incur in choosing a particular tool or technology. Can be harder to quantify, but it essentially means that any choice you make inherently excludes other possibilities. If you choose data stack A, which includes a certain set of technologies to build your data pipelines, then you've chosen the benefits of data stack A over all the other options, effectively excluding, say, data stack B, C D, and so on. 
In this case, the total opportunity cost of ownership is the cost of being held captive to data stack A while no longer benefiting from other data stacks. In the event that data stack A turns out to be the best possible choice, then congratulations. Total opportunity cost of ownership is essentially zero. In reality, however, data engineering tools and technologies are evolving very rapidly. Even if you make the best possible choices today, you still need to evolve your data systems in the future. If some components of data stack A, which were the best choices for yesterday, have now become obsolete, then there will be a cost associated with switching to a different components or a different stack entirely. In order to ensure that your total cost of ownership is minimized, you'll need to build flexible and loosely coupled systems that are easy to update as your data needs change and the landscape of tools and technologies evolves. 
One way to do this is to recognize upfront which components of your data pipelines are most likely to change. In other words, separate the immutable technologies from the transitory technologies. Immutable technologies are those that have stood the test of time. In Cloud storage, these are things like object storage and networking. Or as another example, SQL as a query language has been around for decades and isn't going away any time soon. Transitory technologies, or at least those that are most likely to be transitory, are those that are new at the cutting edge and in areas of the data stack that are rapidly evolving, like stream processing, orchestration, and AI, for example. When it comes to thinking about technology choices in the context of cost optimization. 
FinOps as a concept is closely related to TCO and TOCO. FinOps is about minimizing the costs associated with your data systems, your TCO, and your TOCO, while simultaneously maximizing your opportunity for revenue generation. How do you do that? In short, you could choose Cloud based services that allow you to take an OpEx first approach with flexible pay as you go technologies as well as modular options that allow you to iterate, improve, and grow quickly. Next up. We'll zoom in a little closer on the topic of choosing the right tools and technologies while optimizing cost. Join me in the next video to take a look at the trade offs between building your own data system components versus purchasing off the shelf solutions. 


#### Build vs Buy
So far, we've been talking about choosing tools and technologies for your architecture, and I've emphasized that, in general, building on the Cloud with flexible pay-as-you-go services will be your best bet for the vast majority of companies, when it comes to common services, like, say object storage. Using a Cloud service like Amazon's S3 is a much better choice than attempting to build your own custom object storage solution. However, depending on the requirements of your system, there may be certain tools or technologies that you need to build out and customize yourself. For example, many teams elect to build on top of Open-Source frameworks to get exactly the solution they need. In other cases, a team might choose to build their own solution or customize Open-Source frameworks to avoid licensing fees or simply avoid being at the mercy of a vendor. In fact, for essentially all stages of the data engineering life cycle, you'll have a range of options when it comes to choosing tools and technologies. Of course, for any tool you need, you can just build something from scratch. 
In some cases, this might be your only option if you're attempting to do something that no one else has done before. But for most cases, however, this is not recommended unless you are certain there's no existing solution out there for the thing you're trying to do. Building technologies from scratch when off the shelf solutions already exist can amount to reinventing the wheel, so to speak. You'll hear me refer to this activity as undifferentiated heavy lifting in the sense that it's hard work, and it probably doesn't ultimately add value for your organization. When it comes to existing solutions, these include fully Open-Source options, as well as commercial Open-Source options from vendors, which are essentially a managed version of some Open-Source tool. Then there may also be proprietary non Open-Source software and services to choose from. When it comes to choosing between these options, there are several key parameters you need to consider. 
First off, if you elect to go with a fully Open-Source solution, does your team have the bandwidth and capabilities to implement and maintain that system? Many Open-Source tools have a great community supporting them where you can get help if you need it. But if you're part of a small team, maybe even a team of one, then it's possible that a managed Open-Source or proprietary service will better suit your needs. That's because it can free you up to build and manage your entire data system without getting stuck in the wheels, deploying, and maintaining a single component. Beyond that, even if your team does have the expertise and bandwidth to build from scratch and implement an Open-Source solution. Is it really worth it? On the surface, building something yourself or using an Open-Source solution might seem like a cost savings because you're avoiding licensing fees. 
But the total cost of ownership, as we talked about in the last video, it's much more than just licensing costs. It also includes the cost of the team required to build and maintain the system. In addition to costs, the other important point to consider is whether building out and maintaining a custom or Open-Source solution actually provides value for your organization. In other words, do you get some advantage by building your own system or using Open-Source over what you would get with a managed service, or is it just undifferentiated heavy lifting? In other words, just hard work that doesn't provide any additional value. For most teams and particularly small teams building data pipelines and wondering whether to build their own, use Open-Source or purchase commercial Open-Source or proprietary tools. My suggestion would be to first look at Open-Source or commercial Open-Source solutions, and if you can't get what you need, look at buying a proprietary solution. 
Plenty of great modular services are available to choose from in either case. This will allow your team to focus on the unique opportunities that provide the most value to your organization. Join me in the next video for a look at the differences between so called serverless versus server options for tools and technologies. 


#### Server, Container, and Serverless Compute Options
To host any software application, you need a server, which is essentially a computer. Or a collection of computers that powers your application by providing CPU, memory or ram as well as disk storage and maybe a GPU and networking. Servers provide computing resources over a network, most often through the Internet. When it comes to cloud tools, depending on the service you're considering, you may be required to set up and manage the compute resources required to run the application. And in other cases, you may be able to choose between one or more of the following three computing options. Server container or serverless in this video, I'll go over the differences and trade offs between these three options. If you choose to go with the server version of a service, you will be responsible for setting up and managing the server, such as an Amazon EC2 instance that the service will run on. 
Including updating the operating system, installing or updating packages, software packaging, networking, scaling and security. In contrast to a server, a container is a more modular unit that wraps up your code and all its dependencies into a package that can run on a server. So whereas a traditional virtual machine wraps entire operating system. A container is more lightweight as it just packages up in an isolated user space such as a file system and a few processes. So for a containerized solution, you would still be responsible for setting up the essential elements of the application code and dependencies. But the underlying operating system, networking and all the rest would be provided apart from server and container options for services. It's becoming increasingly common in the world of cloud data tools to encounter the terminal serverless to describe a particular service. 
If you're familiar with the way computers work, a word like serverless might sound a little strange, like how do you run software without a server, right? Well, it turns out that the term serverless doesn't actually mean there is no server. It just means that setting up and maintaining the server is not your responsibility for that particular service. You can interact with the application without managing the servers behind the scenes of or worrying about package installations and dependencies. So the server is essentially hidden from you, like I've drawn here. Typically, serverless technologies run on containers, so these services can automatically scale. They have availability and fault tolerance built in, and they offer pay as you go billing. 
But in the case of serverless services, the containers they run on were also abstracted away. In this way, serverless technologies can allow you to spend less time worrying about your compute infrastructure and more time focusing on developing data products. In the previous week's lab, you worked with a number of serverless services such as Amazon, Athena and AWS Glue. The serverless trend kicked off in full force with the launch of AWS Lambda in 2014, which is a service that allows you to run code in response to an event. With the promise of executing small chunks of code on an as needed basis without having to manage a server. Serverless options have exploded in popularity and diversity, and now go far beyond just running code snippets on demand. The main reasons for this popularity are cost and convenience. 
Instead of paying the cost of a server, you can just pay a little bit each time your code is run or when you use a particular service. So when does it make sense to use serverless services? As with many other cloud services, it depends. As a data engineer, you need to understand the details of cloud pricing to be able to predict the cost of your serverless deployments and decide if it's more cost effective than the server option. For example, looking at the pricing for AWS Lambda, you'll find that using the service in an environment with high rates of events can be catastrophically expensive. As in other areas of your data pipelines, it's critical to model and monitor the services you use, serverless or otherwise. You may need to monitor directly to determine actual event rates, duration and cost per event in a real world environment. 
So you can model the overall cost of a serverless service versus some alternative. Apart from that, cloud serverless platforms have limits on execution, frequency, concurrency and duration. If your application can't function neatly within these limits, it's time to consider a container oriented approach so you can think about it like this. Serverless works best for simple, discrete tasks and workloads. It's not going to work as well if you have many moving parts or require a lot of compute or memory horsepower. In that case, consider using containers and a container workflow orchestration framework like kubernetes. For most modern data engineering applications in the cloud, you can get the job done with serverless or containerized tools. 
So I would recommend looking at using serverless first, and then containers and orchestration if possible. And once you've outgrown these serverless options. Join me in the next video to wrap up this lesson with a look at how the undercurrents of the data engineering lifecycle come into play when choosing tools and technologies for your data architecture. 


#### How the Undercurrents Impact Your Decisions

To wrap up this lesson, I'd like to briefly touch on how each of the undercurrents of the data engineering lifecycle come into play as you choose tools and technologies to build out your data architecture. Last week we looked at each of these six undercurrents, security, data management, data ops, data architecture, orchestration, and software engineering, as they relate to the data engineering lifecycle. Now we'll take a closer look at how these undercurrents relate to choosing the individual components for your data architecture. When it comes to security, different tools have different security features. It's important to understand what those features are and make sure that you put the right authentication technology in place, as well as other best practices we talked about one big thing to watch out for is to only use software and tooling developed by reputable organizations and trusted open source communities. There are known instances of organizations or nation states pushing out data tools that contain suspicious components, essentially spyware that compromises your data pipelines. I won't say any more about that now, but the takeaway is to make sure that you know where your tools come from. 
If it's an open source tool, take a look at the code and make sure you understand how it's implemented. With data management, it's not always clear how certain data governance practices are implemented, and it's a good idea to ask the company or community providing the tool how governance is handled. For example, how will your data be protected against breaches both from the outside and from within? How does the tool comply with GDPR and other data privacy regulations? Or how does the tool provide for verification of data quality? When it comes to data ops, choosing the right tools is mostly about understanding what features they offer in terms of automation and monitoring. If you're looking at a managed service option, be sure to understand the provider's service level agreement, or SLA, that describes their guarantees around reliability and availability. 
With data architecture, as we've talked about throughout this week of materials, you need to look for how any given tool provides modularity and interoperability with other tools. Good modularity and interoperability allows for flexibility and loose coupling between components. For orchestration, the data engineering landscape is currently dominated by Apache Airflow, which you can implement as an open source or managed tool. There are also other offerings like Prefect, Dagster, and Mage that are growing in popularity as well. If you're looking at orchestration tools, just be aware that the space is rapidly evolving and a deep understanding of your own data architecture goals will help you determine which orchestration tools are best suited to your needs. When it comes to software engineering, the big question is, how much do you want to do? What I mean by that is, given your evaluation of your team's bandwidth and expertise, as well as what kind of development activities really add value in your organization. 
Do you want to build your own tool, or go with an open source option, or sign up for a commercial open source or proprietary solution? The main thing to avoid is undifferentiated heavy lifting, which is to say, hard work that doesn't add value for your organization. Check out open source and commercial open source tools first. If they can't meet your needs, then look at proprietary tools. That's it for this brief overview of how each of the undercurrents of the data engineering lifecycle come into play as you're choosing tools and technologies to implement in your data architecture. Once again, we've covered a lot of ground talking about data engineering somewhat in the abstract. I know it might seem like we focus a lot on theory so far, and you're probably excited to get practicing these concepts that you've learned. 
That's great, because there's a lot of hands on practice coming soon. Join me in the next lesson to take a look at the AWS well architected framework and evaluate the architecture choices for your own data architecture. On AWS. 

#### Intro to the AWS Well-Architected Framework
So far on this course, you've been learning about data architecture fundamentals. And the many considerations you need to take into account when designing and choosing tools for your own data architecture. In this lesson, we'll start by taking a look at the AWS well architected framework. This framework consists of a set of principles and best practices that will help you build scalable and robust architectures on AWS. The well architected framework complements the set of principles and practices that we've already been talking about in this course. In fact, in writing the fundamentals of data engineering book and articulating the key principles we felt were most important for designing good data architecture. My co author Matt Housley and I took inspiration from the AWS well architected framework as well as other sources. 
If you do a quick search for the AWS well architecture framework, you'll find all kinds of great resources. Including white paper's focus on specific use cases and a tool that AWS provides to help you in applying this framework. In fact, the framework itself could be the subject of an entire course. For the purposes of this course in the next video, Morgan will give you an introduction to the six key pillars of this framework. And show you where you can go to learn more and get more practice with the framework. And after that, I'll see you in the following video for a walkthrough of this week's lab exercise. We will get a chance to apply the principles of good data architecture on the AWS cloud. 

#### The AWS Well-Architected Framework
When you're building systems on AWS, particularly when you're just getting started, it can be overwhelming. There are just so many options to choose from and different ways you can build out solutions on AWS. So how do you know if you're getting it right? Well, that's what the AWS Well-Architected Framework is all about, evaluating and improving solutions so you can get it right when you're building on AWS. In this video, I'll give you an overview of the Well-Architected Framework and point you to more resources so you can use this framework to evaluate whatever you build on AWS. Let's begin with a little background. At AWS, we don't just provide services for building systems in the cloud. 
We also work closely with thousands of customers around the world to help them build the best cloud solutions to support and run their businesses. AWS solutions architects, subject matter experts, and other AWS personnel have spent decades working directly with customers across a wide variety of business needs and use cases, which has given them a lot of experience with doing things the right way. From that collective experience, AWS has constructed the Well-Architected Framework, which comprises a set of best practices and core strategies for architecting systems in the cloud. The AWS Well-Architected Framework includes six key pillars: operational excellence, security, reliability, performance efficiency, cost optimization, and sustainability. Here, I'll just briefly describe each of these pillars, and you can follow the links in the resources section at the end of this week to learn more if you're interested. The first pillar is operational excellence. This pillar is focused on how you can develop and run your workloads on AWS more effectively, monitor your systems to gain insight into your operations, and continuously improve your processes and procedures to deliver business value. 
Next is the security pillar, which is focused on how to take advantage of cloud technologies to protect your data, systems, and assets. You've already looked at the security undercurrent of the data engineering life cycle with Joe, and the idea here is the same. You need to employ proper tools to secure your systems, as well as encourage a culture of security on your team. Next up is the reliability pillar. Reliable systems are those that perform their intended function correctly, consistently, and can recover quickly from a failure. So this pillar is about everything from designing for reliability to planning for failure and adapting to change. The performance efficiency pillar is focused on taking a data-driven approach to building high performance architectures. 
When it comes to your system's performance efficiency, you'll be evaluating the ability of a set of computing resources to efficiently meet your system requirements, as well as how you can maintain that efficiency as demand changes and technologies evolve. The next pillar is cost optimization. This one is pretty straightforward and closely related to the points Joe made earlier this week about embracing FinOps. Simply put, cost optimization means building systems to deliver maximum business value at the lowest possible price point, and AWS provides a range of services, including the AWS Cost Explorer and Cost Optimization Hub, where you can make comparisons and get recommendations about how to optimize for cost for your systems. Finally, we have the sustainability pillar. While things like performance, scalability, security, and cost might be top of mind when building out data systems, it's also important to consider the environmental impact of your workloads that you're running on the cloud. The sustainability pillar is focused on reducing energy consumption and increasing efficiency across all components of your system. 
Now, it's important to keep in mind that these six pillars of the Well-Architected Framework will not provide specific designs for you to just copy and apply to your solutions. Instead, you can think of them as a set of principles and questions that will help you have productive conversations about your existing solutions and help you design and operate reliable, secure, efficient, cost-effective, and sustainable systems in the cloud. It's almost like having access to your own personal AWS Solutions Architect, who can help you think through the pros and cons of different architecture choices. As I mentioned before, you can follow the links in the resources section at the end of this week to learn more about each of the pillars and explore the Well-Architected tool that allows you to evaluate your own architectures for potential risks and opportunities for improvement. There are also domain-specific applications of the Well-Architected Framework called Lenses that you can explore. A lens is essentially an extension of the AWS Well-Architected Framework that focuses on a particular area, industry, or technology stack, and provides guidance specific to those contexts. Each lens has its own set of questions, best practices, notes, and improvement plan. 
In particular, I'd recommend you to check out the Data Analytics Lens, which is focused on data-specific considerations. The Data Analytics Lens will walk you through evaluating your data architectures for scalability, security, performance, and cost. It can help you evaluate your current data architectures, identify areas for improvement, and implement strategies that align with industry best practices. Next up, it's your turn to try applying the principles we've been talking about this week to your own data architecture on AWS. Through the next few videos, Joe will walk you through the lab exercise for this week, and I'll see you again next week. 

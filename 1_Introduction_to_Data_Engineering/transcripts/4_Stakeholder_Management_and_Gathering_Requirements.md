## Stakeholder Management and Gathering Requirements

#### Overview
In the first week of this course, we took a brief look at requirements gathering as it relates to your work as a data engineer. We looked at how a conversation might go between you and a data scientist at your company about their needs for the analytics and machine learning projects they're working on. You saw how a conversation like this could quickly reveal the need for you to talk to other stakeholders. In this case, the marketing team and software engineers. We also took a look at a framework for thinking like a data engineer, where you first identify business goals and stakeholder needs, then define requirements for your system based on those needs, then choose tools and technologies that meet your requirements, and then finally build and deploy your system. Now there is a lot packed into this little framework, and even when I add in the details of what occurs during each step, things like asking what action your stakeholders will take in the first step, translating stakeholder needs into system requirements in the second step, prototyping and testing in the third, or evolving your system in the last step. You can see that there are still many things that are either left unsaid or unwritten here. 
Weeks 2 and 3 of this course were designed to fill in many of those details so that you can form a more complete mental model of the stream work, the field of data engineering, and what your role as a data engineer will look and feel like in practice. In Week 2, you looked at the stages and undercurrents of the data engineering life cycle. There, we focused not only on the components that make up your data systems, but also on the people involved, including source system owners and downstream end users that you'll work with. In Week 3, we took a deep dive into the principles of good data architecture, where you saw that things like planning for failure and choosing common components or elements of your system design that you should be thinking about right from the start. Now you have all the pieces of that mental model that will be critical in your work as a data engineer. This week, we're going to pull all those pieces together in the context of a practical on the job scenario. We'll start with requirements gathering, then choosing tools and technologies, and finally implementation of your system. 
What I would like to do next is pick up where we left off with those requirements gathering conversations in the first week of this course and follow through to defining and documenting your functional and nonfunctional requirements, as well as the tools and technologies you could use to build a system that meets those requirements , and throughout the week, we'll be pausing along the way to talk about some concepts and best practices to keep in mind when you encounter new and different scenarios out in the wild as a data engineer. At the end of this week, you'll translate the requirements into an architecture design and then build your system on the AWS Cloud. Join me in the next video to get started. 


#### Requirements
Like I said in the previous video, this week is all about pulling together the various aspects of the thinking like a data engineer framework that we've been exploring throughout this course so far. Now, remember that framework begins with identifying business goals and stakeholder needs through the process of requirements gathering. At this point, I'd like you to start thinking about requirements as a hierarchy of needs. At the top of this hierarchy, you have the goals and objectives of the business. These are the things that describe what success looks like for the business as a whole. These might be objectives around growing revenue or market share or user base or something else that represents an optimal outcome for the business. The next thing down on the hierarchy is stakeholder needs. 
Now, you'll hear me say stakeholders a lot in these courses. But in this case, you can think of these stakeholders as the individual employees of a business, and each of them plays a role in delivering on the high level business objectives. To do the work successfully, stakeholders have needs. They need things like proper resources, tools, and management, among other things. For the purposes of these courses, you can assume that they also have a need for robust data systems. Below stakeholder needs in this hierarchy, you have system requirements. In principle, this could be any system, but here we're talking about the data systems you will build and maintain. 
This is a set of requirements that your data systems must satisfy in order to serve stakeholder needs. Well then, separate system requirements into functional and non functional requirements. Functional requirements are those system requirements that can be expressed in terms of a specific functionality namely, what the system will be able to do to meet the needs of stakeholders. For example, a functional requirement for a data system that monitors bank transactions for a fraudulent behavior might be the system is able to immediately report potentially fraudulent transactions. Non-functional requirements, on the other hand, are those system requirements you could think of as characteristics or attributes of the system that allow it to function properly. These could be characteristics or attributes related to things like latency, scalability, reliability, cost, or security. For example, a non-functional requirement in terms of scalability for a streaming pipeline ingesting data from an e-commerce platform might be, the system must be able to scale up to ingest data from 10,000 users shopping at the same time. 
There you have it. This is a hierarchy of needs for business as it relates to your work as a data engineer. The main takeaway here is that like we've been talking about all along, your work in building data systems is directly connected to the work of others in your organization and to the overall goals of the business. This is why it's so important to understand the needs of your stakeholders and the objectives of the business for the work that you do. That brings us back to requirements gathering. In the first week of this course, we started by having a conversation with the data scientists to understand their needs. You could think of that exercise as requirements gathering that started somewhere in the middle of this hierarchy. 
Ideally, though, you'd like to start at the top. That means talking to leadership about the goals of the company. If you work at a small company, you might even be able to have a conversation with the CEO. I would definitely recommend that. If you're at a bigger organization, you just need to go up as far as you can up the chain of command to get oriented with the goals of the business. In the first week of the course, you saw a conversation I had with Soul, who's been the chief data officer at many large organizations. Now, not all organizations are going to have a CDO, but most will have a chief technology officer or CTO, who is the technology executive in a company. 
The CTO is responsible for leading the engineering department to use technology to improve products and services with a goal of growing the business. In the next video, I'll introduce you to a friend of mine, Matt Housley, who has a lot of experience interfacing with C-suite execs, including CTOs, and he's also my co-author for the book Fundamentals of Data Engineering. He'll share some advice on how to get started in data engineering and how to engage with business leaders to come up with solutions to business problems. Then we'll see if we can get him to join our requirements gathering game this week, where he plays the role of CTO at the company you've just been hired at as a data engineer. Join me in the next video to meet Matt. 


#### Conversation with Matt Housley
So I've got a very special guest for you, my co author and best friend, Matt Housley. It's good to see you. >> It's great to be here, Joe. >> Thank you. >> Thank you for the invitation. >> Absolutely. >> So yeah, we wrote this book together, and that was kind of the starting point ultimately for this course. 
Maybe you could talk a bit about what motivated us on the book. >> Yeah. So, fundamentals of data engineering was written from a place of noticing a gap in the book space and the education of data engineering. The materials were largely focused on tools and technologies and didn't really take a step back and attempt to define the field of data engineering itself, the more provide a mental framework for how to operate and think like a data engineer. And so, really, that's the perspective that we entered into writing the book from, is that we didn't really want to write a tutorial on technology, X, Y, or Z or cloud provider ABC or whatever, but really provide a first principles look at the field of data engineering. >> Yeah, fundamentally, I think our starting audience, our core audience for the book was people who had a technical background but wanted to move into data engineering. We also kind of had various secondary audiences. 
So, for example, if you were a product manager who was somewhat technical and needed to work a lot with data engineers on data products or data that was embedded into your application, we also thought this book would help you a lot. Maybe not every piece of it, but you would have a lot to learn there. And I think the idea that the audience for this course is very similar. People moving into data engineering, but also people who can benefit just from a better understanding of the concepts and a little bit of hands on exposure. >> So we talked about the book, and the learner's taking a course, obviously. So how do these things tie together? I think that fundamentally, the book and this course complement each other. 
You've already learned about the data engineering lifecycle and the undercurrents, but this course allows you to apply that mental framework with hands on examples that you're likely to encounter in the real world. And so for this course, we're using tools and technologies and a very popular cloud, AWS that will give you the hands on experience to really tie again, the concepts that you learned in the book with a practical know how of hands on labs and practice. So we often get questions about how to get into data engineering. Maybe you can talk a little bit about some advice for people who want to get into data engineering. >> Yeah so I think, first of all, it's very helpful to learn just core data concepts. I think we frequently find that even if you're coming from like, a software development background, you may not have enough experience with kind of how analytics data is used to immediately jump into it. And so it's helpful to start with just playing around with data, even in something like Microsoft Excel, so you know what tabular data looks like and what kinds of things you consume with data, and then you can sort of expand the circle from there. 
I think one thing we always want to emphasize is that getting into data engineering, getting your first job, is not just about learning a particular stack of tools. Rather, in those early stages, it's good to get experience with a lot of different tools to develop general competence. And then when you start getting into roles, you're going to really focus down on the tech stack within the company where you're working, right? >> Yeah, and I think that's where a book like fundamentals of data engineering really helps, is it provides you with the mental framework to start thinking like a data engineer. And when you have that mental framework, then you can basically put whatever tools into the proper buckets and you'll be able to function as a data engineer. Well, Matt, this has been a great discussion about the book and the course. Maybe if you're up for it, we can do a mock conversation where I will be a data engineer and we're going to talk to you, who is going to play the role of a chief technology officer at this fictitious e commerce company that I've been hired to work at. 
>> Joe and I, running ternary data together and doing consulting, had the privilege of working with a lot of different companies from various domains, ranging from technology companies to operational and logistics companies and in scale, all the way down to ten person startups, all the way up to like big multinational enterprises. And so in that process, we learned a lot about the kinds of things that C-level executives in general and chief technology officers specifically care about when it comes to engineering and when it comes to data. >> Well, thanks Matt. Why don't we jump into the mock conversation with the chief technology officer and me, the data engineer? 

#### Conversation with the CTO
Okay, so in this video, I'm going to be playing the role of the newly hired data engineer at an e commerce company. And I'll be asking some questions of the chief technology officer, or CTO, played by Matt Housley, about the overall goals of the business and technology initiatives. Hi Matt, thanks for taking the time to chat with me today. I am very excited to be starting at this company, and one of the things I'd like to do is get a better understanding of the business goals and technology efforts that you're working on. >> Great to have you on board, Joe. I like to talk to all the new hires within technology to make sure our goals are aligned. I know that you probably won't be able to talk to me all the times you'd like to, but it's kind of my version of walking the shop floor to see what's going on in the world. 
So for the coming year and beyond, your work as a data engineer is going to play a key role in our success. And so I'd like to explain how that's going to tie into our larger initiatives. >> Sounds good. >> So far, our company has been relatively successful as an e commerce platform across a limited number of product lines. We've done great marketing, and as one of the companies that was first the market, we had the first mover advantage. The challenge, however, is that as the market evolves, we're getting a lot of smaller brands coming up and sort of grabbing market share. Another major concern that I have right now with technology within the company. 
And I'll explain how this ties in with the market share issues and such in a second, is that we actually, we're an e commerce platform now, but we inherited some old technology before even.com. And so there's concern that some of that old code running could cause outages that could be very costly to the company and also damaging to our customer brand. And so you're probably aware of some of the outages that have been in the news in various ways, and so we feel like we might have some exposure there. So the big initiatives again are we're expanding market share, new product offerings, and we're also looking to go more international. Now, the thing to understand is that involves a lot of scale. So fundamentally what we're going to try to do from the software side is refactor. So that first of all, we can get rid of that old code that's kind of back in the systems that puts us at risk and then also make everything much more scalable in the process. 
Now, let me ask you, from your perspective as a data engineer, how would that affect you? >> Well, that could affect me greatly. So I could think of a number of ways, and I know it sounds like working with legacy tools is not a lot of fun, but I'm also really looking forward to helping modernize the systems. >> Exactly, and that's exactly the attitude that I'm looking for. One of my major concerns with my CTO experience is that quite often we have this software, I'll call it the software data divide and the software data divide. You probably already know about this, but the basic idea is that people write code kind of in a vacuum, and then the data engineers just have to consume whatever comes out. And that's been a big problem for us because this legacy code generates a schema that's very, very hard to use for analytics. 
So, fundamentally, as part of the team, what I want you to assist with is consulting with the software side as we refactor our code. To make sure that the output of that code is suitable for analytics with minimal post processing. So it's not that we're going to be able to get away with removing all ETL, but we feel like if we generate the data in the right way, with the right schema. Then it's going to be much cleaner going into our analytics. >> What tools can I expect to be working with? >> That's a good question. So part of what we want to do within this initiative as we refactor is convert more from a batch-based approach to a streaming-based approach. 
And so the technologies we're looking at at the moment on AWS are kinesis, which is kind of the Amazon native version of streaming. And then Kafka potentially, as we scale up them for certain pipelines. Do you have experience with those? >> I don't, but I took a course on data engineering and I am really excited to apply some of the things they've learned in the labs about Kinesis and Kafka to real world scenarios. >> That's great. And I think at this stage, again, you're going to work as a consultant with the software side to understand the data they're producing and then help them to produce better quality data. And in the meantime, you can spin up proof of concept demonstrations of these technologies. 
Kinesis and Kafka experiment with their scalability, make sure that you build really strong ground skills in those. Because I see that being one of your key responsibilities in the future is to manage and expand our streaming capabilities. You're going to be doing that under the direction of the head of data, of course, but we expect you to really take the skills that you have now and then expand those to be really strong in this area. >> Now, I know that AI is a very hot topic these days. Does the company have any goals with respect to machine learning or AI? >> Okay, another one of our major goals within our technology initiatives is improving customer retention. And so, the core software refactor that I mentioned will help to support that because it means a more responsive website and less downtime. 
Avoiding downtime in general, so customers are pleased with the experience. In addition, we're working on a recommendation engine, and so your part in this working within the data engineering team is going to be developing data pipelines that feed that recommendation engine. And so this involves essentially looking at our data within our product taxonomy, but also looking at how customers behave on the site. So looking at the clickstream, looking at order behavior, and working with the data scientists to engineer the pipelines that support that process. >> Sounds great. >> It's really great to have you on board. >> I'm excited to work with you, and hopefully we'll get to connect in the future as these initiatives progress. 
>> Well, thanks. 

#### Conversation with Marketing
Now you've got some perspective on the high level business goals for the company, and it's time to get back to gathering requirements based on stakeholder needs. Back in the first week of this course, you learned from the data scientists that the marketing team has two primary requests. One around delivering dashboards with metrics on product sales, and another regarding a product recommendation system. These two initiatives are in line with the larger business goals of launching new products by better understanding market trends, and improving customer retention by recommending products that are more likely to align with their preferences. In this video, we're going to have a conversation with a product marketing manager to get more details on their needs and expectations for these projects. In week one, we looked at four key elements in any requirements gathering process. First, you want to learn about any system or solution that currently exists to deliver data to your stakeholders, and then you want to understand any problems with this existing system or the data. 
Next, you want to understand what actions your stakeholder plans to take with the data you serve them, and finally, you want to identify other stakeholders to speak with or conversations you need to have to gather any missing information. Make sure you pay attention to these elements as you watch the conversation unfold. Hi, Colleen. I'm Joe, the new data engineer. It's been hired at the company to build things. Hi, Joe. I'm so excited to have here. 
Well, awesome. Great to meet you. I've been talking to various people here at the company to try to better understand the data needs and I'm trying to put together a set of requirements for the data systems I'm going to be working on. Definitely, we've been thinking a lot lately about how we can use our data to make more informed decisions and drive the metrics that we care about on the marketing team, and we're really looking forward to having you work on this. Okay, great. In my conversation with the data scientists on our team, I learned that they've been working on a couple of initiatives with you. On around some dashboards for product sales Matrix and another around a product recommendation engine, is that right? 
That's correct. And I would say that both of these initiatives are in a prototype state, and we'd like to work with you on proving both of those. Okay. Sounds good. Could you first tell me more about the current state of the systems so I can get a better idea of where we're at now. For sure. With regards to the dashboards, what we have now is essentially what we want in terms of how the various metrics and trends are displayed. 
It's just that there's a lag of usually a couple of days between when the data is recorded for a new sale and when we can actually see it in the dashboards. So our data scientist has created a nice set of dashboards where we can see product sales by category and region with time plots showing daily numbers. This helps us broadly see what kinds of trends are happening from region to region. We can also double click into any particular region to see the details by individual product or on a more granular time cadence like hourly instead of daily. This is all great, but we want to see all of this stuff in real time rather than when the data is already two day. I see. So it sounds like the dashboards might already be delivering some value when it comes to allowing you to monitor long term trends, but there are things you'd like to do with that that might require more recent data, is that correct? 
That's right. In addition to monitoring long term metrics, we'd like to be able to see in real time when a particular product is trending. So we can try to capture that momentum and push out more targeted promotional campaigns, focusing on a specific region and product. Right now, we occasionally see really interesting regional spikes in demand for particular products, and we would like to know about that when it's actually happening rather than when the demand has subsided. Got it. Okay. So tell me more about these demand spikes. 
What do they look like, and how long do they typically last? Sure. So what we see in many cases, is that demand rises sharply over a span of a few hours or so, and then eventually drops off again, sometimes after a day or two, but sometimes more quickly in just a few hours. Okay, I see. If I'm understanding correctly, what you would like to be able to do is observe one of these spikes while it's happening so that you can take action based of information. That's right. If we see sales ticking up steadily for two or three hours in a row, we could have a plan in place to take immediate action and push out targeted promotions. 
Okay, great. So if, for example, the data in your dashboards was reflecting product sales from just one hour in the past, rather than two days in the past, would you be well positioned to take action? Yes. That would be great if we were looking at data that was only an hour old. Sounds good. I think I understand more or less what you need for the dashboards. I'd like to learn more about what you're interested in doing with the recommender system. 
Do you have something already working for that? Well sort of. We asked the data scientists to determine what the most popular products from the previous week of sales are. And then the platform team just serves those as recommendations to everyone at checkout. So it's more like a popular product of the week thing than any personalized recommendation. I see. You have a pretty basic recommender up and running, but it's not personalized to the customer, and sounds like what you'd like is maybe something that considers a customer's purchase history, as well as what they've got in their car to check out to make some customized recommendation, is that correct? 
Yes, that's exactly right. Just like what you see on a lot of other E commerce platforms where either while they're shopping or they're in the checkout process. You see the recommendations for products you might be interested in. Okay, great. I think it's all I need to know about the recommender for now, at least. And this conversation has been really helpful for me to understand what you need when it comes to the dashboards and the recommendation system. I'm sure I'll have more questions for you as we get started building out these solutions, and I'll let you know as soon as I got something that we can start testing. 
Awesome. Thank you. I really look forward to seeing what you build and feel free to reach out any time. Okay, so that was an example of how a conversation might go between you, the data engineer, and a product marketing manager about what they need. Join me in the next video to dive into this conversation to unpack the system requirements. 


#### Breaking Down the Conversation with Marketing
In the previous video, we had a conversation with the marketing department about their needs. In some ways, this conversation included a confirmation of the things you already learned from the data scientist. But now with more details on exactly what the end users needs are. In this video, we'll go over what we learned and take a look at starting to document the requirements you've gathered. The way I like to document requirements is using the hierarchy that we looked at in a previous video. This way, it's easy to visualize the connections from system requirements here at the bottom, all the way up to the high level business goals. Right here at the top, I can start by writing down the business goals as I understand them. 
In this case, the business goals that we learned about that the company aims to continue on their growth trajectory, throw a focus on customer retention and loyalty, as well as an expansion to new markets and new product offerings. In these efforts, they are aspiring to be data driven in their decision making. Underneath the business goals, we can now add stakeholder needs. For the moment, I'll just consider the data scientist and the product marketing manager together here as the stakeholders that we've talked to so far. At this point, we know they need two main things, some analytics dashboards and a recommender system. Let's first take a look at the analytics dashboards. Both the data scientists and the product marketing manager stated they need "real time" or "current data" presented in the dashboards. 
Through digging a little deeper and asking what action they plan to take with the data, we learned that the marketing team would like to be aware when demand spikes occur. For particular products so they can react to additional promotional campaigns. Given that the duration of these demand spikes is expected to last from several hours to one or two days, it looks like hourly dashboard updates would be sufficient and marketing confirmed, and this would serve their needs just fine. So there you have a stakeholder need, and you might capture the related functional requirement for the data system like this. The data system needs to serve transformed data that is no more than one hour old. When it comes to the recommender system, it sounds like the current solution is just to display a few popular products on the page when someone is checking out. What the marketing team would like to have instead is a system that provides product recommendations that are customized based on someone's browsing or purchase history, as well as what they have in their card at checkout. 
There you have another stakeholder need, and when it comes to a functional requirement for your data system to serve that need, you could write down something like the system first needs to provide the appropriate training data for the development of the recommender model. Then be able to ingest, transform, and serve user data to the trained recommender model, and then return the model outputs perhaps in the form of product IDs back to the sales platform. Now, I'd like to pause for a moment and highlight the fact that translating stakeholder needs into functional requirements for your system can be a little tricky, as you can see here with the analytics dashboard, what the marketing team actually needs are the dashboards themselves, and so it might be tempting to write down some details about the dashboard features or metrics to be displayed. But really building the dashboard is going to be the responsibility of the data scientists. To provide those features and metrics, the data scientist needs access to data that is no more than an hour old. When it comes to your data system, as long as you're able to serve data that the data scientist needs in a timely manner, then your system is achieving this functional requirement. For the recommender system, the marketing team needs to make customized product recommendations. 
To do that, there will be multiple people and teams involved. The data scientists will need to train and deploy a recommender model, and the platform team will need to be able to receive product recommendations from the model to display on the platform. When it comes to your data system, you'll need to provide training data to the data scientists to development model, then you need to be able to pass customer data from the platform to the model and then send the model outputs back to the platform. Here we have documented one functional requirement for each of these data systems. In practice, you could have many functional requirements for any given system as well as more detail than we've listed here for each functional requirement. To keep things simple for now, though, let's leave it at just these two functional requirements. This is how you might get started with documenting your functional requirements. 
Coming soon, we'll also get into documenting non functional requirements. But before doing that, we're going to have some more stakeholder conversations. Join me in the next video for a conversation with the software engineer who maintains a sales platform, which is the source system you'll be ingesting data from. 


#### Conversation with the Software Engineer
When we spoke with the data scientists, they mentioned that they would occasionally get stuck because the data is unavailable for a period of time, or a schema change breaks their processing scripts. As I mentioned in the second week of this course, these disruptions to the data flow or changes in the data format are very common, and the things that you should be anticipating as a data engineer when you ingest data from source systems. The best thing you can do to mitigate these issues is to open lines of communication with the source system owners and discuss how to best anticipate and deal with disruptions or changes in the data when they occur. That's exactly what we're going to do in this video. I'll be having a conversation with the software engineer responsible for the development and maintenance of the sales platform that we're hoping to set up an ingestion data pipeline to. When you talk to source system owners, you can still use the first two key elements of the requirements gathering process to help guide the conversation. You can ask a system owner about the existing system that generates the data needed by your downstream stakeholder and talk to the owner about any problems your stakeholder currently has with the solution. 
In the next course, you'll learn more about source systems and what to consider when working with source system owners. Let's jump into the conversation. Hi. I'm Jove. Hi. It's good to meet you. Nice to meet you too. 
I met with a marketing manager and a data scientist, both of whose name was Colleen, and you look a lot like those other two people. I don't know what you're talking about. I heard they're great, though. They are great. Yeah, for sure. Well, I'm looking forward to talking with you about how I can best set up some data ingestion from the sales platform. Definitely, let's talk about it. 
I think it's great that the company is taking a more serious approach to data, and looking forward to seeing how we can work better together. Great. I think to start with, I'd like to learn more about what you've been doing so far to provide data to other teams, and maybe then we can take it from there. Well, to be honest, it feels like we're just getting started exploring how to best share data with others in the company. For instance, the marketing team has been interested in analyzing product sales data, and so we've been providing data to our data scientists on a daily basis, so they can provide some metrics to the marketing team, but it sounds like they're looking to do something that's a little closer to real time analytics. I've spoken with both the data scientist and the product marketing manager to learn more about what they're hoping to accomplish, and that definitely sounds like more continuous access to the data would be good for what they're trying to do. One of the problems we face is that we aren't able to give them direct access to the production database because that would present a risk to the sales platform. 
That's what led us to what I guess you could say is the sub optimal solution of just providing them files for download on a daily basis. Right, that makes sense. Have you thought about other ways in which you could provide access to the data without presenting a risk to your production systems? Well, yes, in fact, what I think would work is if we set up a read replica of the production database, where we would push a copy of everything that's stored in the product database immediately after it's recorded. We could then set up an API so that you or anyone else for that matter, could query the data in the read replica without disturbing the production system. That sounds like it could work pretty well, actually. The other thing that the data scientists mentioned was that occasionally, they would have to wait longer than expected for the data. 
Do you think that issue will be mitigated to some degree with the new read replica database setup that you just described? Well, yes and no. I think it's fair to say that sometimes we don't get the regular data deliveries out because we are running maintenance on our own systems and fail to export the regular set of files on time. With a read replica database being updated continuously, that should not be a problem going forward. However, we do occasionally experience server or data center failures that cause our platform or database to go down for a period of time. We're working to build in redundancies and minimize our exposure to these types of system failures. But if that thing happens, then the data could be unavailable while the system is down. 
I think the best we could do is send an automatic notification to downstream data users to alert them of the outage. That makes sense. If your systems that are generating and storing the raw data go down, then the best any downstream data consumer could hope for is a notification to let them know about it. I also heard that sometimes the database schema changes, and that could cause problems with the processing scripts. Can I understand there are times when you need to change the schema for one reason or another? Could you say more about your process around those schema changes? Sure. We're constantly working to improve the sales platform, and often that means adding new features for customers to interact with. 
The user interaction data we're recording changes with each new feature we add, effectively changing the schema of the database. We're also expanding into new regions and new product lines on a pretty regular basis, and sometimes we realize we need to add or remove or combine various elements of product and purchase records just to keep up with the changing collection of things we're selling and where we're selling them. That makes sense. How far in advance would you say you usually know when you're going to make a change to the data schema? Well, it varies, but we're usually pretty careful about it since we don't want to introduce any breaking changes to our own systems. I would say we typically know about a week in advance before we're going to deploy any new changes. Well, right now, I'm gathering requirements for the data pipelines, we need to provide data for the marketing teams projects. 
I think it would be great if we could figure out a way for me to stay in the loop about any upcoming changes to database schema, such that I can anticipate those changes and modify my pipelines accordingly. I can also build in automatic checks in the data ingestion process to verify whether the data conform to a particular schema. But it would be ideal if I could get notified as soon as you'll be planning some schema change. For sure, we can definitely keep you in the loop. We certainly don't want to be pushing any changes that break things downstream, so we can make sure you get notified well in advance. We could also think about ways to make the read replica database more stable than the production database. For example, if you know exactly what data you need in terms of customer activity or purchase history for downstream use cases, we can aim to provide that in a consistent manner via the API for the read replica, or in the event that we can't avoid changing something there, we can at least give you plenty of advanced notice. 
Cool. I think with advanced notice of any changes and some degree of stability in the database we're ingesting data from, we should be in good shape. Well, I think it's all I need to know for now, but please let me know when you get the read replica database set up, and I'll start building some data pipelines for testing. We'll do. Thanks Jove. I like your glasses, too, they are pretty cool. That was an example of what a conversation might look like between you, the data engineer, and a software engineer who is responsible for the source systems you're playing to ingest data from. 
Like I said before, open communication with your source system owners can go a long way when it comes to building robust data systems. As you saw here, it's often the case that source system owners will be happy to collaborate with you. Now once they understand what you need from them, join me in the next video to continue the process of documenting requirements and looking at what some of the non functional requirements of the system might be. 


#### Documenting Nonfunctional Requirements
In the last video, we looked at an example of how a conversation might go between you, the data engineer and a software engineer who is responsible for the source system, you aim to ingest data from. Here we'll look at some of the takeaways from that conversation and continue documenting the requirements for our data system. In our conversation with the software engineer, we learned a few key things. First, they suggested that a way to solve the data access issue would be to set up a read replica database and an API to provide continuous access to the data. This is, actually, a pretty common approach to providing access to data while protecting production systems. They also briefly discussed some ways in which they could ensure a degree of stability in the read replica database, in order to minimize breaking changes to downstream systems. They also said that they would be able to provide notifications when any system outages occur or when they anticipate making changes to the database schema. 
Now, let's take another look at documenting the requirements for the data systems we're looking to build. As a reminder, we're approaching the documentation process using this hierarchical format, where I've written down the business goals up top, then stakeholder needs below that, and then system requirements under that. We have two functional requirements written down here. One for the analytic dashboards, and one for the recommender system. Now we can start to write down some nonfunctional requirements for these systems as well. Nonfunctional requirements can be a little trickier than functional requirements in the sense that they won't, typically, be things that your stakeholders explicitly ask for. Rather these or the characteristics or attributes that your system must have in order to get the job done well. 
For the analytics dashboards, we've already wrote down the functional requirement that the data must be ingested, transformed, and served, such that it's available in the database being used for dashboards no more than one hour after it's recorded in the source system. You could write down a nonfunctional requirement around scalability and latency. Given that the volume of data being ingested can vary based on how many users are on the platform, a nonfunctional requirement could be that the system will be able to scale up, to ingest, transform, and serve the data volume expected with a maximum level of user activity on the platform while staying within the latency requirements. In other words, the data processing can't slow down as a result of higher data volume. Another nonfunctional requirement for the dashboards could be regarding the reliability and maintainability of the system. In this case, as mentioned in the conversation with the software engineer, you could set up checks on the data to make sure it conforms to the expected format, and source system engineers can let you know when to expect changes in the data schema. In terms of reliability, your nonfunctional requirement here could be that the system will perform data quality checks to ensure ingested data is conformant. 
Then in terms of maintainability, you could write that the ingestion and transformation stages must be easily adaptable to accommodate any changes in the data schema. Now, let's think about the recommender system in terms of nonfunctional requirements. Here again, you have a latency requirement. As you can imagine, if you're aiming to serve product recommendations while someone is browsing or purchasing items. It needs to be relatively instantaneous. So far, we've been saying real time or a little closer to real time, but what do we mean by that? In practice, defining real time depends on the nuances of the system you're building. 
But here, let's just assume that the data system must have a latency of less than one second from ingestion of user data to serving of recommendation data. In other words, the time that passes between your system receiving customer data from the platform and serving product recommendation data back to the platform, should be less than one second. There may be circumstances where the latency requirement could be smaller or larger. But one second is at least on the order of what would be expected for a typical recommendation system. You would also have a nonfunctional requirement around the scalability for the recommender system. Similar to the dashboards application. Here, you would need your system to be able to scale up to the maximum number of concurrent users on the platform, and you could have a nonfunctional requirement in terms of reliability as well. 
If for example, the system failed to generate recommendations or experience some other error, you might want to execute a predictable behavior that doesn't cause a bad user experience. So for example, we could maybe capture the reliability requirement like this. The system must always return a set of recommendations within one second, such that if the recommender pipeline fails for any reason, it should default back to just serving a selection of the most popular products. So now you have a handful of nonfunctional requirements for these two systems. As I said before, in practice, you could have many functional and nonfunctional requirements for any data system. Here we've just written down some of the relatively obvious ones for these two systems. The key thing to take away from these requirements, gathering examples is that it's important to understand the systems you build not just in terms of how they will function, but how that functionality will serve stakeholders and the ultimate goals of the business. 
Join me in the next video. We're summarize some of the major themes from this lesson, before we go on to choosing tools and technologies based on your system requirements. 


#### Requirements Gathering Summary
This lesson has been all about requirements gathering and how to make sure you have a detailed understanding of how the systems you build will add value for your stakeholders. Now I can imagine this might not have been exactly what you're expecting from a first course on data engineering. You might have expected a course that focuses on the tools and the technology, data engineers used to build really large fancy systems. Well, I appreciate you sticking with me up to this point, and I can assure you that your time has been very well spent. In my own experience, I've seen data engineering done wrong and more times, I've seen it done right. I just want to make sure that you're set up for success with the data systems that you build. And don't worry, we're going to go very deep into the tools and technologies throughout the rest of these courses. 
The main takeaways from this lesson include the following. Before you set out to build or modify any data systems, you need to identify the stakeholders you will serve and understand their needs in the context of the broader goals of the business. The way to do this is by talking to lots of people in your organization, and maybe everyone from company leadership right down to those you work alongside, like data scientists, and software engineers. It's important to ask open ended questions in these conversations to gain an understanding of current systems, potential problems with the systems. And what action the stakeholders plan to take with the data. It's also important to document all of your findings. Having proper documentation of the requirements you've gathered will allow you to confirm with stakeholders, whether the system you're planning to build will serve their needs, as well as the needs of the business. 
In short, once you understand what your stakeholders need, and you've written down a set of functional and non functional requirements for your system, you'll be well on your way to delivering value for your organization. So far, we've been focused on figuring out how to build data systems that optimize for serving stakeholder needs. Something we haven't talked about yet this week is about how to evaluate trade offs in requirements gathering. For example, your stakeholders might want you to build a data system as quickly as possible, or when it comes to costs, you might be working within a limited budget for the systems you build. In reality, of course, timeline and budget constraints will be part of any project you work on. Your conversations with stakeholders will need to include a discussion on what's the most important. The features of the system or the timeline to deployment or the cost. 
There's a concept that emerged from project management, known as the iron triangle, where you have three aspects of any project that are fundamentally in tension with one another. These are the timeline of the project, the scope of the work, and the cost. What I mean by intention with one another is that you can think of each of these things pulling in a different direction. For example, if you increase the scope of work for a project, then either the timeline or the cost or both must increase or say that you want to complete the project in a shorter timeline, then that could require increasing the cost or reducing the scope or both. There's even an old saying that developed around this concept, which is good, fast or cheap. You only get to pick two. In other words, if you want something good and fast, then it won't be cheap, or if you want something fast and cheap, then it won't be good. And so on. 
In reality, every company wants projects done well, and they typically want them done as quickly as is reasonably possible and within certain budget constraints. What do you do? Well, as many authors have pointed out, since the emergence of the iron triangle, the idea that you can't simultaneously optimize for all three of these things is really a fallacy. I won't get into the details right now, but I would encourage you to do a search for the fallacy of the iron triangle or something similar, and see what you find. This isn't to say that you can avoid the need for making trade offs between cost, scope and timelines in the systems you build. In short, the way to break the iron triangle is through the application of principles and processes like those we've been talking about throughout this course, things like building loosely coupled systems, optimizing for two way door decisions and deeply understanding the needs of stakeholders. By applying these principles and processes, you'll be better able to build and maintain high quality data systems on a predictable timeline and budget. 
We'll get into the consideration of trade offs more in the next lesson, where you'll look at some of the actual costs and capabilities of different tools and technologies that you could use to build your systems. I'll see you there. 



#### Requirements Gathering Exercise
>> In this lesson, your goal is to take a project from requirements gathering to implementation drawing on the material we covered in this course. So this will be a sort of micro simulation of what a project in the real world might look like. I say micro because in practice, getting from requirements gathering to a final implementation of your system could take weeks or even months or years or, or longer. And so here we'll be getting from one end to the other in maybe just a couple hours. But I still think this is a very useful exercise to practice what we've been talking about so far in this course. So here's the plan. In the next video, you'll find the conversation between the data engineer and the data scientist we've already been in touch with in week one of this course. 
The focus of this conversation will be the recommender system we looked at in some detail in the previous lesson. You'll also have access to the transcript of this conversation in the reading item that follows this video, so you can easily review any details of this conversation. Your goal in reviewing this conversation will be to extract functional and non functional requirements in addition to those we wrote down in the previous lesson. You'll do this by answering the questions in the quiz that comes up after the transcript reading item. Once you've completed the quiz to identify requirements in the next set of videos, you'll hear from Morgan at AWS about the variety of AWS tools and services that you can choose from to implement your solution. And once you've had a chance to review those, you'll be ready to take the next quiz where you'll identify which tools and services meet the requirements for your data system. Finally, you'll head over to the lab environment where the basic infrastructure for recommended system has been set up for you, and you'll take steps to customize the system based on the tool choices you made in the quiz. 
Now, as I mentioned already, this is a micro simulation and is a much more brief and limited experience than what you can expect on the job as a data engineer. But if you take a moment to imagine yourself going through this experience in the real world and consider what it might be like to build a system like this without all the guardrails, then I think it can be a really useful exercise. And with that, let's jump right into it. Go ahead and work through the reading items and quizzes, and then I'll see you in the video after that for a walkthrough of the lab exercise. 


#### Follow-up Conversation with the Data Scientist
Data Engineer: The last time we spoke we talked about two projects you’re working on for the marketing team: an analytics dashboard and a recommender system. Since then I’ve talked to our product marketing manager to learn more about their needs for each of these systems and I met with one of the software engineers from the platform team to learn more about how we can best access the data they are generating on the sales platform. What I’d like to do now is learn more from you about how you are imagining the recommender system will work so that we can start to prototype the data pipeline to serve this system. Can you tell me more about your work so far on this project?

Data Scientist: Sure thing! So, I’ve been experimenting over the last month or so with some models and ideas for the recommender system. And for now I’ve decided to implement a content-based recommender system. The way that works is that I take a set of features containing information about each product and then also a set of features containing information about each user. I then compute a vector embedding for each user and each product and look for similarities between these embeddings in order to make product recommendations to users. So the model takes user and product features as inputs and generates a list of product ids as output.

Data Engineer: Ok cool. So, you’ve already got something up and running?

Data Scientist: Well, sort of. I’ve been experimenting with some models on my local machine. I’ve trained a content-based recommender with some of the user and product data that I had initially downloaded for the dashboards. The model seems to be working ok, and what I’d like to try next is retraining the model based on a fresh batch of data to see how stable it is and try to get a sense of how often we might want to retrain and update the model in deployment.

Data Engineer: Ok great, well if you can tell me more about the format and structure you need for the training data, I can start working on a plan for a data pipeline to deliver that data in batches. 

Data Scientist: Ok, the training data I’m using is in tabular format. Each row contains data for a single product purchase, where the columns include  a collection of user features, which are: “customer number”, “credit limit", "city", “postal code”, and "country". And then there’s also a set of product features, including “product code”, "quantity in stock", "buy price”, "msrp", "product line", and "product scale". In addition to these user and product features, there is a rating value from 1 to 5 that represents the user’s rating of that product. 

Data Engineer: Got it! Now, if you had to guess, how frequently do you think you might want to retrain and update the deployed model? 

Data Scientist: Well, the short answer is I’m not sure yet. I will plan to monitor the model’s output. In fact, it would be great if, in addition to serving recommendations on the platform, all the model output could be automatically saved for later analysis. And then I would plan to retrain the model if I notice any drift in performance or change in the input data. So, I guess I might want to retrain the model as frequently as once a week, but it might be less than that, like monthly or quarterly. So it would be great if it doesn’t require too much operational overhead for you to deliver a new batch of training data and possibly in a modified format if we have new product or user features we want to incorporate. 

Data Engineer: OK that all makes sense. Now, can you tell me more about how you’re expecting the recommender model will be used in production?

Data Scientist: Yes, so what I’m thinking is that I would like to make recommendations based on both the user information as well as any information about products they have been browsing or that they have in their shopping cart. The way I imagine this could work is that the model will be set up to take user information as well as the information about any number of products they have been looking at as input. I can then find a set of products to recommend based on the user information and additional products to recommend based on the products they have been browsing or have in their cart. 

Data Engineer: How fast will the system need to generate recommendations?

Data Scientist: Sure, so we would like to be able to present recommended products to the user more or less instantaneously as they are browsing through different products or during the checkout process. It usually takes a new page a second or two to render so if the recommender could work that fast it would be great.

Data Engineer: Ok, how long does it take to actually run the model and generate recommendations? 

Data Scientist: Oh the model is really fast, on my local machine it only takes a few milliseconds to generate recommendations given a set of input features. The way I have set it up is that I already have all of the vector embeddings for the products in the catalog stored such that when I run the model I can quickly run the similarity calculation. 

Data Engineer: Ok great, as I understand it the platform records data from users currently online in an event log so it should be possible to just stream events from that log with very low latency. So, assuming that the sales platform is able to both provide user and product data and then receive and render product recommendations with sub second latency, I should have maybe one second or so to complete the round trip with the data, namely, ingest, transform and serve user and product features to the model, then return the recommended product ids back to the platform. 

What about scalability? Do you have a sense of how many concurrent users you would expect to be serving product recommendations to?

Data Scientist: Well, the company currently has around 100k customers in the database, meaning that many individual customers have bought at least one item, and many of them are repeat shoppers. We’re expecting that number to grow as the company expands to new regions and product lines. So, right now, it varies a lot. Sometimes there are only a few people shopping on the platform but we can see activity spikes of up to 10k users on the platform at the same time and we could see more in the future.

Data Engineer: Alright, so it sounds like there could be a wide range in terms of concurrent users. OK, well I think I’ve got everything I need in terms of information at this point. Thanks! 

Data Scientist: Ok great, let me know if any more questions come up!  


#### Conversation Take-Aways
Here are the key takeaways from this conversation. You are tasked with building two data pipelines:

A batch data pipeline that serves training data for the data scientists to train the recommender system;

A streaming data pipeline that provides the top product recommendations to users based on their online activity. The streaming data pipeline makes use of the trained recommender system to provide the recommendations.


The recommendation system will recommend products to customers based on their user information and based on the products the customer has been browsing or that are in their cart.


Notes: 

The lab focuses on providing an end-to-end data pipeline example. Your main tasks will be to interact with the components of the batch and streaming pipelines.  

The lab does not focus on fine tuning the performance of the recommender model. Moreover the data that is used is the same one used in week 2, and may not contain good predictive user and product features. The goal of the lab is not to assess the performance of the recommender problem but to incorporate its computations inside the streaming pipeline.

The recommendation model will compute two vectors -- the product embedding vector and the user embedding vector. It will then generate product recommendations based on these vectors. The product embedding is a vector that holds information about product characteristics, and the user embedding is a vector that holds information about user preferences for each of the product characteristics represented in the product embedding vector. To learn more about vector embeddings and how the recommendation system works, feel free to check out the next optional reading item.


#### Details of the Recommender System
After you implement the batch pipeline in the lab to serve training data to the data scientist, you will be provided with a pre-trained recommender system. The development and training of the recommender system are outside the scope of this data engineering specialization. You will then use the recommender system to find products to recommend as part of the streaming pipeline. 

This section briefly explains how the content-based recommender system works and how it is trained. This optional section can be read at your leisure if you’re interested in learning more about the recommender system.


Recommendation model


Given the features of a specific user, the model predicts what ratings the user will assign to a specific product. By computing the predicted ratings for all company products, the model can provide a list of recommendations for the user (“Based on your personal profile, you may like …”). The model is trained to compute two vectors called embeddings: one describing the characteristics of a product (product embedding) and another describing the user’s preferences (user embedding). Based on these two embeddings, the model can predict if a user will like a product.

Product embeddings can also provide another set of recommendations for the user. Based on the user’s interactions with products on the company’s platform (products purchased, placed in a cart, or searched for), similar products can be found and recommended to the user (“Based on your browsing history, you may like…”). For any product the user has interacted with in a given session, the recommender model computes its embedding vector. This embedding vector is then compared to the embeddings of all company products to find similar items (based on their distances). For finding similar items, you will implement a vector database in the lab, which makes retrieving similar products faster than using the brute-force approach. In the lab, you will be given the embeddings computed by the model for all company products. You will store all the embeddings in a vector database. The vector database organizes the embeddings so that similar products are placed close to each other, accelerating the search for similar items.

The recommender model will provide a combination of recommendations based on similar items and user features.

#### AWS Services for Batch Pipelines
You've talked with stakeholders and gathered your system requirements. Now it's time to translate those requirements into tool and technology choices for your system. There are many ways to combine different Cloud tools and services to get our two form solutions. In the process of converting system requirements to actual data systems, you need to be familiar with what different services it can be used for. I think Joe actually illustrates this point well in his book with the example of choosing a vehicle for the purpose of traveling from one place to another. Imagine for a moment, if instead of a data system, you were tasked with solving the problem of transporting all of the members of your data team from one place to another. Which of these two options would be the best choice? 
So first, you have a jumbo jet which provides a range of 10,000 nautical miles, top speed of 1,100 kilometers per hour, capacity for 250 plus passengers, and a cost of $225,000,000. And next, you have a Tesla model S with a range of 360 kilometers, top speed of 320 kilometers per hour. Room for four passengers and a cost of around $70,000. Which of these vehicles would you choose to transport your team? Well, that's pretty clearly a ridiculous question. Of course, you would need to have a clear sense of your requirements. In this case, like, where do you need to go? 
If it's New York to Paris, maybe you do need the plane. Also, how big is your team? If instead, you're only going from New York to Boston with four people, maybe the Tesla would work, but what if you're in a hurry? Do you need a helicopter or if you're on a tight budget maybe four bicycles. Well, I think you get the point. In addition to requirements for your system, you need to be familiar with the technologies you have to choose from, what they can be used for, and how you can combine them to do what you need to do. So for the data system you'll be working with this week, it looks like there's going to be a batch component and a streaming component. 
In this video, I'll walk you through the basics for some of the AWS services that can support batch workloads on AWS and then in the next video, we'll look at AWS services that can support streaming workloads. As you saw last week, a common approach to batch data processing is what's known as an extract transform load or ETL pipeline. In this week's exercise, you'll need to ingest data from a source system, apply some transformations to get it into the format that your data scientists needs, then load it into storage and provide them access to it. So in this case, the ETL paradigm seems like a fit. The data you'll be ingesting is tabular data. So let's assume that the source system you'll be working with is Amazon Relational Database Service or ARDS for short. As Joe mentioned earlier in the course, databases are a very common source system you'll be working with as a data engineer. 
And in general, you aren't in control of the source system. So we won't spend time here on comparing different source systems for tabular data. Instead, we'll assume the choices you need to make are in the downstream components. For the next steps of ingestion and transformation, there are a number of different approaches you could take. For example, you could simply spin up an EC2 instance and write a bunch of scripts to connect to the database. Ingest and transform the data, then send it onto storage somewhere. This could certainly work, but keep in mind what Joe said about avoiding undifferentiated heavy lifting. 
With an EC two based approach, you'd be responsible for installing software, managing security, and all the complexity that comes with deploying a server on the Cloud. So in this case, creating your own custom solution for a relatively straightforward ETL pipeline might not be the best use of your time. In the comparison of serverless, traditional server, and container options for services, Joe's recommendation was to look at serverless tools first, and then, if you can't get what you need, consider container and server options. I would agree with that advice. And so let's take a look at the serverless options you have in this case. AWS Lambda was one of the first and remains one of the most popular serverless tools on AWS. With Lambda functions, you can have code run in response to a trigger or event and for your ETL pipeline this week, you could certainly consider using a Lambda function to extract data from the source system, apply transformations, and send it onto storage. 
However, Lambda functions come with limitations. Things like a 15 minute timeout for each function call and limitations around memory and CPU allocation for each function among others, which might mean you need to break up your task and do smaller chunks to stay within these limitations. Beyond that, writing Lambda functions requires you to write custom code for your use case, which in this case, again, might not be the best use of your time. In terms of serverless tools that are specifically for batch processing of data, there are two services I'd like to talk about. These are Amazon Glue ETL and Amazon EMR serverless. In some sense, you could say that there is a fair amount of overlap between the things these two services can be used for. And really, it may come down to the nuances of your specific project that determine which tool can best serve your needs. 
But at a high level, the difference between the two comes down to the trade offs between control versus convenience. Simply put, EMR serverless gives you more control over what you can do, while Glue ETL provides a more convenient experience. But let's dig into the details of it so you can see what I mean by that. EMR was designed as a big data tool that supports a wide range of frameworks like Apache Spark and Apache HIVE. So if you're part of a team doing petabyte scale analytics, maybe using Hadoop or requiring the flexibility to incorporate your own custom components, then EMR or EMR serverless might be the right choice. Glue ETL can also handle big data workloads, but the real advantages are the additional features you have access to. For example, when you connect to a source system, Glue uses something called crawlers, which automatically discover and classify data creating metadata in the process, including things like table definitions and schemas. 
This metadata is then used to populate the Glue data catalog, which is a central repository containing information about all of your data assets. With this information about your data now in your data catalog, you can use the Glue Visual ETL tool to design your pipeline using a graphical interface in the AWS management console that will automatically generate the Spark code you need to run in your pipeline. When you run your pipeline, the servers will maintain the Glue data catalog to track the transformations you apply, and that catalog can be used downstream to more easily integrate with other AWS services. There are other options you could consider for building the ingestion and transformation portion of your ETL pipeline on AWS, but those are the main options I wanted to share with you. In the reading item after this video, you'll find more details about the services we've touched on here, as well as others. When it comes to the storage and serving aspects of the batch pipeline you're working with this week, your choice would depend mainly on what downstream use case you're serving. For example, if you're ingesting normalized tabular data, then applying transformations to model it in a star schema for analytics, then one option you could choose would be to store and serve it in another RDS instance. 
Or if you wanted to run complex analytical queries on massive datasets and take advantage of other features that data warehouses offer, you could choose to store and serve the data in Amazon Redshift. Which is a powerful data warehouse solution, albeit at a significantly higher cost than RDS. This week, you'll be serving a machine learning use case, where the data will be used for training a recommender model. When your downstream data consumer is another technical data professional who's planning to manipulate the data and incorporate it into their own systems, oftentimes, the best and cheapest storage and serving option is object storage on Amazon S3. Out in the real world for data systems built on AWS, it's relatively common for S3 to serve as a sort of staging area like this because S3 is flexible, scalable, and relatively cost effective. It allows you to store virtually any kind of data that easily integrates with other AWS services. In S3, you store data in buckets, which are containers for objects. 
When building data pipelines, you may have multiple S3 buckets that are used throughout different stages of your pipeline depending on the task at hand. Well, I could say a lot more about all the services we've looked at here. But at this point, I think you've got what you need in terms of information for this week's exercise. In the next video, we'll take a look at streaming tools. I'll see you there. 


#### AWS Services for Streaming Pipelines
In the last video, you learned about some of the AWS services you have to choose from when it comes to building batch pipelines. In the lab this week, in addition to a batch component, your system will also have a streaming component. In this video, we'll look at streaming services on AWS. As you've learned from Joe, streaming data could be coming to you from a number of different sources. These could be IoT devices or click-stream data from a website or mobile app. Your streaming source could even be a database in the sense that you could be continuously streaming any changes or updates to the data in the database through a process called change data capture or CDC. And just like for batch data processing one way you could potentially ingest streaming data would be to spin up an EC2 instance and write some custom scripts to perform CDC or connect to whatever other streaming source. 
Then transform the data and set it downstream to wherever it needs to go. Just like with batch processing, this EC2-based approach means you'd be responsible for installing software, managing security, and all the complexity that comes with deploying a server on the Cloud. Following the same ideas we discussed with batch processing, you could also consider Lambda functions as a serverless option to create your own streaming system. But, again, this comes with writing custom code for whatever you need to do and the potential limitations of Lambda for your particular use case. On top of that, interacting with streaming sources can be more complex than running batch workloads. I can say that you probably shouldn't build your own custom streaming solutions with EC2 or Lambda. Unless you are 100% certain that your use case cannot be solved with existing open source or managed tools. 
Let's take a look at some of the AWS services that can support streaming workloads. First up is Amazon Kinesis Data Streams. This is a popular AWS service that enables real time data ingestion. The way it works is that you have data producers that are sending data to Kinesis Data Streams and this could be log data from web servers, data from IoT devices, or click stream data as a few examples. Kinesis itself is not picky about the type of data that you send. It's data agnostic. So you can send JSON, XML, structured or unstructured data to a data stream. 
The data gets posted to the stream by the producers, and then data is stored in Kinesis for a configurable amount of time. The default and minimum retention time is 24 hours, but it can also be extended. The stored data can then be pulled by consumers of the data stream. Multiple consumers can pull the same data and process it in different ways. It's common for Kinesis consumers to either take the data and place it somewhere else, such as a storage service or data warehouse, or consumers might be performing some real-time analysis of the data that is passing through the stream. These consumers could be software applications that are pulling data from the stream and processing it, and that application could be running on compute services like EC2 or Lambda. Apart from Kinesis, another option you have is Amazon Managed Streaming for Apache Kafka, otherwise known as MSK. 
MSK is a service that provides much of the same functionality as Kinesis Data Streams. Apache Kafka itself is an open source streaming platform, that is a popular choice for many different streaming use cases. MSK is a fully managed service that makes it easier to build and run applications that use Apache Kafka to process streaming data. MSK runs open-source versions of Kafka, which is useful because any existing applications, tooling, or plugins from the Apache Kafka community are supported. The way MSK works is that you first create an Apache Kafka cluster, and the MSK service manages the heavy lifting of provisioning and operating the nodes that run Kafka for you. This allows you to avoid that undifferentiated heavy lifting and focus more time on your customized application logic. Then you, as the user, interact with what's called the Kafka data plane that MSK manages for you to create topics, produce, and consume data. 
Data producers and data consumers then connect to the cluster to send and receive messages. Both Kinesis Data Streams and MSK can scale up to handle petabyte-level data volumes coming from multiple data sources with millisecond latency, which enables real-time processing and analytics. When it comes to choosing one versus the other, just like when it comes to choosing between similar tools for other aspects of your data systems, it will depend on your use case. But at a high level, you could think of this again as a trade-off between control and convenience. If you're brand new to data streaming architectures, Kinesis is often recommended for its relative user-friendliness and reduced operational overhead. On the other hand, if you're already running a Kafka cluster or maybe you have existing Kafka technical experience in-house, or if you're looking for a higher degree of flexibility and control, then MSK might be the better choice. With regard to the use case of reading the data from a stream and storing it somewhere else, the next service I want to tell you about is Amazon Data Firehose. 
For a little context on why the Data Firehose service exists in the first place, it turns out that Kinesis came first as a service for streaming data systems. AWS realized that a lot of users of Kinesis Data Streams, we're simply taking the stream data and storing it in S3 or somewhere else. But to work with Kinesis in this way, you need to write custom code that creates the connection with the data stream, reads the stream, chunks the data, and then stores it. To make that whole process easier, AWS created the Amazon Data Firehose service, which can integrate with Kinesis Data Streams and is designed to allow you to get data from a stream and store it in a destination like S3 or Redshift, or send it to HTTP endpoints or third party service providers such as Data Dog or Splunk. The main takeaway with Data Firehose is that it helps you more easily read data from a stream and move it to storage without needing to write custom code or create any difficult integrations yourself. Beyond Kinesis Data Streams, Data Firehose also integrates with more than 20 other ADS sources to ingest streaming data, including MSK and others. As with all the other ADS Cloud topics we've discussed in this course so far, there is a lot more to know when it comes to streaming resources and services. 
But I think you have what you need now, so it's time to get hands on with these services yourself. In the upcoming quiz, you will determine which services to use for the batch and streaming components of the data pipeline for the product recommendation system you've been looking at throughout this course. Then after that, Joe will walk you through the lab exercise for this week, where you'll implement this batch and streaming pipeline. Have fun, and I'll see you in the next course. 


#### AWS Services to Meet Your Requirements
In the previous practice quiz, you extracted the functional and nonfunctional requirements of the batch and streaming pipelines. In the quiz that's coming up, you will be given different architecture options for both pipelines. Your task is to choose the combination of AWS services that best meets the functional and nonfunctional requirements. Please make sure to check the feedback of the previous quiz because the discussion of the architectural choices relies on the requirements laid out in the feedback of the previous quiz. 


[Optional] 

In the previous two videos, Morgan walked you through some of the common AWS services that you can use when implementing batch and streaming pipelines. Feel free to also check out the following links to learn more about some of these AWS services.

AWS Service Options for Batch Pipelines

AWS Glue ETL: 
features
 (check the intro and the two sections Discover (the first and last items) and Prepare (the first item)).

Amazon EMR: you can quickly check the overview 
here
 and the first part in
 features
.  

Amazon S3: you can quickly check the overview 
here
.

AWS Redshift: you can check the first three sections 
here
.  


AWS Service Options for Streaming Pipelines

In the lab this week, you will only focus on implementing the part of the streaming pipeline that stores the recommended products. 


Feel free to check out the following links to learn more about AWS services that you might choose to implement in your streaming pipeline:

Amazon Kinesis Data Streams: check the quick overview 
here
 (make sure to check the diagram on the same page) and the features 
here
. 

Amazon Data Firehose: check the quick overview 
here
 (make sure to check the diagram on the same page) and the features 
here
.

Amazon MSK (Management Streaming for Apache Kafka): check the quick overview 
here
 and the 
features
 (the first paragraph is enough)

Additional readings

A guide to choosing the right AWS streaming service: Kinesis VS MSK

Streaming data on AWS: Amazon Kinesis Data Streams or AWS MSK?


#### Lab Walkthrough - Implementing the Batch Pipeline

Now that you've watched the conversation with the data scientist and explored your architecture choices in the quizzes, it's time to implement the batch and streaming architectures for the recommender system. You'll first implement the batch pipeline to serve training data to the data scientists for training a recommender system. Then, you'll set up a vector database to store the output embeddings of the recommender system. And finally, you'll implement the streaming pipeline that uses the trained recommender system and the vector database to output product recommendations for a user given their online browsing activities. The lab contains detailed instructions showing you how to create the resources using Terraform and how to interact with them in the command line terminal. Don't worry if you don't completely understand the details. The main goal of this lab is to help you get familiar with the batch and streaming components of a data pipeline on AWS. 
Here's the architectural diagram of the batch pipeline. The first section of the lab focuses on this part of the architecture, which transforms the data and makes it ready to be used for training. Similar to the lab you saw in week 2, here you're provided with an RDS MySQL database that contains the classic models dataset and an additional table that consists of the ratings assigned by the users to the products they bought. These ratings will represent the labels for the training dataset and the recommender system will be trained using a supervised machine learning model behind the scenes. To prepare the data for the training phase, you'll use AWS Glue ETL to ingest the data from the MySQL database and then transform the data into this form. Finally, you'll store the transformed data into this S3 bucket labeled as data lake. You will create the Glue ETL and the S3 bucket using Terraform. 
So here I already started the lab and opened the lab instructions. Let's start by exploring the ratings table provided in the source database. To connect to the source database, we need to know its endpoint. I'll copy this command from the lab instructions and paste it in the terminal here, replacing mysqldbname with dec1w4rds and then running the command. This will return the database endpoint. Next, I'll connect to the database using the mysql command. The host is the endpoint I just got from the previous command. 
The database username is admin and the password is adminpwrd. Now that the connection is established, I'll choose the classic models database to view the tables inside. You can see that there's an additional ratings table. Let's check the contents inside this table by running this query, which returns the first 20 rows of the tables. Each row consists of the customer number, the product code, and the product rating. Once you're done exploring the database, you can type exit to quit the connection to the database. You'll learn more about connecting and working with databases in course 2. 
In the next part of the lab, I'll use Terraform to create the resources for the batch pipeline. First, I'll run the script labeled setup.sh, which includes the shell commands for defining some environmental variables for Terraform. I'll change my working directory to the Terraform folder. Before running Terraform, let's quickly check out the structure of this folder. On the left under Terraform, I'll click on Modules. In this lab, the resources needed for each part of the lab are grouped into folders or modules. So, if I click on ETL, you can see the Terraform files for Glue. 
When you organize your Terraform files into modules, you'll need to declare these modules in the main Terraform file defined outside the modules so you can pass in values to any input variables and use any output values from the module. Within the main file, there's a section that declares the ETL module, which contains a link to the module and some values that are passed to its input variables. I'll uncomment the section by removing the number sign in each row. Make sure to save the updates to the file. Next, I'll edit the outputs Terraform file and uncomment the section that declares the output variables of the ETL module, namely the ID of the S3 bucket, data lake. You'll get a deeper understanding of these various Terraform files in Course 2. After saving the updates, I'll go back to the detailed instructions. 
In the terminal, I'll type terraform init, then terraform plan, and finally terraform apply. Terraform will always prompt for confirmation before creating the resources. Once the resources have been created, you can see the output values. Now, let's run the glue job that will transform the data into the desired form. If you're curious to see what the script for this glue job looks like, you can open the Terraform folder, click on Assets, and then glue underscore job to check out the Python script that contains the transformation logic. I'll go back to the instructions and copy this command and paste it in the terminal to start the glue job. You can check the status of the glue job by running this command. 
I'll replace the job run ID with the ID returned by the previous command. So, the current status is still running, but if you wait a few minutes, the status will change to Succeeded. We'll get further into the details of AWS Glue in Course 4. Now, let's check that the S3 bucket, data lake, contains all the resources. Let's check that the S3 bucket, data lake, contains the training data. In the console, I'll type S3, click on that service, and then choose the bucket that has data lake in its name. Here you can see a folder labeled as Ratings underscore ML underscore Training, which contains additional folders, each associated with a customer number. 
In object storage, this way of organizing the data is called partitioning, and it helps you quickly locate information that's related to any customer. So, up to this point, we've transformed the ratings data into a format that can be used to train the recommender system. In the next video walkthrough, we'll take a look at the next part of the lab, where we'll set up a vector database to store the outputs of the recommender model. 


#### Lab walkthrough - Setting up the Vector Database
In this video walkthrough, we'll go through section 2 of the lab, where you'll set up a vector database that will store the outputs from the recommender model. Let's go back to the architectural diagram of the batch pipeline. Now we have the transformed data stored in the S3 bucket, Data Lake, which is shared with the data scientists so that they can use the data to train the recommender system. In this lab, you won't train the recommender system yourself, as that would be the job of the data scientist, but instead you'll be provided with a model that is already trained. The output of the model is shared in the S3 bucket labeled as ML Artifacts. So let's start section 2 of this lab by checking out the contents of this S3 bucket. In the console, I'll go back to the list of the available buckets, and then choose the bucket that has ML Artifacts in its name. 
You can see that the bucket has three folders, Embeddings, Models, and Scalars. The Embeddings folder contains the embeddings of the users and items, meaning products, that were generated by the model. The Model folder contains the trained model that will be used for inference. And the Scalars folder contains the objects that were used in the pre-processing part of the training. For now, let's just focus on the embeddings. If you click on that folder, you will see two CSV files, one for the items, or products, and another one for the users. Those embeddings will be used by the model to find what products to recommend for a given user. 
So for example, the item embeddings will be used to retrieve similar products to the ones that a user has placed in the shopping cart. The recommender model will first compute the embedding vectors of the products in the shopping cart. Then, it will perform a similarity search over the item embeddings to find similar products. The data scientist asks you to upload the item embeddings CSV and user embeddings CSV files into a vector database to make retrieving similar products more efficient. So let's go through the steps for creating the vector database and uploading the embeddings. Following the instructions in section 2, I'll open up the main.tf file and uncomment the section that declares the VectorDB module. Then, in the outputs.tf file, I'll uncomment the output variables of this VectorDB module. 
When you create this database, these output files, like the database username, password, host, or endpoint, will be returned to you so that you can use them to establish a connection to the database. Make sure to save your updates to the main and outputs files. Now we can create the resources associated with the vector database. In the terminal, I'll run terraform init, then terraform plan, and finally terraform apply. After you confirm that you want to create these resources, it might take around 7 minutes to create the PostgreSQL database. By the way, Terraform will not overwrite any resources that were earlier created. It will keep them and focus on creating the new resources. 
Once the database is created, you will be able to find the hostname or endpoint. Since I need this information to connect to the vector database, I'll copy this link and paste it in a separate note for later. I also need the password and the username, but they are market sensitive, so I'll have to run these commands from the lab instructions and copy the password and the username to a separate note. Now let's add the embeddings to the vector database. I'll click on the SQL folder on the left, and then open up the SQL file. Here you can find some SQL instructions that transfer the item and user embeddings from the MLArtifact S3 bucket to the vector database. In this file, you need to specify the name of the bucket. 
Let's go to the console, go to the list of buckets, and then copy the entire name of the MLArtifacts bucket. Then in the SQL file, let's paste the name of the bucket in these two SELECT statements. Make sure to save the file. Now you'll need to connect to the vector database and then execute these SQL statements. To connect to the database instance, I'll copy this command from section 2 of the lab instructions, then replace the host with the hostname of the database that I kept aside from earlier. You'll be prompted to enter the password, so again, just use the one that you set aside. It will look like you did not type anything, but when you hit enter, the password will be read. 
Now to work in the particular database named Postgres, I'll run this command in the terminal. Again you'll be prompted to enter the password, so just use the same one from the previous instructions. Now to run the SQL statements from the embedding SQL script, I'll run this command in the terminal. Then I'll run this command to see all the available tables. I'll scroll down using the arrow key to find the tables containing the embeddings, then type q to exit. When you're done, you can quit the connection by typing slash q. Now that we've set up the vector database and uploaded the model embeddings to it, in the next video I'll walk you through the streaming pipeline portion of this lab that will be used to pass user and product data to the recommender system, then pass the product recommendations into an S3 bucket. 
I'll see you there! 

#### Lab walkthrough - Implementing the Streaming Pipeline
As a reminder, by the end of the last video, we had trained a recommender system and a vector database ready to be used in the streaming pipeline for generating and storing product recommendations. Here's the architectural diagram of the streaming workflow. On the left side, you can see a lambda function labeled as model inference. This lambda function will use the trained model stored in S3 and the embeddings from the vector database to provide the recommendations based on the online user activity. Here's how it works. AWS Kinesis Data Streams will receive the online user activity from the sales platform logs. Data Firehose will then read the events from the Kinesis Data Streams and act as a delivery service to load the data into the S3 bucket labeled as the recommendation bucket. 
Before it loads the data into S3, Data Firehose will invoke the lambda function labeled as stream transformation to extract the user and product features from the data streams and then pass these features to the lambda function model inference to get the recommendations. Data Firehose will finally load the recommendations to the S3 bucket recommendations. The model inferences lambda function is already implemented and provided to you, but its configuration is not complete. So before implementing the streaming workflow, you'll need to follow the instructions in section 3 of the lab to configure the environmental variables and the lambda function so that it can connect to the vector database. For that, we will again need the database host, username, and password, which I have already set aside in a separate note from the previous video. In the console, I'll open up the lambda service, then choose the lambda function that has model inference in its name. I'll scroll down and click on the configuration tab, then at the left, I'll choose environmental variables and click on edit. 
Here I'll paste the database host, password, and username that I previously set aside, and don't forget to save these updates. Finally, we can follow the instructions in section 4 to implement the streaming pipeline. In the main.tf file, I'll uncomment this final section that declares a streaming module. I'll do the same thing in the outputs file. Then, I'll run the three terraform commands, terraform init, terraform plan, and terraform apply. This should create the firehose and the stream transformation lambda functions. Note that terraform will not create Kinesis data streams and the recommendations S3 bucket because it's already provided to you as one of the lab resources. 
When you start the lab, a process will run in the background and will automatically stream some events to the Kinesis data streams. After you create the streaming pipeline, the data firehose will automatically start reading the events from Kinesis data streams, invoke the lambda function to transform the data into recommendations, and finally transfer these recommendations to the S3 bucket. You will learn more about the underlying design of Kinesis data streams in course 2. For the final task in this lab, let's take a look at the contents of the S3 bucket. In the console, search for S3, then choose this bucket that has recommendations in its name. Inside this bucket, you'll find that the data is partitioned by year, month, day, and hour. Again, partitioning helps S3 locate specific data more quickly. 
You can also check the logs of the transformation lambda functions. In the console, search for lambda, and then click on the transformation lambda. I'll click on the monitor tab, then on View CloudWatch Logs. These are the logs of the function generated while it was performing transformation. So now it's your turn to try this lab. Make sure to follow the lab instructions carefully and revisit these videos if needed. Once you've completed all the tasks in the lab, don't forget to submit the lab on the Lab Setup Instructions page. 
Keep in mind that the lab environment will expire after two hours. Again, don't worry if any of the lab steps are not completely clear for you. You'll develop a deeper understanding of the tools in future courses. After that, I'll see you back here to wrap up this course. 
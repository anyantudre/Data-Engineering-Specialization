# Serving Data

#### Serving Data for Analytics and Machine Learning
There's more than one way to serve data to your end users. Sharing data as files is one common and straightforward way to serve data. A data scientist might need a text file of customer reviews to perform sentiment analysis. An analyst might need numerical data in a CSV file of invoices to perform some statistical analysis. Or a machine learning engineer might use images of products to develop a product classification system. Well, of course, you could serve text or numerical or image data using a database or object storage. In some cases, you might just share a file directly, say through an email. 
It's difficult to manage the versioning of files this way. Using a data sharing platform helps you ensure a coherent and consistent version of the files you share with your end users. Serving single files one at a time might be sufficient for certain ad hoc requests, but this practice is very hard to scale. If you want to share large files of semi structured or unstructured data, then you would need to share your data through object storage or a data lake. To scale beyond sharing single files, you might choose to serve data directly from your OLAP database or data warehouse. In this case, an analyst or data scientists can query the storage system using SQL or another query language, and then export those results to a downstream application or analyze the results in a notebook. Serving data from a database has its benefits. 
A database imposes order and structure on the data by enforcing a schema. Databases give you fine grain permission controls at the table, column and row level, allowing you to craft complex access policies for various roles. Modern OLAP databases and query engines can offer high performance for complex, computationally intensive queries. If you're working with streaming data, serving data and files may be impractical or impossible, and databases on their own might not have the functionality you need. In that case, you'll need to work with streaming systems that serve data in real time. For example, operational analytics databases are becoming increasingly popular because they allow your end users to perform analytical quarries with low latency across a large range of historical data and up to the second current data. When you serve data from these databases, you are effectively combining the features of an OLAP database with a stream processing system. 
When it comes to data management, you need to ensure that stakeholders trust the data you serve them and that they can interpret it correctly and use it in a consistent manner. So you want to ensure that the data encompasses proper data definitions and logic. Data definition refers to the meaning of data as it's understood throughout the organization. For example, the definition of a term like a customer should be documented and made available to everyone who uses the data Data logic consists of formulas for driving metrics from data, say gross sales or customer lifetime value. Proper data logic depends on proper data definitions and contains details of statistical calculations. So, for example, to compute customer churn metrics, you would need to understand what the word customer means to the end user. Then you can write a SQL query to define this metric once, and it can be reused across the organization. 
This helps avoid a messy and unmaintainable sprawl of SQL code. So formally, declaring data definitions within your organization goes a long way to ensuring data correctness, consistency, and trustworthiness. While you can model the data to help capture data definitions and logic, you can also build a semantic layer on top of your data model to translate the underlying data elements and structures into business terms that are more intuitive and useful for your end users. The semantic layer ensures a single consistent definition for each business term and helps your end users more easily navigate the data to find what they need. The semantic layer can live in a BI tool, or you can create this layer using something like DBT. With DBT, you can define your standard business metrics using YAML file and SQL queries. Once your end users receive the data, they might use a visualization tool or business intelligence platform, such as Amazon QuickSite, Apache Superset, or Looker to create an analytics dashboard, or a data scientist might use notebooks to explore the data, engineer features, or train a model. 
You might also be responsible for helping set up and manage the Cloud platforms that are designed to handle Cloud data science workloads, such as Amazon Sagemaker, Google Cloud Vertex AI, and Microsoft Azure Machine Learning. Those are several ways you can serve data for analytics and machine learning. In the next video, we'll dive into the approach for serving data from a database, focusing specifically on using views and materialized views. 


#### Views and Materialized Views
Interactive Transcript - Enable basic transcript mode by pressing the escape key
You may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key
When you give downstream stakeholders direct access to a database. In addition to serving the data as tables, you can also serve data using table like objects such as views and materialized views. You can create these objects during the transformation stage in your data pipeline or before you serve the data to your end users in the data consumption layer. A view is just a query that you can store in your database to give you and your stakeholders easier access to common queries, and it can also help simplify the process of writing complex queries. Here's an example of how you can create a view. You first type the keywords create view followed by the name of the view along with the as keyword. I'm calling this view customer info and then you specify the query statement. 
Here I'm selecting the first and last name, email, phone number, and address fields from the customer address, city and country tables. The view represents a virtual table, not a physical table that you or the end users can select from, just like any other table. When you select from a view, the database creates a new query that combines the view with the query that referenced it, and then the query optimizer optimizes and runs the full query. Suppose that a marketing analyst needs to frequently run a query on the results from joining the customer address, city and country tables. When you create this customer info view, you are joining together the four tables into a wide table. And so the marketing analyst can simply write queries that filter and perform aggregations on top of this view, rather than having to write a query to join these tables together themselves every time. You can also use views to apply security principles when serving your data. 
For example, you can create a view that selects only specific columns and rows. Then, when you serve your downstream stakeholders with this view, you are effectively restricting their data access to only the data that they need. CTE or common table expressions that you learned back in Course 3 is a SQL concept that's similar to views. Remember that you can create a CTE using the with clause followed by the name of the CTE and then a query enclosed in parentheses. But that query represents some temporary results that you want to reference in a subsequent SQL query, so both CTEs and views help organize the code by making it cleaner and easier to follow. However, CTEs only exist within the scope of the main query where they're referenced, so once the main query is executed, the CTE is discarded and cannot be referenced in other queries. On the other hand, a view is an actual database object that can be accessed by external database users. 
So the body of the query that represents the view is actually stored in the database disk, and it can persist in the database until you explicitly drop it. So views can be referenced and used by your end users across different sessions and queries. Now, with views, you can't perform and store any precomputation, meaning that the query represented by the view needs to be executed every time the view is referenced. So using a view to store a complex query that your end users need to run frequently can be extremely expensive. A materialized view, on the other hand, does some or all the view computations in advance. It then caches the query results and allows you to refresh the data periodically. Here's an example of how you can create a materialized view. 
You start with the create materialized view keywords. Then you give the materialized view a name followed by the as keyword. I'll call this materialized view rental_by_category. Then you specify the query you want to represent with this materialized view. For this example, I want to join together the payment, rental, inventory, film, film category, and category tables. Then I'm going to select the category name and the sum of the payments. Then group by the categories, the query is executed, and the results of the six table joins are saved and cached. 
Then, when a user references this rental by category materialize view, they're querying from the prejoined data. A materialize view is useful when you can tolerate some amount of latency between the refreshes. For the first lab of this week, you'll work with the star schema model you saw back in the first week of this course, you'll create analytical views on top of the model using DBT. After that, you'll be ready to dive into the Capstone labs. In the next lesson, I'll give you a summary of all the key concepts you learned throughout this program. Then I'll give you a quick walkthrough of the Capstone before you jump into those labs. I'll see you there. 


#### Summary of the Program Concepts
In this program, we've covered a lot of ground in the fundamentals of data engineering. We've walked through the stages of the data engineering life cycle and its undercurrents and explored technologies and best practices for building data solutions on the Cloud. You've got hands-on practice with each of these data engineering concepts in the lab exercises, and now you're ready to bring it all together. In this video, I'll quickly recap the major themes and concepts you've been learning about in these courses, and after that, we'll dive into the final capstone labs of these courses, where you'll be applying everything you know. We started this program by taking a look at a framework for thinking like a data engineer. When you start a new data project, you should always work backward by first identifying the needs of your stakeholders and how they will derive value from the data you serve them. You can then translate those needs into system requirements and then choose the appropriate tools and technologies that can help you meet those requirements. 
Only then can you start building and iterating on your data systems. When you focus on your end users and their data needs, you'll be well on your way to adding value for your organization. When building your data system, you will typically start by ingesting data from source systems. How you'll ingest your data will always depend on the type of source system you're working with, for example, a database, a file residing in a file system or object storage, or streaming system or an API. You can ingest historical data from a file or a database by following a batch ingestion pattern, or you can use streaming ingestion to ingest data from streaming systems in real time. Although batch and streaming ingestion are often discussed as separate paradigms, they actually exist along a continuum that ranges from infrequent large batch data ingestion to real time streaming of individual messages as are generated. In between, you have a wide range of batch, micro-batch, and streaming approaches. 
The ingestion approach you take should be guided by your stakeholders needs and your system's requirements. Both batch and streaming ingestion can serve as the extract phase in an ETL, as well as an ELT process. With ETL, you apply some in-flight transformations to data before loading it into the target system. Or you could follow an ELT process where you apply the data transformations after you load the data into your target system. The choice between these two patterns will depend on what transformations you want to apply to the data, the hardware specification of your processing tool and your target system, and the size of your data. Transforming the data encompasses cleaning and combining data from multiple sources, as well as converting the data into a target schema. The target schema depends on the data model you created. 
If you're serving data for analytics, then you might choose to model your data on a star schema or one big table. These models can help your end users more easily and efficiently write analytical queries. If you're serving data for data science or machine learning, how much you process the data will depend on your organization and whether end users want to explore the data, use data to train a machine learning model, make predictions, or something else. To apply transformations, you could issue simple SQL queries or write flexible and modular code in a non declarative language like Python. Depending on the size of your data, you could process your data using a non-distributed processing tool, such as Pandas, or a distributed framework, such as Spark. You also learned that for analytical use cases, you could process large amounts of data inside a Cloud data warehouse to leverage the massively parallel processing power of Cloud computing. Transformation tools, such as DBT, also facilitate data modeling inside a data warehouse. 
Like relational databases, a data warehouse expects structured datasets with a well defined schema. When your end users issue analytical queries to your data warehouse, its query optimizer looks for the best execution plan and then returns the results based on this plan. Data warehouses are more suitable for analytical workloads because they're based on columnar storage, which makes it more efficient at aggregating queries than transactional databases that are role-oriented. On the other hand, you can use a data lake built on top of low cost object storage to support applications, such as machine learning and big data processing that require massive amounts of structured and unstructured data. To prevent data lakes from becoming unusable data swamps, you can create a data catalog to track and manage data stored in your data lake. You also learned about the data lakehouse architecture, which combines the low cost and scalable storage of data lakes with the superior structured querying performance and data management features of data warehouses to provide a unified platform for serving both low latency analytics and machine learning. As data warehouses adapt lake-like features, and data lakes incorporate warehouse-like capabilities, the alliance between data warehouses, data lakes, and data lakehouses are blurring. 
In terms of the data engineering undercurrents, you use IM to ensure the security of your data systems on the Cloud by preventing unauthorized users from accessing your data and resources. Networking concepts such as VPCs, route tables, network ACLs, and security groups can also help secure your resources. You practice good data management by modeling your data, using data catalogs, and properly organizing your data storage to make your data easier to find by your end users. Your end users also need to trust your data. By testing and monitoring the quality of your data, using tools like Great Expectations and Glue Data Quality throughout your data pipeline, you ensure that you're serving data that can deliver value to your stakeholders. You also apply the automation pillar of DataOps, by using infrastructure as code tools like Terraform to automate the creation and management of your data pipeline resources, and use Airflow to orchestrate entire data pipelines. You now know how to design and build data engineering solutions that encompass each stage of the data engineering life cycle and incorporate the key undercurrents of data engineering. 
In the final lab of this program, you will get a chance to work with an end-to-end data pipeline to put all these concepts together. The capstone lab consists of two parts. In the first part, you'll create and configure the resources of your data pipeline. In the second part, you'll integrate data quality checks and orchestration to the same data pipeline. To help you prepare for the capstone lab, join me in the next video to go through some example code and additional materials that can help you complete the lab. After that, feel free to start the lab or watch the lab walkthroughs if you'd like to get an overview of the lab tasks before starting the lab. 


#### Program Conclusion
Great job on completing this data engineering professional certificate program. I hope you've fallen in love with data engineering and are feeling confident in your abilities to apply what you've learned in your own work as a data engineer. In this final video, I'd like to share with you my thoughts on how I think this field is changing and some personal advice that might help you as you continue to explore data engineering. In this program, you've been learning about data engineering in the context of the life cycle and undercurrents that Matt House and I described in our book. Our goal in defining the life cycle stages and undercurrents as we did was to present a framework that will stand the test of time, even as the underlying tools and technologies that support each stage of the life cycle continue to change and evolve. Throughout these courses, we've been looking at the role of the data engineer in the context of other roles in the data team, like software engineering, data science, and machine learning. However, the boundaries between software engineering, data engineering, data science, and machine learning are growing increasingly fuzzy. 
As data becomes more tightly embedded in every business process, new roles will emerge in the realm of data and algorithms. It's impossible to predict the future, but one possibility is that data and machine learning engineering will blend together. Machine learning tool sets become easier to use and manage Cloud machine learning services grow in capabilities. Machine learning is shifting away from adhoc exploration and model development to become an operational discipline. The primary goal of this blended role will be to create or utilize the systems that automatically train models, monitor performance, and operationalize the full machine learning processes for model types that are well understood. They will also monitor data pipelines and quality overlapping into the current realm of data engineering. Today's machine learning engineers might become more specialized to work on model types that are closer to research and less well understood. 
Another area in which titles may morph is at the intersection of software engineering and data engineering. Data applications, which blend traditional software applications with analytics will drive toward this trend. Software engineers will need to have a much deeper understanding of data engineering. They'll develop expertise in things like streaming, data pipelines, data modeling, and data quality and data engineers will be integrated into application development teams, and software developers will acquire data engineering skills. The boundaries that exist between application back end systems and data engineering tools will be lowered as well with deep integration through streaming and event driven architectures. As data technology continues to evolve at an exhausting pace. What's next for data engineering when it comes to tools and technologies? 
A major trend I see continuing in the field of data engineering is a shift toward simplified and easy to use tools and toward interoperability across applications and systems. Abstraction streamlines the development process and allows engineers to focus more on solving complex and value added problems rather than managing low level infrastructure and code. If tooling becomes easier to use, data engineers will move up the value chain to focus on higher level work, I also think that streaming pipelines and real time analytical databases will become more accessible and pervasive. While these technologies have been around for some time, you'll be able to deploy them more easily with rapidly maturing, managed Cloud services in conjunction with a clearer focus on the business use cases of streaming data. Of course, batch transformations will entirely go away. Batch will still be useful for model training, quarterly reporting, and more. But streaming transformation will become the norm. 
Now, what about the hot topic of integrating AI in your workflow as a data engineer? Every company seems to want to do AI, whatever that means. When I dig into actual AI use cases, that can move the needle. Stuff like GitHub Copilot comes to mind as a gateway drug for many companies to get value from AI. It's like staffing your engineering team with interns and junior level engineers who are super convincing and often need their code reward. But for the most part, the code output is passable or at least eventually usable. But there's a catch, I don't expect AI enabled workflows to replace handwritten code anytime soon. 
As engineers will still prefer writing code. Engineers want to engineer. Clanking on a keyboard is a very tactile and cathartic experience for engineers. AI coding assistants and agentic workflows just make it easier most of the time to get work done. These are some trends I expect to see in the field of data engineering in the near future. Finally, I want to leave you with a few words of advice as you continue to explore this field on your own and work on building your data engineering skills and foundation, I encourage you to work in your own data engineering projects. In the resource section, you'll find links to some examples of data engineering projects that can serve as inspiration and blogs that you can dive into to continue educating yourself about data engineering. 
Also, I encourage you to continue the conversation as part of a community, participate in meetups, ask questions, and share your own expertise. You should also keep up to date with the latest developments by reading books, blog posts, and papers, and listening to talks by domain experts who can help you uncover the strengths and pitfalls of trendy technologies and practices. This is the end of the course, and you've made it. But if you're interested in hearing more insights and career advice from industry experts, check out the final and optional lesson in this course. There, you'll find interviews that are recorded with some of my friends, including Jack Wilson, Carly Taylor, and Ben Roggan, where they talk about what it's like to start a career in data and provide their own personal tips and advice. Thank you for joining me on this journey through data engineering. I wish you all the best in your career, and I can't wait to see what you build by applying what you've learned in this program. 
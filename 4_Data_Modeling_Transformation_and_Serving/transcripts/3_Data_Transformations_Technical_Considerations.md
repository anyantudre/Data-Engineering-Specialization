#### Data Transformations and Technical Considerations


#### Batch Transformation Patterns and Use Cases
Interactive Transcript - Enable basic transcript mode by pressing the escape key
You may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key
In my discussions with data teams from many big tech companies, I've noticed that a lot of a data engineer's work is batch processing to serve data for analytics, and in some cases, for training machine learning models. With batch transformations, you manipulate discrete chunks of data on a fixed schedule like daily, hourly, every 15 minutes or less to support ongoing reporting, analytics, and machine learning use cases. Let's go through some common batch transformation patterns. Suppose you created a model for your data. It could be based on a Kimball Star schema, Data Vault, or some other data modeling approach. Now, you're ingesting data into your system, and you need to apply transformations to restructure the source data into the expected form. You have a few options here. 
You could follow a traditional ETL approach and rely on an external transformation tool to extract and transform the data based on the data model you created before loading the transformed data into a target system, such as the data warehouse. Or you can follow an ELT approach where you extract raw data from a sore system and import it directly into a data warehouse. Then you clean and transform the data in the warehouse itself, relying on the storage and computing capabilities of the warehouse. Or you can take a hybrid approach called EtLT, where the t refers to the simple transformations you apply to clean the data, like duplicating the data before you load the clean data into the data warehouse. Then the T refers to the transformations you apply inside the data warehouse to restructure the data based on the model that you defined. In fact, you already experimented with both the ETL and ELT approaches in previous labs. Back in Course 1, you implemented an ETL pipeline with AWS Glue ETL as the external transformation tool to transform the data before loading it into S3. 
AWS Glue ETL is based on a distributed processing framework called Spark. You can use it to perform more complex transformations on larger datasets. We'll cover Spark in more detail later this week. Then in the first week of this course, you implemented an ELT pipeline using DBT to transform the data within a database itself. Note that DBT is not an execution tool like Spark, meaning it doesn't come with compute resources, but it is instead a SQL tool that you can use to facilitate the transformation task within a database or data warehouse by relying on the computing resources of the storage system. In addition to transforming your data into a target schema, you might need to apply transformations to clean and normalize your data. For example, the source data you extracted might have missing values, duplicate entries, outliers, or other inconsistencies. 
This process of taking messy malformed data and turning it into clean data is called data wrangling. You could write your own code to perform data wrangling like you did last week to turn raw data into a useful form for training a machine learning algorithm, but I highly recommend to use a data wrangling tool to avoid undifferentiated heavy lifting. There are many third-party data wrangling tools available to you, and many major cloud providers typically offer their own version of these tools. For example, AWS offers AWS Glue DataBrew as a visual data preparation service for cleaning, standardizing, and transforming data. After you store your transformed data in the data pipeline, you might need to periodically or continuously update the data to make sure that it's in sync with this data in the source system. You can apply a simple approach known as truncate and reload, where you delete all records from your table and then reload the data from the data source, rewriting any transformations needed to get the data into your target system. This approach works well when you have a small dataset and only need to update the data in your target system once in a while. 
However, if your dataset is large, this approach can become very resource-intensive. In this case, you might want to adopt a CDC or change data capture approach, where you first identify the changes made in the source system and then update the tables in your target system based only on those changes. For example, you can check the Last Updated column in a relational source database, or you can check the databases transactional logs. Each row in the log can be labeled with an I, if the row is inserted, a U, if the row is updated, or a D, if the row is deleted. When handling updated rows, you can apply an insert-only pattern or upsert/merge pattern to update your target system. With an insert-only pattern, you insert new records without changing or deleting old records, and you add additional information, the new record to distinguish it from the old one. With an upsert/merge pattern, you take a set of source records and look for matches against your target table by using a primary key or another logical condition. 
When a match occurs, you update the target record by replacing it with a new record. When no match exists, you insert the new record. When handling deleted rows, you can adopt a hard or soft delete pattern. With a hard delete, you permanently remove a record from your target system, whereas with a soft delete, you mark the record as deleted. You might use hard deletes to remove data for performance reasons, say a table is too big, or if there's a legal or compliance reason to do so. You can use soft deletes when you don't want to permanently delete a record, but you also want to filter it out-of-query results. You can also delete a record in an insert-only manner when you insert a new record with a deleted flag without modifying the previous version of the record. 
Single row inserts are commonly performed on row-oriented databases. However, a problem I see a lot is that some data engineers try to perform single row inserts into an OLAP column oriented database. This is an anti-pattern that could put massive load on the OLAP system. It also causes the data to be written in many separate files, which is extremely inefficient for subsequent reads. Instead, I recommend loading data in a periodic micro-batch or batch fashion. When you insert data in bulk, the data can be organized more efficiently into row groups and better compressed. If the OLAP system is distributed, you can leverage the distributed parallel processing capability instead of loading records one by one. 
Those are some of the common batch transformation patterns. Like I mentioned earlier, if you're only working with simple transformations and small datasets, you might be able to get away with performing the transformations on a single machine. However, as the transformations you need become more complex, and the datasets become larger, you'll need to consider using distributed processing frameworks to meet the scalability and performance requirements. While Cloud data warehouses leverage distributed processing power, you might also need to apply transformations outside the data warehouse or inside a data lake. Join me in the next video to begin our discussion on distributed processing frameworks. 


#### Distributed Processing Framework - Hadoop
Over the years, engineers have developed many big data tools to handle the growing amounts of data. Let's take a moment to understand the evolution of key tools and frameworks that have led to today's data ecosystem. In this video, we'll be focusing on Apache Hadoop, one of the earliest frameworks for dealing with large datasets, and it's still surprisingly relevant today. With the explosion of data in the early 2000s, traditional monolithic databases and data warehouses of the 1990s. Were not able to handle the massive amounts of data in a cost effective, scalable, available, and reliable way. At the same time, commodity hardware such as servers, ram, disks, and flash drives also became cheap and ubiquitous. During this time several innovations led to the large scale distributed computing and storage on massive computing clusters that you see today. 
In 2003, Google published a paper on the Google file system, or GFS, which provided a fault tolerant and distributed file system across many clusters of commodity hardware servers. Shortly after that, in 2004, Google published a paper on MapReduce, a new parallel programming paradigm for large scale processing of data distributed over GFS. Google's publications constituted a big bang for data technologies and the cultural roots of data engineering. The Google papers inspired engineers at Yahoo to develop the open source framework Apache Hadoop in 2006. Google's GFS paper provided a blueprint for the Hadoop distributed file system, or HDFS, and MapReduce became part of the framework. Although Hadoop is not considered a bleeding edge technology today. I still think it's important for you to understand the concepts behind Hadoop, because MapReduce still influences many distributed systems that data engineers use today. 
And HDFS is still a key ingredient of many current big data engines such as Amazon, EMR, and Spark. The Hadoop distributed file system is similar to object storage, but with a key difference. Hadoop combines compute and storage on the same nodes, whereas object storage typically has limited compute support for internal processing. Hadoop breaks large files into blocks, each block holding a chunk of data less than a few hundred megabytes in size. The file system is managed by what's called the name node, which maintains directories, file metadata, and a detailed catalog describing the location of the file blocks in the cluster. In a typical configuration, you would replicate each block of data to three nodes, called data nodes. This increases both the durability and availability of data. 
If a disk or node fails, and the replication factor for some file blocks fall below three. The name node will instruct other data nodes to replicate these file blocks so that they again reach the correct replication factor. By combining compute resources with storage nodes Hadoop allows in place data processing. This was originally achieved using the MapReduce programming model. In the MapReduce programming model, you send computation code to the nodes that contain the data favoring locality, rather than bring your data to your application. The computation code consists of a collection of map tasks that read individual data blocks and produce a set of key value pairs. These map tasks are followed by a shuffle that redistributes results across the cluster so that values with the same key are collected together in the same node. 
And finally a reduce step that aggregates data on each node. For example, suppose that you wanted to run the SQL query that selects the user_id and count star, meaning the count of all records from the user_events table, and you'll group by the user_id. So the result will be all the user ids along with the number of records associated with that user_id. With HDFS, the data in the user events table is broken down into data blocks and spread across many nodes. Let's zoom in on one data node that contains these three data blocks. The MapReduce job generates one map task per block. Each map task essentially runs a query on the respective block and generates key value pairs, where the key represents a user id that appears in the block. 
And the value is the corresponding count of records associated with that user id within that block. So, for example, in this first block there are six records associated with user2 and ten records associated with user1, and so on. While the table might contain petabytes of data, each block might only contain hundreds of megabytes. So it's much faster to run the query on a single block than it is on the full table. You then redistribute the results by key, which is the user Id in this example, so that each key ends up on one and only one node. So here all the user1 key value pairs end up in one data node and the user2 key value pairs end up in another data node. This is the shuffle step, which is often executed using a hashing algorithm on keys. 
Once the map results have been shuffled, you sum the results for each key. The key, along with its computed total count, can be written to the local disk on the node where they're computed. Then finally you collect the results stored across nodes to view the full query results. This model is extremely powerful, but it has some shortcomings. It utilizes numerous short lived MapReduce tasks that read data from disk and write intermediate computations to disk. In particular, no intermediate state is preserved in memory, and all data is transferred between tasks by storing it on disk or pushing it over the network. This simplifies state and workflow management and minimizes memory consumption, but it can also drive high disk bandwidth utilization and increase processing time. 
So engineers developed other data processing frameworks that still include some elements of map, shuffle and reduce but relax the constraints of Mapreduce to allow for in memory caching. And since RAM is much faster than SSD and HDD's in transfer speed and seek time, persisting even a tiny amount of chosen data in memory can dramatically speed up specific data processing tasks. For example, Spark, which we'll discuss in the next video, was designed around in memory processing. With Spark, you treat data as a distributed set that resides in memory, and treat disk as a second class data storage layer for processing. Which you only use if your data overflows from the available memory. Nowadays, Hadoop is no longer a hot bleeding edge technology, and MapReduce is not widely used by data engineers. And I think that advancements in leveraging memory for data transformations will continue to yield gains for the foreseeable future. 
Join me in the next video to learn more about Apache Spark, a distributed in memory processing framework that we use in the upcoming lab. 


#### Distributed Processing Framework - Spark
Interactive Transcript - Enable basic transcript mode by pressing the escape key
You may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key
To address the shortcomings of Hadoop MapReduce, researchers at UC Berkeley started the Spark project back in 2009. The goal was to come up with a distributed framework that borrows ideas from MapReduce, but is simpler and faster. Supporting in memory storage for intermediate results and interactive processing of the data. Spark has since evolved to support streaming processing as well as machine learning and graphing libraries. And as new features are continuously being developed and added to this framework by the Apache Spark community, I think it's critical that you gain some experience with Spark. Unlike Hadoop, which integrates compute and persistent storage via the Hadoop distributed file system, Spark is just a computing engine designed for processing distributed large datasets. With Spark, you can perform parallel computations and retain intermediate results in memory, which limits disk IO interactions, enabling significantly faster computation than Hadoop MapReduce. 
You can use Spark locally in data centers or in the cloud. You can load data from and store the final results on separate persistent storage systems such as relational or key value databases, object storage, or even Hadoop file system. Spark provides a unified platform that allows you to run different types of analytical workloads as self contained Spark applications on one processing engine. For example, you can use Spark to perform SQL queries to load data, train and test machine learning algorithms, and apply streaming transformations on your data over the same computation engine. You can write your workloads in Python, Java, Scala, and R using Spark core APIs. Spark also offers built-in libraries such as Spark SQL for writing SQL queries, ML lib for machine learning applications. Spark structured streaming for interacting with real time data, and graphics for graph processing. 
Beyond these standard libraries, you can also use external third party libraries published and maintained by the open source communities. These include connectors that allow you to connect to a variety of external data sources and storage systems, monitor performance, and much more. Let's zoom in on the underlying components of a Spark Application and see how they work together to process your code. A spark application consists of a cluster of nodes. It has a driver node, which is the central controller of a Spark application. A cluster manager node, which communicates with the driver to allocate computational and memory resources across a cluster and manage these resources. And a set of worker nodes where each node contains a spark executor that executes tasks assigned to them by the driver. 
Spark applies a partitioning scheme to break up the data into partitions when loading it from disk, and allocates to each spark executor the partitions that are closest to it in the network. So each executor's cpu core gets a partition of data to work on. When you write a spark application, you start by instantiating a spark session object that represents a single, unified entry point to all Spark's functionality. Through this entry, you can define data frames, read data from sources, and perform SQL queries. The driver node translates the instructions you wrote, which could be in python, scala, or another language, into Spark jobs, which will be executed one by one based on the job's priority. To do this, the driver transforms each job into a sequence of stages and represents these stages as a DAG. For example, this job here has three stages and is represented with this dag. 
So each DAG is sort of like the execution plan for the corresponding job. Each stage is further broken down into tasks written in smart code that can run in parallel. Here, stage 1 has four tasks that run in parallel, and stages 2 and 3 each have three tasks that can also run in parallel. You'll run stages that have shared dependencies serially and run those without dependencies in parallel. For example, in this stack, stages 2 and 3 depend on the results of stage 1, so they can't start until stage 1 is done. But stages 2 and 3 don't have any shared dependencies and can be run in parallel. So to execute this job, Spark will start with stage 1 and run all four tasks in parallel. 
Once these tasks are done, stage two and three will both start, and the three tasks in each stage will run in parallel. Returning to our spark application once the DAG execution plan is developed, the driver communicates with the cluster manager to request computational and memory resources for the executors. Each task is assigned to a single core within an executor, and each executor works on a single data partition. The executor executes the task and communicates the computation results back to the driver node, and finally, the driver node aggregates these computations and returns the results back to you. When interacting with Spark, you don't need to worry about all these underlying details. So whether you write your application using your favorite programming language or using any of the standard libraries. Behind the scenes, your code will be decomposed into tasks and assigned across the Spark executors. 
There's a lot we can cover with Spark, but I want to focus on PySpark, which is the Python API for Apache. Spark Pyspark supports all Spark features, including Spark SQL, Spark dataframes, machine learning, and structured streaming. In the lab, you'll work with structured data using Spark dataframes and Spark SQL. So in the next video I'll show you how to create and work with Spark dataframes in the video after that, we'll go over Spark SQL. 


#### Spark DataFrames
Interactive Transcript - Enable basic transcript mode by pressing the escape key
You may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key
Last week, you performed simple transformations on a small tabular customer churn dataset using Pandas DataFrames. With Spark DataFrames, you can work with much larger tabular datasets that are distributed across Spark executors behind the scenes. But Spark abstracts these details for you, so you can view and interact with your data as if it were a single table. Spark DataFrames are actually built on top of a low-level data structure called Resilient Distributed Dataset, or RDD, which represents the actual partition collection of records that can be operated on in parallel. To work directly with RDDs, you would need to manually define and optimize all the operations you want to perform on your data. But with Spark DataFrames, you can interact with the data using simpler and more expressive high-level operations, such as filtering, selecting, counting, aggregating, and grouping, and Spark will compile these operations down to the RDD level behind the scenes. Spark DataFrames and the underlying RDD are both considered to be immutable data structures, which is what makes them resilient, in other words, fault-tolerant. 
You can classify the operations on distributed data into two types, transformations and actions. Transformations, such as filtering, selecting, joining, and grouping, create new data frames from existing ones without modifying the original data. This is why data frames and their underlying RDDs are considered immutable. Actions, such as count, show, and save, trigger the execution of these transformations. In fact, all Spark transformations are evaluated lazily, meaning that they are not executed immediately. Instead, they're recorded as a lineage and only executed when an action is invoked. This lazy evaluation allows Spark to optimize the execution plan by rearranging transformation operations for efficiency. 
Moreover, the lineage and immutability properties ensure fault tolerance, because these properties allow you to reproduce an original state in the event of failures. In the next video, I'll give you a quick demo of PySpark DataFrames and show you some of the common data frame operations you can perform when working with data in Apache Spark. I'll see you there. 



#### Demo: Working with Spark DataFrames Using Python
Let's take a look at some basic operations you can use to create spark dataframes, manipulate existing dataframes, clean data, aggregate data, and define your own functions for working with dataframes. For this demo, I'll use this pip install command to install Pyspark locally. In the lab you'll be provided with a Spark cluster running inside a docker container. And I've included links in the additional resource section at the end of this week to show you how you can download the full version of Spark. Next, let's install another library called Findspark, which will automatically add Pyspark to the system's path during runtime so that your system knows where to find Spark. I'll import both the Pyspark and Findspark packages, and then I'll initialize findspark. Before you can create a Spark dataframe, you need to create a Spark session, which works as the entry point for working with Spark dataframes. 
So from pyspark.sql, let's import the SparkSession class. Then I'll create a spark session with the name example by calling this get or create method. This method will get an existing spark session with the name example if it exists, or it will create a new one. Here are the details of the session. You can see the version number of the spark session and that its name is indeed example. Now let's create a dataframe. You can do this manually, or you can import data from external data sources. 
Let's say you want to manually create a dataframe that holds the data from this order details table. I'll create a dataframe called orders_df by specifying the spark session object that I created from before and then calling the CreateDataFrame method on it. This method expects two arguments. The first argument is the data, which I'll provide as a list of tuples, each consisting of the data from one row. The second argument is the schema, so I'll specify the name and the type of each column. You can use the show method to view the rows of this dataframe. Let's say instead you want to create a dataframe by reading data from a CSV file that contains transaction data of an online retail store. 
Let's create another dataframe called transactions_df that uses the same spark session object as before. But this time it calls the read.csv method, specifying the name of the CSV file and indicating that the data contains a header. You can view the first five rows of this data by calling the show method with n=5. Now let's say you only care about some of the columns in this data frame. You can first use the columns command to view all the column names, and then select only the columns you want by using the select method. So here I'll select only the price, quantity and country columns, and then I'll use the show method to view the first five rows of these three columns. If you want to quickly explore the contents of these columns. 
You can use the describe method to compute basic summary statistics for each column, including the value, count, mean, standard deviation, min, and max of each column like you see here. Next, let's take a look at how you can manipulate data frames such as adding, updating, renaming, or removing a column from an existing data frame. Say you want to create a new column that represents the amount paid for each product. You can call the withColumn method, then specify the name of this new column which I'll call amount. And the values for this new column which I'll get by multiplying the price and quantity columns from the existing dataframe. Spark checks if the amount column already exists in the dataframe, and if it does, then it will replace this column with the new values. And if this column does not exist, then it will create a new column with these values. 
Remember that spark dataframes are immutable, so this method actually returns a new dataframe, but I can assign this new dataframe back to the same transactions _df variable. You can verify that this new column was indeed added by calling the show method. Next, let's rename the invoice column to id. For that we'll call the withColumnRenamed method and specify that the name of the existing column is invoice and that the new column name should be id. And finally, let's remove the description column. I'll call the drop method and specify the name of the column that I want to remove. In terms of cleaning your data in a dataframe, you can easily remove the rows that contain null values by calling the dropna method. 
You can also call the filter method to remove rows based on Boolean conditions. For example, lets say that the quantity column contains some negative values and you only want to consider the rows that contain positive values. You can call the filter method and provide it with a condition where the quantity column must be greater than zero. That way you'll only get the rows that don't contain null values and contain positive quantity values. Next, lets take a look at some aggregation operations on dataframes. For example, suppose you want to find the total amount spent on each order. First ill use the groupby method to group the rows via the order id. 
Next, I'll sum up the amount column to get the total amount for each order id, I'll call the show method to view the results. As another example, let's count the total number of rows for each country and order the count in descending order. Again, I'll call the groupby method to group the rows by country, and then I'll call the count method to sort the results. I'll call the order by method to order by the count and specify false for sending so that the results are sorted in descending order. While spark provides lots of built in functionalities that you can find in the documentation, it also supports user-defined functions, or UDF. For example, let's say you want to convert all the country names to uppercase. You can define a function called toUpper that takes in a string and returns a string in uppercase. 
Then you need to wrap this function in a UDF object. To do so, let's import the UDF class from pyspark SQL functions. Then I'll instantiate a UDF object called UDFtoUpper specifying the function that I just defined along with this return type. Now I'll select the ID column and the country column and apply the UDF to upper object to the country column. You can see that all entries in the country column have now been transformed to uppercase strings. Alternatively, instead of wrapping the function you defined inside a UDF object, you can add this decorator with the return type inside the brackets. You can then directly apply the toupper function to the country column. 
Again, remember that spark dataframes are immutable. So with the UDF you actually created a new data frame that has a country column in uppercase. Let's replace the country column in the transactions data frame with a column containing the values in uppercase. I'll call the with column method and specify that I want to replace the values in the country column with their uppercase values. And here's the new transactions dataframe. Now that you've seen how Python UDFs work, you should be careful when working with these functions. This is because Spark is native in Scala, and each executor in the worker node is a Java virtual machine, or JVM process that hosts a partition of data. 
When you use Python UDFs, Spark starts a separate python process inside the worker node and requires data transfer from the JVM. For the data to be processed by Python Spark serializes it to a format that Python can understand. The Python process then executes the function row by row on that data, and finally returns a result of the row operations to the JVM. This process is not efficient since the JVM and Python processes will compete for memory resources and is expensive to serialize and deserialize the data. For better performance, you should consider rewriting Python UDFs in Scala or Java because these functions will run directly within the JVM. And the good news is that after you register these UDFs in Spark, you can still use them in Python. So that was a quick overview of the basic operations you can do with Spark data frames. 
Join me in the next video to see how you can issue SQL queries with Spark SQL. 


#### Demo: Working with Spark SQL
Interactive Transcript - Enable basic transcript mode by pressing the escape key
You may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key
In the previous video, you saw how you can use spark data frames to explore and process your data using Python code. In this video, you'll learn how you can also write SQL code to interact the same data. When working with PySpark, you can choose to manipulate your data, using SQL code, Python code or a mix of both. Both types of code will run on the same computation engine and will compile to the same low level code. Here we have the same Jupyter notebook that you saw in the previous video, and here's the transactions data frame we ended up with. To issue SQL queries to interact with this data, you need to create a temporary view from this data frame. A temporary view is a virtual table that doesn't actually hold the data. 
It persists as long as the spark session is running and provides an interface for you to work with the table data using SQL code. Here, I'll call the createOrReplaceTempView method on the transaction data frame to create a temporary view called orders. Now you can issue SQL queries on this temporary orders table. Using the Spark session object, you can call the SQL method and specify the SQL query as a string inside the brackets. This method will return a data frame representing the results of the query. For example, let's say you want to find the total amount spent on each order. I'll select the ID and the sum of the amount column, which I'll rename as total from the orders table and group by the ID. 
Then I'll order the results in descending order. Here's the return data frame. You can see the total amount spent in descending order. You can also define your own function and use it inside the SQL query. When working with data frames, you saw that you need to either wrap your function inside a UDF object or use the UDF decorator. With SQL queries, you need to register your function. As an example, let's write a function called two lower that transforms a word into lowercase letters. 
To register this function, I'll call the register method using the Spark session object. Then for the first argument, I'll specify a name for the function, which I'm calling udf_to_lower, and then I'll pass in the function itself as a second argument. Now, using the name of this function, you can use it inside the SQL query. For example, you can select the distinct countries from the orders table and apply this UDF to lower function on the country column. You can also create more than one view, meaning more than one virtual table, and join them in a SQL query. For example, let's first create another data frame called product_category_df that contains the code and the category of three products. Then using this data frame, I'll create another temporary view called items. 
Then in the SQL query, I'll join the orders table from before with this new items table to find the average amount for each category. I'll select the category and the average amount from the items table, left joined with the orders table based on the item ID from the items table and the stock code from the orders table. Then I'll group by the category, so I can find the average order amount per category. That's a quick overview of how you can issue SQL quarries to interact with data using PySpark, but now it's your turn to practice. In the next lab, you'll use PySpark to transform your data into a star schema using the same data you saw from the first week of this course, but before that, in the next video, Morgan will share more details on Amazon EMR, which you'll use to run PySpark in the lab. After you finish the lab, we'll dive into the technical considerations for transforming your data. 


#### Amazon EMR
Back in Course 1, you were introduced to Amazon EMR as a big data tool that supports a wide range of processing frameworks. In the upcoming lab, you'll use EMR to run Spark jobs in an Amazon EMR Studio notebook. In this video, I want to help you understand a bit more about what's going on behind the scenes when you run Spark jobs on Amazon EMR, as well as some of the other features of EMR. So let's get into it. If you remember our earlier lesson about Amazon Redshift, you learned how it uses massively parallel processing to tackle big data analytics. EMR works in a similar way, in that there is a cluster with multiple nodes and each node does a portion of the work. When you submit a job using EMR, it gets run across these nodes working in parallel, and each node processes a part of the data. 
Because of this parallelization, the job is completed much quicker than what could be achieved by a single machine. The size of your cluster can impact how quickly your jobs run, and with EMR, a cluster is elastic, meaning that it can scale up or down as needed. Once your job completes, the results are stored in your desired destination. This could be in Amazon S3, HDFS, or another data store option. You can then analyze the results or feed them into another application or workflow for further processing. EMR also supports numerous popular big data frameworks, including Apache Spark, Hadoop, Hive, Presto, Flink, HBase, and many more tools and frameworks that enable data analysis tasks. EMR provides a managed environment that simplifies the setup and scaling of these frameworks and integrates natively with other AWS services. 
With EMR, you can focus more on your data workflows rather than the underlying infrastructure. For example, if you want to analyze data stored in S3 with Hadoop, you can do that with the integration between S3 and the Amazon EMR file system. This allows you to decouple compute and storage and analyze large amounts of data that may not be able to fit on local storage. The way this works is that when you launch your cluster, EMR streams the data from S3 to each instance in your cluster and begins processing it. Another advantage of storing your data in S3 is you can use multiple EMR clusters to process the same dataset in different ways. EMR can also integrate with other AWS data sources like Amazon DynamoDB, Amazon RDS, and Amazon Redshift as a few examples. In the up coming lab, you'll use Amazon EMR Studio, which is a browser-based IDE for Jupyter Notebooks that runs on EMR clusters. 
You'll use a notebook to run a Spark job. Let's run through a basic example of using a notebook so that you're prepared to complete the lab. Here I am in the AWS Console, and I first need to create an EMR cluster to work with. To do that, I will navigate to the EMR dashboard by typing EMR into the search bar, then selecting "EMR". From here, I can create the cluster. For this, I want to launch a serverless cluster. I will select "EMR Serverless" from the navigation, then "Get Started". 
To manage EMR Serverless applications, you need EMR Studio. I will select "Create and Launch EMR Studio". Now we can create an application for EMR Serverless. I'll give the application a name like example app, then leave the type as Spark, and accept the default release version. Then I need to select the application setup options. I can choose from accepting the defaults for batch workloads, accepting the defaults for interactive workloads, or use custom settings. I'm going to use the defaults for interactive workloads, which includes enabling the interactive endpoint for EMR Studio, which needs to be configured so that we can use EMR Studio to run applications on our cluster. 
Now I will select "Create and Start Application". The application is starting, which will take a few minutes. We'll come back when this is done. The application is now started, which means I can create the workspace and notebook that we will use to run Spark jobs. I'll select "Dashboard", then "Create workspace". This brings up an error at the top that tells me I need to enable this studio for interactive work spaces. So I'll select "Edit Studio". 
This takes me to a page where I can edit the configurations for my studio. To use workspaces for interactive workloads, you have to configure the storage location for work spaces and the studio service role. I'll select an existing IAM role for this studio. This IAM role lets the studio interact with other AWS resources. Then I will also navigate to an existing S3 storage location in this account. This is the backup location for workspaces. Once both of these values are filled out, I'll select "Save changes", and this studio has now been updated. 
I can then select "View Workspaces" to go back to creating the workspace needed to launch the notebook. From here, I'll select "Create Workspace". Then I can give the workspace a name. I'll call this example workspace. Then select "Create Workspace", accepting the rest of the default values. Now that this workspace is created, I can launch it to gain access to the notebook. Once I open the workspace, I can check to see if it's connected to any specific EMR application because this is just a frontend to interact with the EMR cluster on the backend. 
We need to attach this notebook to compute resources. If I select the "Compute" tab, I'll see that this notebook is not connected to an EMR Serverless application. To do that, I will select the drop-down to select the application we created earlier. Then I also need to select the interactive runtime role the notebook will be using. This is the role that will be assumed to call other AWS services on your behalf. I'll select the drop-down and select EMR Serverless role, which is an IAM role I already created in this account. Now that this is configured, I can submit jobs to this application. 
Now, if I go back to the files in this workspace, there is a notebook created for me that is empty. I'll select that, then I need to choose which kernel I want to use. I'm going to select PySpark for the kernel. Next, I want to run a very simple PySpark script to do some data analysis on a file that I have uploaded to an S3 bucket in my account. This file contains generated sample taxi data. I want to write a script that will calculate the average fare for the taxi rides between two dates. So I will paste in a script to perform that task, and then hit "Shift+Enter" to run it. 
This will take some time to run, and the output will show under the code snippet. Okay. So the job is done running, and the average fare for the rides contained in this dataset comes out to be $50.64. This code was run as a job on the EMR Serverless cluster this notebook is attached to. Behind the scenes, this code had to call some AWS APIs to be able to read the file from S3. All right. That was a simple example of using EMR Studio and workspaces. 
Hopefully, this gives you a better understanding of EMR, and you'll have a chance to do some more interesting analysis on data yourself in the upcoming lab. Good luck. 


#### AWS Glue Overview
Hi there. My name is [inaudible] I'm a Senior Solution Architect, specialized in analytics AI and machine learning. In the development of this course, I've been working behind the scene with Joe and the team at DeepLearning.AI, to put together all the lab exercises you have been working through. First of all, I would like to say congratulations to making it all the way to the final course in the program. You have really accomplished a lot, and the skill you have learned are going to serve you well in your work as a data engineer. In each of the labs so far, there have been some aspect of the data infrastructure that you have worked on directly or set up yourself and other aspects that we have provided for you. When it comes to batch ETL process specifically, in many cases, you have used AWS Glue, but for most of the part, the details have been abstracted from you. 
Here, I would like to pull back the curtain, so to speak, and show you the details, what's going under the hood and how to set up those jobs you have been running to ingest and transform data. Behind the scene, AWS Glue leverages Apache Spark to process the data. Now you have learned about Apache Spark. I would like to show you how you can create PySpark script to run your ETL processes using Apache Spark on AWS Glue. But before we get into the details, let's learn a bit more about AWS Glue as a service. As you know, when you're setting up the data pipeline for analytics machine learning, or really whatever end use case, you will often be ingesting data from multiple data sources, light databases, object stores such as Amazon S3, logs, APIs, data from streaming platform or others. First and foremost, AWS Glue is a data integration service that help you bring all this data together from different data sources, perform the transformation you need on the data, and then load the data into some downstream complain of your system, which could be a database, a data lake, a data warehouse, or other destination. 
When you create an ETL pipeline to accomplish all these tasks using AWS Glue, we call that Glue job. There are three options when it comes to creating and running Glue job. The first option is to use AWS Glue DataBrew, which is no code or low code informant. With AWS DataBrew, you can visually see how the data looks as you perform all transformation and data manipulation. It's like working on an Excel spreadsheet, but power of Spark. With the DataBrew, you don't need to be proficient in Spark or do any coding at all to get the job done. The next option is authoring Glue job in a Glue Studio. 
This is slightly for more advanced ETL user who have a fair understanding of building ETL pipeline and are ready to write some Spark codes. In the Glue Studio user interface, you can drag and drop different data sources, transformations, and target destinations. Your third option is to create Glue job in a Jupyter notebook, where you will need to write your own Spark code from scratch, and then run it using AWS Glue, or you can use help from Amazon Q Developer or Amazon Q chatbot to get started. You can then perform ETL operation using Glue ETL and extract your ETL pipeline using Glue triggers, blueprints, and workflow. Another key feature you have seen in a lab in the previous courses is AWS Glue centralized data catalog and data governance, which are essential for building data lakes and lake house. In the lab, you have used AWS Glue Crawler to crawl over the data from different data sources, extracting the metadata, and storing this information in your Glue Data Catalog. This allow you to see things like definition, structures, data type, and partition, information from your data. 
AWS Glue is a serverless in nature, so you don't have to worry about provisioning and managing any resources. You can easily scale up your ETL jobs. You can start with a few data processing unit or a DPU, which is basic unit that Glue uses to process data and then scale up as needed. You have also seen how AWS Glue Data Catalogs support integration with other AWS too. You can integrate AWS Glue Data Catalog with Amazon Athena to run SQL query against your data with a QuickSight to build BI dashboards and a SageMaker to build, train, and deploy your machine learning model, and with many other services. In the first lab of this specialization, way back in the first course, you have spun up a data pipeline that looked like this, where you were ingesting normalized data from Amazon RDS database instance, performing some transformation to the model the data into star schema, and then loading the data into Amazon S3 bucket to serve a data analytics and use case. You perform the extract, transform, and load or ETL portion of this pipeline by running Terraform script to spin up Glue and run a Glue ETL jobs. 
The Python script called Glue_jobs.py was provided to you in the lab. In the next video, I walk you through exactly how you can generate the code for that Glue ETL job yourself. I will see you there. 

#### Demo: AWS Glue Visual ETL
As I said in the previous video, I would like to show you now how to use Glue Studio to generate the Glue job. Here, we will generate the code in that glue_job.py script that you have run in the first lab for this program. In that lab, your architecture looked like this, and it's this Glue ETL job here that we have been looking at in the details. Remember, the purpose of Glue job was to ingest normalized data from an Amazon RDS instance, apply transformation to model the data into star schema, and then load the data into Amazon S3 bucket. Let's see how that works. First off, recall that schema of the normalized data in the database looked like this, where you have tables from customers, product, productline, orders, orderdetails, payment, employees, and offices, and you transform the data into star schema, consisting of a central fact table containing some measurement related to the orders, and then surrounding dimensions table provided more context on those order, including information about customers, products, and location associated with each order. Now let's take a closer look at the glue_job.py file that you have used to accomplish this task. 
There are several section in the script, and I will just quickly walk you through each of these sections, so you are clear on what's going on here. To start, of course, this is a PySpark script. We start with some imports. As you can see here, we are importing some packages from awsglue, and then also from pysparks, because, of course, when you run this script, you will be using Spark behind the scene. Next, there is the functions' definitions. This one is called sparkSqlQuery to run SQL queries on data frame. After that, you are using getResolvedOptions method from the awsglue to pass command line argument, such as JOB_NAME, glue_connections, glue_databases, and target S3 path. 
Following that, this is the sum context setup, where you've been creating a SparkContext and a glueContext, as well as initializing a Glue job using the provided JOB_NAME. Then comes the actual ETL portion. Without getting too much into the details here, with this section, you are setting up the database connection and extracting data from source tables. Next, you are applying transformation to the data to build star schema. You can see you are accomplishing that running a series of SQL statement. Finally, you are loading the transformed data into S3. Now, of course, you could sit down and write a script line by line, but instead, the way we generated this script for you in the first lab was using Glue Studio. 
I'm going to show you how to do that now. Here, I'm in AWS console, and I have an RDS database instance up and running. I also have an S3 bucket here where I want to send the transform data. Now I'm going to head over here and open up the Glue Visual ETL tool in a Glue Studio, and you can see that I have blank Canvas to start creating my Glue job. When I click this "+" button on the left, I get a list of choices for the various element I can add to the glue jobs. Here you can see there are tabs for sources, transform, and target. You can select from this to add different extract, transform, and load component to your data pipeline, and we're going to do that here. 
Since the first thing we need to do is extract data from RDS, at this source, I'm going to choose MySQL data source node and add it here, and I will name this node customer_source to indicate this will represent the ingestion data from customer tables. I will add a database connection here, choosing the connections from the RDS database that I've been running now. You can see this here, this will set up as a JDBC connection. Here, I will indicate the table name that we will be ingesting data from. Then I need to indicate an IAM role for a Glue job to assume when running this step of the job. You would first need to set up this IAM role, but I have already set up this role called Cloud9-de-c1w2-glue-role, and I will choose that here, and that will give Glue the appropriate permission to read the data from RDS, as well as to eventually write data to S3. Now I can see a preview of the data that will be extracted here from the customer table. 
The next step will be to add a transform to create the customer dimension table for the star schema. I can go back to the + button here, and then in the Transform tab, you can see there are many different transforms I can choose. But in this case, I'm going to choose SQL Query, and notice, I have the customer_source node selected here. When I choose SQL Query, I will add that here below the customer_source. Since this will be transformation, apply to the customers data. Now, over here, I need to write a SQL code to perform here. I'm going to cheat a little bit because I already have a SQL I need over here in the final glue_job.py script that we were looking at earlier. 
I will just copy that and then paste it over here to save some time. If you don't already have your SQL code written, of course, you could just type it in here, and I will name this query dim_customers, because what we are doing here is creating the customer dimension table of the star scheme. You can see here it's already connected with the source. Then here under SQL aliases, I will write customers, and then that is what the source will be referred in the query. Down here, where we are saying from customer in the query, the reference will be customer_source, which, of course, is reading from the customer table in RDS. With the rest of the query here, you can see that from the customer_source, we are extracting this fields and creating that dim_customer table. The other source and transform step will be quite similar. 
I will just step through them quickly, and I will add product_source as next MySQL source, and I will add RDS connection and specify the table name. If we look again, the glue_job.py script, you have seen that for the product dimension table, we actually need to extract from two sources, product and productline, and join them. I will just copy this SQL code from next step where we add the SQL Query transform. But first, I will add another MySQL source here, use the same setting and specify that this one will read from the productsline table. Now I'm ready to add that SQL Query transformation, and I will call this one dim_products, and then I'm going to choose two sources, both the product_source and productline_source. Then I will page the SQL code in here that will be performed on the data from this sources to create dim_product tables. Then I need to add the appropriate alias here. 
Now we need to create one more dimensions, and that's a dim_location table. Again, looking at the SQL code here from the glue_job.py, you can see that this is also drawn on the data from the customer table, and so I will add another SQL query here, and I will call it dim_location and choose customer_source as a parent node. I will set up customer [inaudible] in alias, and then I will grab the SQL code to define the transformation. Those are the three dimensions, and now we just need to create the fact table. Now just peeking over here at the SQL code for the fact table, you can see we need to do a bunch of joints. We need to join the order table with the ordersdetail, product, customers, and locations table. I will just copy this code. 
I will just quickly create two new MySQL sources, one for the orders and one for the orderdetails, and then I will add other SQL Query transformations, and then I will call this one fact_order. Then for the parent source node, I will choose customer, products, order, orderdetails, and the dim_location table. Then I will paste the SQL code in here. Then I need to add the appropriate alias here. That's it for the extract and transform step for the pipeline. Now we just need to specify the target destination to low the data into. The target destination is going to be Amazon S3 bucket, and I already have bucket setup, and I have created this folder just to keep things organized. 
I have a processed_data folder, and then inside there, I have one folder for each of the fact and dimension table. These are all empty right now, as you can see. But when we run the Glue job, we should be able to see this have been populated with the transformed data. Now I'm going to come back over here and add a target node, and I will choose Amazon S3, and I'm going to set up a separate target node for each of the fact and dimension table. I will call the first one dim_customer_target, and then I will choose dim_customer as the parent node. I will choose Parquet as the format, and then for now, you don't need to worry about the rest of the settings. I will just accept the defaults. 
Then I can click here to browse S3 to identify the dim_customer folder where I want this to go. Then I'm just going to do the same for other's tables. I will add S3 targets and set them to pull from the appropriate parent nodes and deliver to the appropriate folder on S3. Now we have everything for the Glue job defined here in the Visual ETL interface. Then if I click over here on the "Script" tab, you can see that script for this job has now been written. To run this Glue job, I can first click over here on the "Job details". This is where you can define number of workers. 
I'm just going to set this to two, and the job timeout, which I will set to three minutes, since this shouldn't take too long. I can verify here that job has her appropriate role assigned. Then I will hit "Save" up here. Next, I can click over the "Run" tab and select Run job. After a couple of minute, I can see that job was completed successfully, and I can head over to S3 and verify that transform table now exist in my S3 bucket, and they are ready to be queried by the analyst. Just to summarize what we did here. I started with an RDS database instance, an S3 bucket, an IAM role all ready to go. 
I then use Glue Studio Visual ETL tool to define source, transformation, and target for my ETL pipeline. With all these things defined, I could have a look at the script that was generated for the Glue jobs. Then with a few more parameters set, I saved the whole jobs and ran it. Now, as I mentioned, this is just one way to create and run a Glue job. You could have choose the low-code, no-code data proof approach, or you could go more manual Jupyter Notebook approach. That's it. I hope you have enjoyed this little demo and are feeling much more confident now in your ability to set up and run Glue job of your own. 
Good luck. Thank you. 

#### Technical Considerations
Interactive Transcript - Enable basic transcript mode by pressing the escape key
You may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key
To transform your data using PySpark, you saw that you can issue SQL queries with Spark SQL or operate directly on Spark DataFrames using Python. When choosing between using SQL versus Python on Spark DataFrames, you should consider things like the transformation complexity, code reusability and testability, and the skills and technical background of your team. When you're working with PySpark, performing simple transformations, such as filtering, grouping, and aggregation, directly on a Spark DataFrame using Python, or coding these transformations as SQL queries, generally results in comparable performance. This is because both Python and SQL approaches are ultimately translated into the same execution plan and executed by the same underlying Spark computing engine. However, if the transformation is more complex, you might not be able to implement it in SQL or at least not in a straightforward way. For instance, if you want to apply the transpose operation to swap the rows and columns of a table, you can simply call.T the DataFrame. But transposing is not supported by Spark SQL. 
Or as another example, you can normalize columns and clean columns with missing values in SQL, but that might require more code than working with Python on Spark DataFrames. Moreover, working with DataFrames would help you write more testable, maintainable, and modular or reusable code. While reusable libraries are easy to create in Spark and PySpark, the SQL doesn't have a good notion of reusability for more complex query components. You also want to consider the skills and technical background of your team. You might find writing SQL queries to be simpler and easier than working with Python on Spark DataFrames. Depending on your transformation use case, you might find one of these approaches more suitable than the other. You can even try to combine both the Spark DataFrames and Spark SQL approaches to realize the best of both worlds. 
Now that we've covered the considerations for using Spark SQL versus using Python on Spark DataFrames, let's take a moment to discuss when you'll want to consider using a distributed framework like Spark to begin with. Like you saw last week, instead of using Spark DataFrames, you could just use Pandas DataFrames to process the data. However, Pandas is not a distributed framework, it'll load the entire data into the memory of the machine on which your Python code is running. If your dataset is not very large, and by that I mean, you can fit your entire data into memory, then you can use Pandas instead of Spark. In fact, using Spark on a small dataset can be overkilled, since you need to manage the cluster of nodes. On the other hand, if your data is so large that it doesn't fit entirely in the memory, or if you want to leverage distributed computations to enhance processing performance, then you should go with Spark and maybe run it on the cluster running in the Cloud. In any case, whether you're working on a single machine or a cluster of nodes, the best practice would be to extract only the data you need from the source. 
The less data you have to process, the less resource heavy and more performant your code will be. You might need to apply transformations, such as joining, grouping, and filtering inside the database before ingesting the data to reduce the size of the data. Choosing the right coding approach for batch transformation is about balancing the simplicity of coding with SQL with the flexibility and modularity of coding with non declarative languages such as Python. Choosing the right tool for batch transformation depends on the size of the data you want to transform and the specification of the hardware on which you're running your code. Make sure to understand the trade off between these different approaches. Now that we've talked about batch transformations, join me in the next lesson to learn about streaming transformation and how a streaming processing tool can affect the latency of your system. 


#### Streaming Processing
Back in Course 3, we discussed how you can apply queries to streaming data, and you practice writing those queries using flank. Those streaming queries can help your stakeholders perform real time analytics on streaming data, but you can also use those queries to apply transformations to your streaming data. Streaming transformations aim to prepare data for downstream consumption by converting a stream of events into another stream by enriching it with additional information or joining it with another stream. For instance, you might have an incoming stream carrying events from an IOT source. These IOT events carry a device ID and event data, and you might want to dynamically enrich these events with the device metadata stored in a separate database. You can use a stream processing engine to query the separate database containing this metadata. Then you can generate a new stream of events by adding this metadata to the existing IOT events. 
In fact, you already have some experience with this. In one of the previous labs, you applied a streaming ETL to transform a stream of user session events by enriching each event with a timestamp, representing the processing time and some additional metrics that you computed from the data of each user session. As another example of streaming transformation, you can use window queries to dynamically compute roll up statistics on windows and then send the output to a target stream. In terms of joining two streams, you could, for example, use streaming transformation to combine a stream containing website click stream data with another stream containing IOT data to get a unified view of the user activities. All of these examples, events are typically streamed to you by a streaming platform such as Kanesa Data Streams or Kafka, and then you can process these events using a stream processor. Similar to batch processing. There are distributed stream processing tools like Spark streaming and Flink that you can use when you have large datasets. 
Both of these tools are open source and allow you to write Python code or SQL queries to process large streams of data. When choosing a streaming processing tool, it's important to understand your use case, the latency requirements, and the performance capabilities of the framework in question. Some of these tools like Spark streaming, process your data in a microbatch way, providing near real time performance. It accumulates small batches of input data anywhere from two minutes to seconds. Then processes each batch in parallel using a distributed collection of tasks similar to the execution of a batch job in Spark. On the other hand, true streaming systems, such as Flink are designed to process one event at a time. Each node in the system is continuously listening to messages from other nodes and outputting new updates to its dependent nodes. 
True streaming systems can deliver the process events at a lower latency than the microbatch processing systems. However this comes with significant overhead. Depending on your use case and the acceptable latency, you may choose one over the other. If you're collecting sales metrics published every few minutes, micro batches are probably just fine as long as you set an appropriate microbatch frequency. On the other hand, if your ops team is computing metrics every millisecond to detect malicious attacks, you might need true streaming. To learn more about the underlying differences between Spark streaming and Flink, I included some optional materials that compare the architectures of both tools along with code examples showing you how to use Spark Streaming. Then after that, you'll complete the final lab of this week, or you'll practice performing streaming transformations by implementing a streaming change data capture or CDC pipeline. 
You'll use diabesium as a tool to capture the changes from your source database, then push those changes to a CFI stream and process the changes using Flink. To get an overview of this lab, you can check out the lab walk through, or you can jump straight into the lab. After that, join me for a summary of this week's materials. 


#### Summary
>> This week, we looked at the details of transformations and the technical considerations for various data processing frameworks that you can use to transform your data. You saw that you can transform your data not only to model it according to your target schema, but also to increase its value and quality by cleaning it and enriching it and keeping it updated in your data pipeline. When it comes to batch transformation, we discussed several considerations, including the size of the data you need to transform and the memory specification of the machine that is running your code. You saw how you can use distributed processing frameworks if the data you need to process doesn't fit into memory, or if you want to leverage parallel computation. And you learned how these big data tools such as Spark evolved from Hadoop MapReduce, where intermediate results are stored on disk. You got an overview of how Spark works behind the scenes and got the chance to practice coding with Spark DataFrames as well as Spark SQL. While coding in SQL might be handy for simple transformations, coding with DataFrames helps you implement more complex transformations and makes your code more modular, hence easier to maintain and test. 
When it comes to streaming transformations, understanding your use case and the performance requirements of your system will help you choose between a micro batch framework, such as Spark Streaming, or a true streaming system, such as Flink. In the final lab of this week, you implemented a log based CDC approach using Debezium and Apache Kafka and used Flink to process the database updates. And with that, we finished discussing the critical transformation stage of the data engineering lifecycle, where you turn your data into something useful for downstream use cases. Next week, you'll learn how to serve your transformed data to downstream stakeholders. As the final week of this entire program, you also get the chance to put many of the data engineering concepts you learned throughout this program together in a Capstone project. I'll see you there. 



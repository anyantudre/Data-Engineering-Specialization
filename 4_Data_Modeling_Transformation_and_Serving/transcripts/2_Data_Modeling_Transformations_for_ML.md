## Data Modeling and Transformations for ML

#### Overview
Last week, we covered several data modeling approaches for batch analytics that structure data in a way that supports analytical queries. This week, we'll cover how you can prepare and model data for machine learning use cases. The goal of this type of modeling is to structure the data in a way that can help the data scientist or a machine learning engineer understand what the data represents so they can use the data to develop machine learning systems and discover hidden patterns in the data. Building a machine learning system involves several phases, so here I'm going to refer to the machine learning project lifecycle framework that Andrew presented in his machine learning and production course. You'll find this framework useful for a large variety of machine learning projects. These phases include scoping out the machine learning project, collecting and preparing the data, developing the machine learning model, and deploying the machine learning system. Your job as a data engineer is to build and maintain the data pipelines that serve data to one or more of these stages. 
In these pipelines, you collect data from multiple sources, then combine the data into a format that is suitable for the machine learning algorithm. You might also be tasked with cleaning the data, converting it, or even creating some additional columns or features. And finally, you store and share the data with the machine learning or data science team to help you avoid confusing a machine learning model with what you'll be doing as a data engineer, which is modeling the data. I'll rename this modeling phase to algorithm development to clearly indicate that this phase of the machine learning project lifecycle is all about developing the algorithm for the machine learning project. This framework is highly iterative, and we'll dive into the details of each phase later this week. So you might wonder how the role of a data engineer is different from that of a data scientist or machine learning engineer. Admittedly, the boundaries between machine learning engineering, data science, and data engineering are increasingly fuzzy, and the responsibilities of each of these roles vary dramatically between organizations. 
Some organizations might have entirely separate data teams that handle the entire lifecycle for all machine learning projects. In other settings, you, the data engineer, might be responsible for serving just the raw data to the machine learning or data science team. Then the machine learning engineers or data scientists take over the data processing tasks. Or you might be asked to process the raw data so that these downstream stakeholders can use the process data directly to train the machine learning algorithms. If your organization is small or doesn't have a mature machine learning team, you might even handle some of these tasks that are extremely specific to machine learning, such as featurization of data. But in any case, as a data engineer, you play a key role in helping your organization adopt a data centric approach to machine learning, which focuses on enhancing the machine learning system by collecting high quality data. You might have heard the saying garbage in, garbage out, which refers to the idea that the quality of the output of any system is determined by the quality of its inputs. 
So by carefully preparing the data for the machine learning algorithms, you can help the data scientists or machine learning engineers extract accurate and meaningful insights, creating more useful machine learning systems. And so I suggest you develop a basic understanding of how machine learning works because this will go a long way in helping you provide value to your organization when it comes to building machine learning systems. I won't attempt to teach you everything about machine learning, but will focus on how to structure tabular, image and text data when working with classical or advanced machine learning algorithms. To learn more about these machine learning techniques, I have included a list of books and courses that you can check out in the resource section at the end of this week. So this week I'll start with an overview of some key machine learning terminology, and we'll dive into the phases of the machine learning project lifecycle. Then we'll discuss how you can structure tabular data for classical machine learning algorithms. In particular, you'll perform some aspects of feature engineering and data processing steps in the first lab of this week. 
In lesson two, you'll learn how to model and process unstructured data. You'll prepare image data for classical machine learning and also more advanced machine learning algorithms such as convolutional neural networks. And finally, you'll work with text data. Nowadays, most preprocessing steps for textual data can be handled by large language models. However, you should still be aware of these steps in case you need to manually process your textual data to meet the specific cost or system requirements of your machine learning project. So I'll cover some basic preprocessing techniques and show you how to transform text into vectors that can be used to train a machine learning algorithm. And you'll get a chance to practice working with textual data in the second lab of this week. 
So there is a lot to cover this week. Join me in the next video to get started. 



#### Machine Learning Overview
Interactive Transcript - Enable basic transcript mode by pressing the escape key
You may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key
Let's go through some basic machine learning terminology and concepts that you should be familiar with as a data engineer. In this video, I'll cover topics such as supervised versus unsupervised learning, classification versus regression, and training versus testing. I'll also go over the different phases of the machine learning project lifecycle to give you a framework for thinking about how data is used throughout the development of a machine learning system. So if you're already familiar with these machine learning concepts, this video should feel more like a review for you and feel free to skip ahead. But if you're new to machine learning, this video should cover what you need to be ready for this week's materials. Let's say you work as a data engineer at an e-commerce company that's interested in developing three machine learning systems. The first system aims to predict customer churn, in other words, whether a customer will stop purchasing from the company over a specific period of time. 
Suppose you have access to a historical customer demographic data, their browsing and purchase history, and the past ten years worth of customer churn data. The company wishes to learn from this data and create a rule or formula that can take in a customer's information and predict whether they will churn or not within a specific time period. Instead of manually looking at the data to figure out this rule, which will likely be very complex, the company can train a machine learning algorithm to automatically learn the rule through a technique called supervised learning. It learns from the characteristics of historical customer information, which are also called features, and the value that needs to be predicted, meaning whether or not a customer will churn, which is called a label. This type of machine learning is called supervised learning because the labels supervise what needs to be learned from the features. So when you create a data pipeline that serves data for supervised machine learning use cases, you would need to work with the machine learning team to learn what features and labels they want the data to contain. The second system the company wants to develop aims to predict the sales of products for the next new year holiday. 
This is also a supervised machine learning problem because the machine learning algorithm is learning to predict future sales from a set of features and labels, with the features being characteristics of historical product and sales data and the labels being historical holiday sales amounts. But the type of labels used in this second system, meaning the kind of values we're aiming to predict, is different from the first system. In the first case, the label is categorical and only takes on a limited number of values. The customer will either churn or not. This type of supervised learning is called classification you are classifying a customer as a customer who will churn or a customer who will not churn. In the second case, the label is numerical, representing the value of sales. This type of supervised learning is called regression. 
So as a data engineer, the type of labels you will serve will be different for classification and regression machine learning use cases. The third system the company wants to develop aims to segment customers into groups based on their purchasing behaviors. Although you have lots of historical customer data, this data is not labeled for this purpose, meaning that it does not contain specific information about the groups themselves, such as the number of groups or the characteristics that separate the groups, and it does not indicate which group each customer belongs to. This is what the company would like to know by applying what is called an unsupervised learning algorithm. In this case, there are no labels in the data to supervise the learning. The algorithm will instead have to figure out how to group the customers based on the similarity of the profiles or purchasing behaviors. When you serve data to this type of learning, the data will only contain features without any labels. 
So now that we've covered these three types of machine learning systems, let's take a look at the phases of the machine learning project lifecycle framework that Andrew proposed. You'll find this framework helpful for many machine learning projects, and we'll be focusing on where you can provide value as a data engineer. The first phase is scoping, where the machine learning engineer, or data scientist, has to define the project and decide exactly what business problems they want to apply machine learning to. Then the second phase is all about data. The machine learning engineer will work with you to determine what features and labels you need to collect for training the machine learning algorithms. This is the phase that you will be most involved with as a data engineer and includes defining the data and establishing a baseline, and also labeling and organizing the data. The third phase involves selecting and training the algorithm and performing error analysis. 
The machine learning engineer takes the data, you serve them, maybe processes it further, then uses it to train machine learning algorithms. They typically split the data into a training set and a test set. Then they use the training set data to train several machine learning algorithms with different configurations. They can choose from classical machine learning algorithms such as linear regression, logistic regression, decision trees, random forest, and boosted trees. These classical algorithms are generally easy to implement and train, and they expect data to be in a tabular form. However, as the size of the data increases, the performance of these classical machine learning algorithms will reach a plateau. This is where more complex machine learning algorithms, such as deep neural networks, can help. 
These complex algorithms not only work with tabular data, but they can also work with image data using convolutional neural networks, time series data using recurrent neural networks, and text data using large language models. After the machine learning team train several machine learning algorithms, they select the best one in a process called cross-validation. Finally, they evaluate the selected model using the data from the test set to analyze the model's performance. They might discover that they need to fix something in the collected data, add more features, or simply collect more data. So the team might ask you to serve an updated set of data to continue with the algorithm development phase. This iterative process can continue until the team is happy with the performance of the machine learning system. The final phase is deployment, before the machine learning engineer takes the system to deployment, they'll carry out a final check to make sure that the system's performance is good enough and that it's sufficiently reliable for the application. 
Then they will write the software to put the system into production, monitor the system, track the data that continues to come in, and maintain the system. During the deployment phase, you might be responsible for preparing and serving the data that needs to be fed to the deployed model. You might also help with maintaining the machine learning system by serving an updated set of data that's used to retrain and update the model as needed. So in the machine learning project lifecycle, you might not be involved with the details of the algorithm development and analysis, but you'll be responsible for setting up the pipeline to serve data that supports these phases. In the next video, we'll start with the classical machine learning algorithms, focusing on how you can prepare data that meets their data structure requirements. I'll see you there. 


#### Modeling Data for Traditional ML Algorithms
When you serve data for training classical machine learning algorithms, typically, the data is expected to be in a tabular form containing only numerical values. When you model the data for these use cases, you need to decide what features to include and what label to use. The decisions are usually made by the machine learning or data science team. Depending on the project and the level of iteration, you might just serve raw data to the team if they are interested in exploring the data, or you might need to process the data and convert the features and the labels into a numerical tabular form before serving it to them. Let's go through some basic pre-processing steps to prepare tabular data for training. Back to the customer turn example that I introduced in the previous video, here's what the raw dataset might look like. Each row corresponds to a customer showing the number of purchases they've made, the date of their last purchase, the customer's income, the time they spent on the platform, their account type, and whether they have churned or not. 
Depending on the project and the team you're working with, you may be expected to serve raw data like this, or you might be asked to process the data and convert it into a numerical tabular form, which might look something like this. Notice that I've separated the churn column into a vector that contains the labels for each customer, where one means that the customer has churned and zero means that they have not. You can see there are no missing values or duplicate rows, and each column consists of numerical values that are within a similar range. Moreover, notice that I've created a new purchases per minute feature by dividing the number of items purchased column by the minutes on platform column. Combining or modifying existing columns to create new features is something that's typically decided by the machine learning team and communicated to you. This numerical tabular data is what most classical machine learning algorithms expect to receive as training data. In machine learning, when you process a raw column or create a new feature, that's called feature engineering. 
This process includes operations like handling missing values, feature scaling, converting categorical columns into numerical ones, and creating new columns by combining or modifying existing ones. Let's take a closer look at these common feature engineering operations. You'll likely encounter missing values when working with data. You should first understand why the values are missing then determine the most appropriate way to handle this issue. The simplest approach is to delete either the columns or the rows that contain the missing values. But you might unintentionally lose important data this way. Only delete rows or columns when there's no risk of losing valuable data. 
Another approach would be to impute the missing values with some summary statistics from the column, such as replacing the missing values with a mean or median of the column or with a value from a similar record. However, when you impute the missing values, you might introduce noise or bias to the data. In our customer turn example, the third row had mostly null values. Suppose I spoke with the machine learning engineer and determined that the non-null value that we have isn't particularly valuable. I decided to delete that row. For the missing customer income value, I decided to replace it with a value from a similar record. There's no single perfect way to handle missing values, and you'll usually work with the machine learning team to select the best approach that doesn't impact the performance of the machine learning system. 
After dealing with any missing values, you want to scale the numerical features so that the values of each feature end up within a similar range. Without going into the technical details, machine learning algorithms are essentially optimization algorithms that use training data to calculate a set of parameters that result in the most optimal outputs. If the feature values vary drastically, it might take a long time for the algorithms to converge. Moreover, certain machine learning algorithms are based on distance metrics, so their accuracies can be affected by the different ranges of the feature values. In our customer turn example, the range of values for the number of items purchased feature will be much smaller than the range for the customer income feature. To scale the values for each column, you can apply standardization or min-max scaling. With standardization, you take each value within a column, subtract the column mean, and then divide by the column standard deviation. 
The standardized values in the column will have a mean of zero and a variance of one. With min-max scaling, you take each value within a column, subtract the minimum column value, and then divide by the difference between the maximum and minimum column values. This way, the normalized values in the column will be between zero and one. Let's say the customer income feature has a minimum value of $0 and a maximum value of 100,000. What values would you get if you were to apply min-max scaling to these first two values in the customer income column? For the first value, you would get 50,000-0/100,000 , which is 0.5. Then, for the second value, you would get 40,000-0/100,000, which is 0.4. 
Now, what if your raw dataset contains non-numerical categorical values? For example, suppose that the account type can either be basic, family, or platinum, like you see here. Since classical machine learning algorithms expect each feature to be numerical, you need to apply a pre-processing step to convert this categorical feature into a numerical one. One way to transform this column into a numerical one is to apply a method called one hot encoding. With this method, you replace the account type column with three columns, the first one representing basic, the second representing family, and the third representing platinum. Since the first customer here has a family account type, I will denote the family column with a one and the other two columns with zero. You do the same for all of the other rows. 
The converted columns are easy to interpret, but if the number of unique values in a column is large, it can increase the number of columns in the dataset significantly. Another encoding approach is to use ordinal encoding, which is useful when there is a natural ordering between the unique values of the categorical column. Suppose that the account types can be ordered by their subscription fee with basic being the cheapest, family falling in the middle, and platinum being the most expensive account type. Then you can replace basic with a one, family with a two, and platinum with a three. This way, you can convert a categorical column into an numerical one without adding columns to your dataset. There's also other methods such as hashing where you apply a mathematical hash function to replace a category with a calculated hash value, or you can create embeddings, which is something I'll go into later this week. Again, you will work with the machine learning engineer to figure out what method to use depending on your use case. 
These are some of the pre-processing or feature engineering steps that you might apply to prepare data for training and machine learning algorithm. You should always work closely with the machine learning team to decide on the steps and methods that are most appropriate for the given project. Next up, I'll go over a short demo to show you how to apply these feature engineering steps to an actual dataset using Pandas. If you aren't already familiar with Pandas, you might want to check out the Pandas tutorial link and the resource section at the end of this week. Before we dive into that demo, I also included an optional video where I speak with West McKinney, the creator of Pandas. If you'd rather skip that video, I'll see you in the demo after that. 


#### Demo: Processing Tabular Data with Scikit-Learn (Part 1)
>> Let's apply some of the preprocessing steps you saw in the last video on an example data set. I'm going to use the open source machine learning library scikit learn. This library includes several modules that you can use to define and develop supervised and unsupervised machine learning systems. It also provides tools for preprocessing data. I've included a link to the scikit learn user guide in the resource section at the end of this week. And I encourage you to check it out to learn more about the various preprocessing tools available. In our example, I'll use two preprocessing methods, standardization for the numerical columns and one hot encoding for the categorical columns. 
For this demo, I have downloaded a customer churn dataset from the data science platform called Kaggle. If you'd like to follow along with me, you can find this notebook file and the dataset CSV file, under the resource section of this video. Let's quickly explore this dataset using pandas. I'll import pandas and then read the CSV file, that I sort locally into a panda's data frame called data. Checking the shape of the data, I see that it consists of around 440,000 rows and 11 columns. You can use the head method to examine the first few rows of this data. Each row corresponds to a customer and contains a set of features, including the customer's age, their usage frequency, the number of support calls they made to the service provider, their subscription type, and so on. 
And it also contains the column churn, which consists of the target label for each customer. A value of one indicates that the customer has churned, and the value of zero indicates that they have not churned. You can also use the describe method to quickly explore the summary statistics for the numerical columns. So you can see that the mean age of the customers is almost 40, the number of support calls range from 0 to 10, and the median total spend is $661. You can also see that the count for each feature is one less than the total number of rows, indicating that there's one null value in each of these numerical columns. You can verify this by using the isnull method. From these outputs, you can see that the table contains one entire row of missing entries. 
Since it's just one row and all the columns for this one customer are nulls, I'll drop it from the dataset, using the dropna method. You can verify that there are no null values anymore, in any of the columns. To quickly explore the categorical columns, which are subscription type and contract length, I'll use the value counts method on each column, which will return the unique categories and the percentage of rows that belong to each category. You can see that each categorical column has three unique values. Since the number of unique values is small, you can use one hot encoding to convert these categorical columns into numerical columns. But first, since most of the machine learning models expect the features and labels to be stored separately, I'll create a variable called features and assign it all the columns from our data set, that represents the customer features. Then I'll create a variable called labels and assign it the single column that represents the labels. 
You can take a look at the first few rows of the features and labels to make sure they're separated properly. Now assume you met with the machine learning team and you're asked to prepare this data, for training a machine learning model. For that you're asked to split the data into 80% for training, and 20% for testing. In each set you'll need to preserve the customer id standardized numerical columns that consists of age, tenure, usage frequency, support calls, payment delay, total spend and less interaction, and perform one hot encoding on the categorical columns. The training and test data sets, should be stored in tabular format as parquet files. Given these requirements, here are the steps I'll follow to prepare the data for training. 1.First I'll split the data into training and testing sets. 
2.Then I'll focus on processing the training data first, I'll extract the numerical columns of the training set and then standardize each numerical column. Ill extract the categorical columns of the training set and encode them using one hot encoding. Then ill combine the process numerical columns and the encoded categorical columns, with a Customer ID into a Pandas data frame. Before converting this pandas data frame into a parquet file, concatenating the process columns into a Pandas data frame, brings together the metadata, meaning the row and column labels and the values into a single object. This makes it easier to store the data as a parquet file. 3.After that, I'll repeat the same processing steps on the test set. 
When evaluating or testing a machine learning system, you should apply the same preprocessing steps, with the same computed statistics used on the training set to the test set. This is a good practice because, the test set is used to evaluate the machine learning algorithm on data that it hasn't seen before. So to transform the test set, you should use any statistics used to transform the training set with. So with this plan in mind, let's move on to the next video to see how to carry out these steps in scikit learn. 


#### Demo: Processing Tabular Data with Scikit-Learn (Part 2)
>> Here's the plan you saw in the last video to prepare tabular data for training a classical machine learning algorithm. Let's start with splitting the data into training and test sets using SciKit -Learn's train test split method, which I'll import here. I then need to specify the features, labels and test size. The random state parameter ensures that you can reproduce the splitting when you run this code again at a later time, so I'll just set it to 42, but you can use any integer you want. This method returns the features denoted by X and the labels denoted by Y for the train and test sets. Now that we have our training and test sets, let's focus on preprocessing the training dataset first. Then I'll apply these same steps at the end of this video to the test set. 
Starting with the numerical columns, since I want to scale these features first from the SciiKit-Learn pre processing module, I'll import the standard scalar class. I'll create a list called numerical columns to specify the age, tenure, usage, frequency, support calls, payment delay, total spend, and last interaction columns. Then I'll create a dataframe called x_train_numerical to extract just the numerical columns from the training features dataset. In order to use the functionalities of the standard scalar class, I'll instantiate a standard scalar object. With this object I can call two methods fit and transform. The fit method computes the mean and standard deviation of each numerical column, and the transform method uses the computer statistics to scale the values of each column. So, I'll call the fit method on the numerical dataframe And then ill call the transform method on the same dataframe. 
The transform method returns the scaled columns as a numpy array. Since I'll need to concatenate the process numerical and categorical columns into a pandas dataframe, I'll first convert this numpy array into a pandas data frame so it'll be easier to create the final pandas data frame later on. So using the pandas dataframe constructor, I'll pass to it the scaled features, the index of each training data frame to ensure each row in the output dataframe maps to the correct row of the original training set. And finally I'll pass to it the names of the numerical columns. So now you have a data frame containing the scaled numerical columns. Here are the first five rows of this data frame. As you can see, all the features have been scaled to lie within a similar range. 
Let's set this aside for now and move on to the categorical columns. Since I want to convert these columns into numerical columns using OneHotEncoding, from the sklearn preprocessing module, I'll import the OneHotEncoder class, similar to what we did before, I'll create a categorical columns list to specify the subscription type and contract length categorical columns. Then I'll create a data frame called x_ train _categorical. To extract the categorical columns from the training features data set. To use the OneHotEncoder class on this categorical data frame, I'll first instantiate it. With this encoder, you can also call the fit and transform methods. The fit method, will check the unique values within each categorical column to know how many columns need to be created for each feature. 
And it'll prepare the labels of the output columns. So I'll call the fit method of the encoder on the categorical data frame. And finally we'll call the transform method of the encoder on the same categorical data frame. The transform method returns the encoded columns as a compressed matrix. So if you try to get its type you will see that it is a sparse matrix stored in a compressed sparse row or CSR format. A sparse matrix contains a lot of zeros which is what happens when you use OneHotEncoding. So instead of storing the entire matrix, only the locations that contain a one are stored. 
You can see this information when you print this compressed matrix. To convert this matrix into a regular matrix, you can use the two dents method. So these are the encoded columns for each category of the original categorical columns. To know what each column represents, you can use the encoder method called get features names out. So you can see that the first three columns represent subscription type and the last three columns represent the contract length type. Now let's convert this matrix into a data frame that represents the encoded categorical columns. Again I'll use the pandas DataFrame constructor. 
This constructor expects a regular matrix. So first I'll convert the encoded outputs from the compressed matrix to a regular matrix using the too dense method you saw earlier. Then I'll pass it the index of the training dataframe. And for the name of the categorical columns, I'll use the encoder method, get features names out. So that creates a data frame with the encoded categorical columns. Here are the first five rows of this data frame. You can see that these categorical features have all been converted into numerical values. 
Now I can use the pandas concat method to concatenate the customer Id, the scaled numerical features and the encoded categorical features into one data frame. The parameter access equals one specifies that it's a horizontal concatenation. Here are the first five rows of the data frame that represents the process training data. Finally, ill apply these same steps for scaling the numerical features and encoding the categorical features on the test set. So first ill scale the numerical columns and then transform the scaled columns into a pandas dataframe. Then ill encode the categorical columns and then transform the encoded columns into a pandas DataFrame. Finally, ill concatenate the customer id column with the process numerical and encoded categorical columns. 
Its important to note that I did not refit the scalar and the encoder on the test set. I simply use the same scalar and encoder that I previously fit with the training set. That's because you want to ensure the same scaling is applied to the numerical features, meaning the same mean and standard deviation are used to scale each column in the test set. And finally, to save the training and test features as parquet files, I'll call the method to parquet as shown here and there you have it. We just applied scaling on all the numerical columns of the training and test datasets and applied OneHotEncoding on the categorical columns for both training and test sets. You would then pass these datasets to the machine learning engineer or data scientist to train and evaluate the various machine learning algorithms. For the first lab of this week, you'll revisit the lab from course one, but this time you'll use sklearn to process the data used to train the recommender system. 
After that, join me in the next video to discuss how you can process images before using them to train machine learning algorithms. 


#### Modeling Image Data for ML Algorithms
Aside from predicting sales and segmenting customers based on tabular data, you can also use machine learning algorithms to identify patterns and images. Some applications of this include image classification like identifying the type of plant from an object, object detection, such as recognizing a pedestrian in a photo of an intersection, and pixel segmentation, such as detecting cancerous regions in an x-ray image. When in preparing images to be trained by a machine learning algorithm, what you'll do will depend on the type of algorithm being used. Let's take a look at these different cases. As we discussed in the previous lesson, traditional machine learning algorithms or even regular neural networks expect data to be in tabular form. Consider, for example, the set of gray scale images with these pixel representations of each image. Let's say the machine learning team wants to use a set of images to train a traditional machine learning algorithm. 
Then you would need to flatten each image into a long vector of pixel values and then concatenate the vectors from all images into a tabular form like this that can be used by the machine learning algorithm. Each row in this table corresponds to the flatten pixel values of one image. However, this approach has limitations. When you flatten an image, you ignore the spatial structure of the image, and with that, you lose a special information that can be extracted from how one pixel is located with respect to other pixels. Moreover, this approach can create a high dimensional vector of features for each image. For instance, if each image is of Size, 1,000 pixels by 1,000 pixels, then by flattening each image, you obtain a vector size one million, which would require a lot of compute and memory resources if trained on a regular neural network. Without going into too much of the technical details, this can also affect the performance of the machine learning algorithm, especially if the dataset contains way fewer than one million images. 
I include the link to Andrew's deep learning specialization in the resource section, so feel free to check that out for more details. The alternative approach is a train and more advanced deep learning algorithm known as a convolutional neural network or CNN on these images. A convolutional neural network can work directly on images without having to first flatten them. These types of networks consist of several layers where each layer tries to identify more and more features in an image that can help with the machine learning task. The first layer learns generic features that could be applied to any images, things like lines, horizontal and vertical edges, and simple textures. Later layers learn features such as more complex patterns and textures, features that are more specific to the task at hand. This is why you'll find that many machine learning teams use pre trained convolutional neural network algorithms that were trained on a large set of images, and then fine tune these models for their specific tasks by training the deeper layers using their own specific set of images. 
You might be responsible for providing the machine learning team with the set of specific images that they'll use to fine tune a convolutional neural network or even to train these networks from scratch. In any case, you would still need to prepare the images for the training algorithm. For instance, you would need to resize the images into a shape that is expected by the neural network algorithm, or you would need to scale the values of the pixels so that they are in the range of values expected by the algorithm. Another pre processing technique that you might apply is data augmentation. Augmenting images is a technique that you can use to create new versions of existing images by applying geometric transformations, such as flipping and rotating or other techniques, such as cropping or adjusting the brightness of the images. Data augmentation can help increase the size and variety of the training data, which in general, can help with the performance of the machine learning algorithm. You can apply these pre processing steps using open source tools like TensorFlow, which is a framework that's used to build and deploy deep learning models. 
TensorFlow provides pre processing functions that you can apply directly to your images. I included an optional code example in the reading item after this video that shows how you can use TensorFlow to resize, scale, and augment a set of images. That was a brief overview of the techniques you can use to pre process images. Aside from pre processing image files, you might also need to pre process a PDF file that contains a scan document that you need to extract textual data or tables from. Also techniques that can help you with this task. For more information, I included a link to a short course on pre processing unstructured data for large language model applications that shows you examples of these techniques. In the next video, we'll continue discussing how you can pre process unstructured data for machine learning algorithms, but now we'll focus on textual data. 


#### Preprocessing Texts for Analysis and Text Classification
Interactive Transcript - Enable basic transcript mode by pressing the escape key
You may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key
As organizations increasingly generate and collect large amounts of text, including product reviews, social media posts, and customer support interactions, it's essential for them to extract insights from this textual data to make critical business decisions. As a data engineer, you could be tasked with pre-processing and serving textual data to machine learning engineers so you can analyze the data and use the train machine learning systems for various applications, such as sentiment analysis of product reviews, classification of news articles, chatbots, and virtual assistants, spam detection, customer segmentation, product recommendation, and much more. Natural language processing or NLP, is a subfield of AI that enables computers to process, understand, and generate human texts. NLP is an old field that has existed for more than 50 years, and it encompasses various text analysis techniques that have evolved over time. While classical machine learning algorithms are effective for NLP tasks like text classification, and sentiment analysis, the latest development in NLP, known as Large Language Models or LLMs, have significantly enhanced computers capabilities in interpreting and generating text with remarkable accuracy, and fluency. In this video, we'll look at some text pre-processing techniques you can use to prepare textual data for training machine learning systems for NLP tasks. Let's say your company wants to perform sentiment analysis to analyze customer reviews to determine if they are positive or negative. 
This technique typically relies on using a pre-trained machine learning system or on training a machine learning system on your customer reviews. As a data engineer, in addition to collecting and storing these reviews. You can also help transform this textual data into a format that the machine learning algorithms can understand. The degree of pre-processing that you need to apply will depend on the type of machine-learning algorithm used. Classical machine learning algorithms cannot work directly with sentences. In this case, you need to apply pre-processing techniques to first clean the text and then vectorize it. On the other hand, LLMs can work directly with a sequence of tokens or words from your sentences without you having to first vectorize the sentences into numerical form. 
In any case, I think it's important that you are familiar with the various strategies for pre-processing and preparing textual data for further analysis. This is because textual data might contain typos, inconsistencies, and repetitions that you need to address in order to provide clean and high-quality data to train a machine learning model and ensure its performance, regardless of whether the machine learning algorithm is classical or advanced, like an LLM. Moreover, not all words or characters that are in your textual data might be relevant for the given NLP task. In order to reduce processing and storage costs, you would need to remove any words or characters that do not carry any relevant information for your use case. Finally, it's true that LLM is currently excelling at many NLP tasks, but training such an algorithm is still expensive and time-consuming, so they might not be the best solution for all use cases. The machine learning team you work with might still prefer using classical machine learning algorithms where simpler solutions are enough to address the use case requirements. There are also use cases where you would need to combine numerical, categorical, and textual features in a train dataset. 
For all these reasons, let's take a look at some common pre-processing techniques for prepping textual data. To clean your text, you can start by removing punctuations, extra spaces, or any character that doesn't add meaning to the text. For example, here are three original text reviews, and here is what you want to end up with after cleaning the data. I've included the Python code used to clean the data in the reading item that follows this video. Feel free to look at the explanation of that code if you're unfamiliar with working with strings. After cleaning the data, you can apply normalization to standardize a text by converting it into a consistent format. This might include transforming the characters to lowercase, converting numbers or symbols to characters, or expanding contractions or acronyms to reduce variations in spelling use for the same words. 
For instance, looking at our three example reviews, the customer in the second review wrote amount as AMT, and you could have another review that maybe capitalizes the A in amount. When you apply normalization, you resolve these inconsistencies by replacing all these instances with the lowercase spelling of the word amount. In the third review, you also see the contraction don't, which you can replace with do not. Other examples of text normalization include converting common units like Kg to kilograms or LBS to pounds or replacing acronyms like DE/D.E with data engineering. Again, you can take a look at the Python code that performs this normalization and the reading item that follows. After you normalize the data, you should end up with text that looks like this. To prepare the text for further analysis, you then apply tokenization for each text review, meaning you split each review into individual units or tokens, which are typically words but can be any meaningful unit of text, such as subwords or short sentences. 
For simplicity, I'll just split each review into a vector of words. This can be done with the Python string methods split that you'll see in the reading item. Next, you can remove frequently used words such as "is", "are", "the", "for, "a" that usually don't add any meaning to the data. These words are referred to as stop words. You can define your own list of stop words or use NLP libraries like SpaCy, NLTK, Gensim, and TextBlob, among others that come with a built-in set of stop words. Here's what the example reviews look like after I remove the stop words that belong to the set. Finally, you can replace each word with its base form, known as its lemma, using a technique called lemmatization. 
For example, the base form of getting and got is get. When you apply lemmatization to text, you replace all these variations with its lemma, which is the word get. Again, you can use NLP libraries to obtain the lemma of each word. This is what I got after applying lemmatization to all three reviews. Depending on the needs of the machine learning engineers or data scientists, you might not need to apply all of these steps, or you might need to apply other pre-processing steps. This depends on the models your end users are planing to use and how much processing they want to perform themselves. I included links in the resource section to some additional courses that talk about other text pre-processing steps. 
Feel free to check them out to learn more about preparing textual data for training and LLM or other algorithms. In any case, after you perform the necessary steps to clean and pre-process the data, the next thing you might have to do, especially when working with machine learning algorithms that expect numerical data and tabular form is to vectorize the data. In the next video, I'll cover some common techniques for vectorization, where you convert cleaned and tokenized text into a vector of numbers. 


#### Text Vectorization and Embedding
Now that we've covered the common preprocessing steps for textual data in the previous video, let's go over how you would turn your preprocessed text data into vectors. The traditional techniques to vectorize text consists of methods like bag of words and term-frequency inverse document frequency, or TF-IDF. These methods assign a number to each word in a text based on its frequency of occurrence. Here's the example you saw in the previous video. I'll refer to each customer review as a document, and the three reviews collectively make up what is known as the corpus. In a real example, the corpus would include all the customer reviews and will be much larger. Let's first extract the vocabulary of the corpus, meaning the list of unique words that make up this collection. 
Then you can convert each document, meaning each customer review, into a vector that has the same length as this list of vocabulary in the bag of words method. Each entry in the vector reflects the number of times that corresponding word appeared in the document. So in the first review, the words this wonderful price amount you and get all appeared once, and all the other words in the vocabulary appeared zero times. You can use scikit learn to quickly vectorize your text using the bag of words method. Feel free to check out the specific code in the reading item that follows this video. However, the bag of words method only takes into account how frequently each word in the vocabulary appears within each document. In a large text corpus, some words might appear very frequently across all documents, but carry very little meaningful information about the actual contents of the document. 
For example, if you're looking at a large dataset of product reviews, the words purchase and buy might appear many times across many documents. If you were to feed this frequency count data directly to a document classifier that performs sentiment analysis on the reviews. These high frequency but low meaning words might overshadow the frequencies of more rare but more significant words like break or exceptional. With the TF-IDF method, you can account for the weight and rarity of each word or term when vectorizing the documents. For each word, you will consider its term frequency, which represents the number of times a term occurred in a document divided by the total number of words in that document. And you will also consider the inverse document frequency, which indicates how common or rare that word is in the entire corpus. The closer this number is to zero, the more common it is, and the closer it is to one, the more rare it is. 
In practice, you would use scikit learn or another library to help you vectorize your text using the TF IDF method. So I won't go into details of the calculation here, but if you're curious about these details, feel free to check out the reading item after this video. So here's the TF-IDF vectorization of these text reviews the traditional way of vectorizing a text is considered a simple, easy to understand. And interpretable method of vectorizing texts, and you might choose to use these methods for quick experimentation. These simple vectorization methods are useful when you have a smaller data set, and they can perform very well on document classification. Where the presence of specific key terms strongly indicate the class of the document. However, it gives no attention to the meaning of the words or the sentences in the document, and you can end up with a high dimensional vector with very sparse values. 
If you want to better capture the semantic meaning of a word, you can replace the word by a vector called a word embedding. These vectors are generated in such a way that if two words have similar meaning, they're mapped to vectors that are geometrically closer to each other. So, for example, the embedding vector of the word useful should be closer to the embedding vector of the word helpful than that of the word tree. You can generate embedding vectors using popular word embedding algorithms such as word2vec and glove. Which have been trained to learn the embeddings of words from their co-occurrences and very large collections of text. The idea is that if two words frequently appear together, they should share similar contexts and those have similar meaning. Let's go back to our text reviews, here you want to represent each review, and not just each word, by one vector. 
You could just compute the word embeddings for each word in the sentence, then add a bullet of vectors to get a vector that represents the sentence. But this method doesn't take into account the position of the words in the sentence. For example, a man ate a snake has a different meaning than a snake ate a man. But if you add the word embeddings of each word in these two sentences, you'll get the same vector. This is where sentence embeddings can be helpful. A sentence embedding is a vector that reflects the semantic meaning of the entire sentence. It takes into account the position of the words in a sentence and the meaning of each word. 
Similar to word embeddings. If two sentences have similar meaning, their embedding vectors would be close to each other. Moreover, a sentence embedding is a vector that has a lower dimension than the dimension of the vector generated by a TF-IDF. You can obtain these sentence embeddings using NLP models that are pre trained on large datasets. Several open source and closed source embedding models exist, but these state of the art models are based on large language models. For example, sentence transformers is a Python framework that enables you to access open source embedding models. On the other hand, LLM research companies such as OpenAI, Anthropic, and Google offer closed source API embeddings. 
Let's apply sentence embeddings to the text reviews. Here, I'm using the sentence transformers Python package. First, I need to instantiate an instance of the sentence transformer and specify an open source LLM model. In the documentation page you can find a list of open source models that you can use with sentence transformer. I'm going to use a model called all-MiniLM-L6- v2, which you'll see again in the lab exercise later on. Then I'll take the text reviews that I've cleaned and normalized and pass it to the embedding model. For each review, the model returns an embedding vector that has 384 entries. 
Let's see how similar these vectors are to each other. I'm choosing to use the cosine similarity metric here, but you can choose other distance metrics as well. The cosine similarity between the first review and the second review is 0.51, and the cosine similarity between the first review and the third review is 0.14. Meaning that the first review is closer in meaning to the second review than it is to the third. This makes sense since both the first and second reviews are about the amount of product the customer received. Now, what can you do with these embeddings? These embeddings can serve as features to train a machine learning algorithm for product recommendation, or they can be used for clustering or performing similarity search. 
In the second lab of this week, you'll be provided with the text review data set you saw in the previous course. You'll apply embeddings to this dataset and process the tabular metadata using similar techniques that you used in the first lab, and after that join me for a summary of this week. 


#### Summary
Interactive Transcript - Enable basic transcript mode by pressing the escape key
You may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key
>> This week, we looked at approaches for modeling and pre processing tabular data, images and text to prepare them for machine learning use cases. You've seen how a machine learning project lifecycle looks like and how you can help the data science or machine learning team by serving the data they need throughout the stages of the project. Many machine learning algorithms expect data to be in numerical tabular form. However, real data doesn't always exist in this form. It can have missing values or contain categorical columns. To handle missing values, you can apply data imputation, or in some cases, you can simply delete the empty records or columns to convert categorical columns to numerical ones. You can use encoding techniques such as one hot encoding, ordinal encoding, or other methods. 
We also discussed the importance of scaling the numerical features in a training dataset to help the machine learning algorithm converge faster. In the first lab of this week, he implemented these processing steps and more using the learn library. When it comes to image data, you need to unroll the images in a long sequence of pixels if you're working with classical machine learning algorithms. More advanced neural network techniques such as convolutional neural networks can directly process images and learn properties about the images in each layer. While you don't need to unroll images when working with convolutional neural networks, you might still need to apply preprocessing techniques to the raw images, such as image reshaping and normalization. Or you might be asked to help augment the dataset of images by applying techniques such as flipping, rotation, or adding distortions. Finally, when it comes to textual data, you learned how to preprocess text. 
If the end use case involves more advanced NLP models, then you might just serve the raw data or the clean text data to the machine learning engineer or data scientist. However, if the end use case involves training classical machine learning algorithms, or you want to reduce processing and storage costs by pre processing the text before feeding it into an LLM, then you would need to perform extra pre processing steps to convert the text into numerical vectors. You learned about two traditional vectorization techniques, bag of words and TF-IDF, and you also saw a more advanced technique, sentence embedding. In the second lab this week, you applied sentence embedding to the Amazon review dataset and combined it with other product features. When it comes to building machine learning systems, developing a basic understanding of how machine learning works will help you serve quality data to your stakeholders and provide value to your organization. And with that, we've covered modeling techniques for analytical use cases and processing techniques for machine learning use cases. Next week, we'll discuss more transformation techniques and look at transformation frameworks such as Spark. 
I'll see you there. 

## Data Modeling and Transformations for Analytics


#### Overview
In Courses 2 and 3, you learned how to ingest data into your data pipeline and explore various storage solutions for hosting your data. Now, before you serve the data to your end users, you need to transform and model it in a form that supports its intended use cases. In this course, we'll start with data modeling and then look more closely at transformation in the second half of this course. Modeling your data involves deliberately choosing a coherent data structure that aligns with the business goals and logic. What is a data model? Here's my definition. A data model organizes and standardizes data into a precise structured representation to enable and guide human and machine behavior, inform decision making and facilitate actions. 
The first part of the definition implies that when you model your data, you define the structure, relationships, and meaning of the data. For example, when you model tabular data, you need to think about the tables that make up the model, how to label the tables, how the tables relate to one another, and what columns to choose for each table. You should structure the data in a way that connects back to the organization and this is what the second part of the definition implies. For the data to serve its purpose, you'll need to make the data understandable and valuable to humans, if you're modeling the data for analytical use cases such as creating reports or dashboards. If the data is going to be used for machine learning use cases, you need to model the data into a form that is meaningful to a computer. A well constructed data model should reflect the business goals and logic while incorporating business rules like requiring invalid payment method before processing in order to ensure compliance with operational standards and legal requirements. A good data model should also outline the relationships between business processes. 
For instance, linking sales data with product inventory data to ensure that the sales process is directly informed by current inventory levels, preventing overselling. Beyond that, a robust data model serves as a powerful communication tool, creating a shared language among stakeholders like engineers, analysts, and executives, by standardizing business vocabulary such as clearly defining what constitutes an active user, is that someone who has logged into their account in the last 30 days, someone who has made a purchase in the previous six months, or something else entirely. Carefully defining business terms can have a massive impact on downstream reports that describe customer behavior and things like projecting customer churn. To ensure successful data modeling, recall the framework for thinking like a data engineer from Course 1 and always start with talking to your stakeholders. Understanding the business definitions, rules, and goals is the first step to modeling the data and providing the business with quality data for actionable insights and intelligent automation. On the other hand, poor data models that are created haphazardly and don't reflect how the business operates can create more problems than they solve. Instead of promoting communication and shared understanding, poor data models might provide stakeholders with inaccurate information and create confusion. 
Another professional oversight that I often see is when data teams ignore data modeling entirely because they see it as a slow, tedious, and irrelevant process that's only reserved for big companies. They jump directly into building data systems without a plan for how they will organize their data to make it useful for the business. Well, that is a huge mistake. Data modeling has been a practice for decades and was traditionally used to structure data stored in data warehouses and relational databases. With the rise of Data Lake 1.0, NoSQL and big data systems, engineers started ignoring traditional data modeling, sometimes for legitimate performance gains. However, the lack of rigorous data modeling created data swamps, along with lots of redundant, mismatched, or simply inaccurate data. Nowadays, the growing popularity of data management, in particular, data governance and data quality is pushing the need for coherent business logic. 
I see data modeling as a critical practice that enhances your understanding of the data throughout its life cycle. As a data engineer, data modeling helps you improve data quality and integration and encourages the adoption of data throughout the organization. No matter the size of the business you're a part of, you should take a targeted approach to data modeling by focusing on specific business domains. For instance, you could create a data model to help the marketing team better understand customer behavior and campaign effectiveness. Or you can model the company's financial transactions so that the finance team can analyze spending patterns and identify cost saving opportunities. Targeted data modeling efforts can provide valuable insights to drive better decision making and impactful AI models, even within highly complex businesses. In the first week of this course, I'll mainly focus on discussing batch data modeling since that's where most data modeling techniques come from. 
We'll start by taking a quick look at the three traditional levels of data modeling; conceptual, logical, and physical, each differing in the degree of detail they provide. Then we'll go over two basic schemas, the normalized schema and the star schema, which you'll implement in the labs this week. Finally, we'll dive into some popular modeling techniques for analytical use cases such as the Inmon and Kimball modeling approaches, as well as other techniques like data vault and one big table. In Week 2 of this course, we'll discuss data modeling and transformation techniques for machine learning use cases. Then in Week 3, we'll dive deeper into transformations and discuss the various technical considerations for choosing data processing frameworks. Finally, we'll bring everything you learned in this program together in the fourth week of this course where you'll get a chance to build an end to end data pipeline that encompass all the stages of the data entering life cycle, as well as the key undercurrents. Join me in the next video to take a look at the different levels of data modeling. 


#### Conceptual, Logical and Physical Data Modeling
As a data engineer, you can add value by building and maintaining a data model that promotes communication and shared understanding across your organization. But where should you start? Well, my suggestion is to start with a high level conceptual data model that describes the business entities. Then you can fill out more of the details to create a logical data model. Finally, you'll create what's known as a physical data model, where you'll decide on the database or other storage systems that you'll use to store and serve the data, as well as outline the implementation details, meaning the actual tools you'll use to implement the storage systems in your data pipeline. Zooming in on the conceptual model. It should focus on the high level business entities, the relationships between them and the attributes of each entity. 
It should also reflect the business logic and rules. For example, for tabular data, this description could include the tables, relationships between the tables, and column names. When you create a conceptual model, you can visualize it with an entity relationship or ER diagram, which is a standard tool for visualizing the relationships among different aspects of your data, like customers, products or events. Here's part of the ER diagram of the classic models data set you worked within Course 1. As you can see, it contains data about products and order details for each order. It encodes a connection between product data and order details using these symbols. This symbol stands for one and only one, meaning that each order details record can be associated with one and only one product. 
It's common to call this relationship from order details to product a one to one relationship. This symbol over here stands for zero or many, meaning that a product can be associated with zero order details if no one purchased that product, or it can be associated with many order details if it was purchased many times. This relationship from products to order details is a zero or one to many relationship, or more commonly known as simply a one to many relationship, meaning that one product can be associated with many order details. Notice that the relationship changes depending on the direction you take when viewing the relationship. This ER diagram also shows that the relationship from order details to orders is one to one, meaning that one order details can only be associated with one order. The relationship from orders to order details is one to many, meaning that one order can be associated with many order details. For example, if a customer brought a bunch of products within the same order, then there would be many order details, one for each product purchased. 
Each of these order details would be associated with only one order, but the order would be associated with many order details. The next level after the conceptual model is the logical model, where you add more details about how you'll implement the conceptual model. For example, you would add information on the types of columns for each table and map out the primary and foreign keys. Then finally, you create the physical model where you choose the specific DBMS and define how you'll implement the logical model in that system. This physical model should define the configuration details describing how data is stored, such as on disc or RAM or using a hybrid approach, and how processes like partitioning and replication are implemented. When you model your data, you'll move along this continuum from abstract modeling concepts to concrete implementation. Next up, let's sit into the details of data modeling, starting with an in depth look at normalization. 


#### Normalization
Back in course 2 when we discussed relational databases as source systems, you learned how normalization can reduce the duplication of data and improve data integrity. In this video, we'll revisit this topic and discuss the various forms of normalization. So normalization is a data modeling practice that's typically applied to relational databases to remove the redundancy of data within the database and ensure referential integrity between data tables. It was first introduced by relational database pioneer Edgar Codd in 1970, and here are some of the normalization objectives that Codd outlined at that time. To free the collection of relations from undesirable insertion, update, and deletion dependencies. As well as to reduce the need for restructuring the collection of relations as new types of data are introduced. And thus increase the lifespan of application programs. 
To better understand Codd's normalization objectives, let's take a look at this example where our sales order data is represented in two different ways. The first model represents the data in one giant sales order table, and the second model represents the same data but spreads it across multiple tables. The first model is less normalized than the second one, meaning that the large sales order table contains more redundant data than the smaller tables in the second model. So, for example, if you want to update the address of a customer, let's say for Joe Reis here, you would need to update every single row that corresponds to Joe Reis in this first table. In the second model, on the other hand, the customer data is stored in a different table. So whenever you want to update the address of a customer, you only need to change a single row in the customers table. Now, let's say you want to add the orders shipment information. 
In the less normalized model, you would need to change the structure of the table by adding new columns for the shipment information. But in the more normalized model, you can simply create a new table for the shipment data, then use the order id as the foreign key to link this new table through the existing orders table. And this way you won't have to make changes to any other tables. The first model that is less normalized is called the first normal form, and the more normalized model here is called the third normal form. On the spectrum of normalization, there's also something called the denormalized form and second normal form as well. Each form contains a different level of redundancy and incorporates the conditions of prior forms. Using the same sales order example, let's start with a denormalized form and transform it step by step to turn it into third normal form. 
So here's a denormalized table that contains the details of each order placed by a customer. It contains six columns with the order Id as the primary key. A denormalized form can contain not only redundant data but also nested data. In this example, the order items column contains nested objects where each object contains information such as the items sku, number, price, quantity, and name. To convert this denormalized table to first normal form, which I'll denote here as 1NF, you need to make sure that each column is unique and has a single value, meaning no nested data. And the table must have a unique primary key. So let's unnest the order items column and replace it with four columns. 
In this case, the item sku, price, quantity, and name are the new columns. Now, each row represents an item in a given order, and because an order can contain several items, the order Id is no longer a unique primary key for this table. To create a unique primary key, let's number the items in each order by adding a column named itemnumber. So now the composite key consisting of orderid and itemnumber together represents a unique primary key for this table. But this form still contains redundant data and can be further normalized by converting it into the second normal form, which I'll denote here as two nf. For the second normal form, the requirements of first normal form must be met and any partial dependencies should be removed. A partial dependency occurs when there is a subset of non key columns that depend on some columns in the composite key. 
So, for example, the customer Id, customer name, customer address, and order date are non key columns that all depend on the order id. Meaning that if you know the order id, you can uniquely identify the information in these last four columns. And so you can split the sales order table into two tables, an order items table and an orders table. The composite key consisting of order id and the item number is now a unique primary key for the order items table, while order id is a primary key for the orders table. So now there are no more partial dependencies in these tables, but they have another form of dependency called a transitive dependency. A transitive dependency occurs when a non key column depends on another non key column. So, for example, in the order items table, the price and the name of an item depends on its SKU, and in the orders table, the customer name and the address depend on the customer id. 
While this type of dependency can exist in a table thats in second normal form. A table in third normal form needs to meet all the requirements of second normal form and have no transitive dependencies. So to convert these tables from second normal form to third normal form, you can remove the transitive dependencies from the order items table by creating another table called Items. That contains a name, price, and SKU for each item. The SKU is now a unique primary key for the items table. Let's also remove the transitive dependencies from the orders table by creating another table containing the customer name and customer address of each customer. The customer id is now a unique primary key for the customers table. 
A database is usually considered normalized if it's in third normal form, and that's what we'll be using as a convention in this course. As a data engineer, you might ingest data from source databases that are normalized, especially if they represent a transactional system. Or you might work with a data warehouse that contains normalized data. When you model your data, the degree of normalization that you should apply to your data depends on your use case. There's no one size fits all solution, and you might encounter cases where denormalization actually has performance advantages. Because it doesn't require you to perform any join operations between tables. In other cases, you might prefer the normalized form to ensure efficient read and write operations and better data integrity. 
In the first lab this week, you'll get a chance to practice creating a normalized data model similar to the exercise we just did together in this video. You'll be given a denormalized table and have to apply several normalization steps to convert it to third normal form. I'll see you after that to continue our discussion on data modeling. 


#### Dimensional Modeling - Star Schema
While normalized models focus on connecting data entities and modeling the relationships to reduce data redundancy, the star schema, also known as a dimensional data model, focuses on structuring the data in a way that facilitates faster analytical queries and delivers data that is more understandable to business users. A star schema collects business measures in a table called the fact table and surrounds this table with the necessary contextual information stored in dimension tables in a way that resembles a star-like structure, hence the name star schema. Let's zoom in on the fact and dimension tables to see how they can better support analytical queries. The fact table contains quantitative business measurements that result from a business event or process. For example, when you order a ride share, this event generates measures such as the trip duration, trip price, tip paid, trip delays, and so on. These business measures are what we call facts, so each row in the fact table corresponds to the facts of a particular business event. When designing a star schema model, you also need to decide on what's called the grain, meaning the level of detail you want to show in each row of the fact table. 
In the rideshare example, each row in the fact table can represent all the rides completed by all customers within a single day, all the rides of a single customer in a day or a single ride completed by a single customer, and so on. While various grain levels are possible, I suggest you go with what's called the atomic grain, which refers to the most detailed level at which data is captured by a given business process and so for the ride share example, ideally, each row of the fact table would correspond to one completed ride by a single customer. Since facts relate to events and you can't change an event after it's already happened, the data in the fact table is immutable. In other words, fact tables don't change and are append-only and so you'll find that most fact tables are typically narrow and long, meaning that they won't have a lot of columns, but can have a lot of rows that represent events. A fact table is always accompanied by dimension tables, which provide the reference data, attributes, and relational contexts for the events stored in the fact table. They describe the what, who, where, and when of each event in the fact table, and they often have many columns. In contrast to the narrow and long fact tables, dimension tables are typically wide and short, meaning that they will have lots of descriptive columns, but fewer rows. 
In the rideshare example, you can have dimension tables that contain information about the customers, drivers, and trip locations. In a star schema, you have the fact table in the center that contains the facts about the business event, surrounded by dimension tables that provide additional context, and in some cases, you may even connect a dimension table to more than one fact table from various star schemas. A dimension that is reused across multiple-star schemas is called a conformed dimension. The fact tables are connected to the dimension tables through foreign keys. Each dimension is defined by a primary key, and the fact table has its own primary key, which could be the natural primary key from the production tables but the best practice is to create a substitute for the natural key, also known as the surrogate key. This way, you can combine data from different source systems that have natural primary keys written in different formats, and you can decouple the primary keys of your star schema from the primary keys of the source database, which can be subject to change. Here's an example of a star scheme you worked with in the first lab of course 1. 
Each row in the fact table corresponds to a product placed within an order and contains business measures, such as the quantity ordered, price each, by price, and so on. This fact table is connected to three dimension tables that provide further features related to customers, products, and the locations from which an order was placed. You can see that the fact table has a primary composite key consisting of the order number and the order line number, and the table contains three foreign keys to connect each of the three dimension tables. How exactly does the star schema help with analytical queries? Well, you can start with a fact table by applying aggregate queries to find, for example, the sum, average, or maximum value of a particular fact measure in the fact table. You can then use the dimension tables to filter or group the facts. For example, let's say you're interested in finding the total sales amount for each product line within the USA, so you need to sum up the order amount column in the fact table. 
To do this using SQL, you can select the sum of the order amount from the fact table, and let's call this sum of the total sales. Then you'll need to use the product line column from the products dimension table to group the sales, so you can join the fact table with the products dimension table based on the product code. You also need to specify the product line in the select clause, and then you can group the results by their product lines. You'll then have to use a country column from the locations dimension table to filter the results so that only the sales that occurred in the USA are computed. You can join the locations table based on the postal code, then filter the results where the country equals USA. In the end, you'll have a SQL query that looks like this. Say you wanted to find the same information, the total sales amount for each product line within the USA, but using the normalized version of the data set, how would that query look? 
Well, you would first need to locate the business measures you're looking for, meaning the total sales amount. You can get this by multiplying the price each column with the quantity ordered column and the order details table. Then you'll have to join the order details table with the products table, then the products table to the product lines table to group by the product line. Finally, you'll have to join the customers orders and orders details table to filter the results by country. While the two models contain the same information, star schema organizes the data in a way that's easier for business users to understand and navigate. It also results in simpler queries with fewer joints which speeds up the query performance. Normalized forms and star schemas both have their own use cases. 
While normalized forms ensure data integrity and avoid data redundancy, star schemas facilitate analytical workloads. In the next video, we'll discuss how these two forms are used in data warehouses. I'll see you there. 


#### Inmon VS Kimball Data Modeling Approaches for Data Warehouses
Back in course three, we discussed data warehouses as a storage system that you can use to separate transactional systems from analytical systems, but we didn't discuss how the data is actually modeled within the data warehouse to support this. There are many approaches to modeling data for data warehouses. And the big ones you will likely encounter in your work as a data engineer are the Kimball, Inmon, and data vault modeling approaches. In this video, we'll go through the Inmon and Kimball approaches, and then later this week we'll look at data vault and other data modeling techniques. My good friend Bill Inmon, who is widely known as the father of the data warehouse, created his approach to data modeling for data warehouses in 1989. With the goal of separating the source system from the analytical system. As you've seen in course three, Inman defines a data warehouse as a subject oriented, integrated, non volatile, and time variant collection of data in support of management's decisions. 
Here's a continuation of the definition, the data warehouse contains granular corporate data. Data in the data warehouse is able to be used for many different purposes, including sitting and waiting for future requirements, which are unknown today. The subject oriented and granularity aspects of this definition mean that an Inmin model organizes data into the major subject areas of the business and includes all the details related to those subjects. For instance, in an e commerce company, you can have subjects such as products, orders, customers, shipments, and so on. And for each subject, the data model must contain all the details, such as business keys, relationships, and attributes. So with the Inman data modeling approach, you need to consolidate data from various data sources and model the data into a highly normalized form that is then stored in a data warehouse. You can then serve the data for downstream reports and analysis via department specific data marts. 
With this approach, the data warehouse represents a single source of truth that supports many data use cases, even if the current analytical requirements are not yet defined. This strict normalization requirement in the data warehouse reduces data duplication, leading to fewer downstream analytical errors, better data integrity, and consistency. Here's an example of how you can apply the Inmon modeling approach to e-commerce data. Let's say that the business stores information about orders, inventory, and marketing in separate source systems. You can ingest the data from these sources and store it in the data warehouse in a highly normalized third normal form. To meet department-specific data needs, you can then take this data from the data warehouse, model it into various star schemas or other appropriate models. And place them in downstream data marts for sales, marketing, and purchasing, with each department having its own data structure that's unique and optimized to its specific needs. 
This way, data users from each department can easily query the data for their use cases. Unlike the Inmon modeling approach that uses a data warehouse to store data across a business that's been modeled into a normalized form and then serves department specific analytics through data marts. The Kimball modeling approach focuses more on modeling and serving department-specific analytics directly in the data warehouse without first normalizing the data. Created by Ralph Kimball in the early 1990s, this approach effectively allows you to serve data that's structured as star schemas or similar variants directly from the data warehouse itself. Incorporating data marts into the overall warehouse architecture. Here's an example of how a Kimbell data warehouse can be used for e-commerce. After you ingest the data from the orders, inventory, and marketing source systems, you model the data into multiple star schemas to address the different facts of the business and then store them directly in the data warehouse. 
Kimball's approach enables faster modeling, hence quicker iterations, than Inmon's approach. But with a trade off of potential data integrity issues because you're storing star schemas that have more data redundancy and duplications directly in the data warehouse. So if your organization prioritizes quick, practical insights into specific business processes and is looking for rapid implementation and iteration of their data warehouse, then I suggest you adopt Kimball's approach to data modeling. On the other hand, if data quality is your highest priority, or if the analysis requirements are not yet defined, then I suggest you choose Inman's data modeling approach that treats a data warehouse as a single source of truth. All data marts are then built on top of the highly normalized data warehouse to ensure data consistency and integrity. Depending on the organization, you might need to apply both the Inmon and Kimball modeling approaches when modeling data for different data warehouses. So it's important that you understand how to handle data that it's highly normalized and data that's in a star schema. 
And so, before we discuss another approach to data modeling, join me in the next video to learn how you can convert a normalized data model that's in third normal form into a star schema. 


#### Another Modeling Example
Assume you work at a car rental company and you are tasked with developing a star schema model that can help the  company gather and analyze information on rental trends and customer preferences. For example, you'd like to determine peak booking times, identify the most popular cars being rented, and adjust car rental rates based on demand.

Identify the business process and the grain: the business process consists of the car rental transactions. The grain would be an entire rental booking made by a customer for a particular car.  So each row in the fact table should correspond to one rental booking and could be identified by the booking id. 

Identify the dimension tables: to provide context for each rental event,  you can create the following dimension tables:

dim_customers that contains the customers' details (name, address, phone number, driver's license number);

dim_cars that contains the cars' information (VIN - Vehicle Identification Number, model, brand, make, color, purchase date);

dim_dates that contains the information of a given date (year, month, time, day, quarter, day of the week). 

dim_stores that contains the information of the rental store (zip code, state, city, address)

Identify the fact table: Each row represents one rental booking which is identified by the booking id. It also contains the dates that describe this rental period: rental start date, rental end rental date, and return date. It also contains the foreign keys: customer key, car key and store key. And the business measures are the booking fee, insurance fee, fuel charge, extra rental days, fuel level, and total cost.


#### Data Vault
The Inmon and Kimball modeling approaches focus on the structure of business logic in the data warehouse. Whereas another modeling approach called Data Vault focuses on separating the structural aspects of data, meaning the business entities and how they're related from the descriptive attributes of the data itself. It uses separate tables to represent core business concepts, the relationships between those concepts, and the descriptive attributes about those business concepts. You can follow a Data Vault model to allow for a more flexible, agile, and scalable data warehouse structure by keeping the data as closely aligned to the business as possible, even while the business and its data are changing. Dan Linstedt introduced Data Vault in the 1990s as a different approach to modeling data in the data warehouse. This approach has evolved over time. Nowadays, the Data Vault architecture consists of three layers, the staging area, the enterprise data warehouse layer, and the information delivery layer. 
You can load draw data from source systems into the staging layer, and an insert only manner, meaning you don't alter the data or enforce business logic, except to ensure that the expected data type is ingested. Next, you model the data in the enterprise data warehouse layer, using hubs, links, and satellites to separate business objects and their relationships from their descriptive attributes. We'll dive into this layer in more detail later. Then finally, in the information delivery layer, you load the data into downstream data marts that can be modeled as star schemas or other structures to support various business areas. Operations like aggregation and grouping, which modify the meaning of the data to meet user needs or applied in the information delivery layer. Unlike other data modeling approaches you've learned about so far, there's no notion of good, bad, or conformed data in a Data Vault. You only change a structure in which the data is stored. 
This way, you can easily trace the data in the data warehouse back to its source, and you can avoid having to restructure the data in the warehouse in case the business requirements change. Let's take a closer look at the model in the enterprise data warehouse layer. The Data Vault model consists of three main types of tables, as I mentioned before. These are hubs, links, and satellites. In short, a hub stores a unique list of business keys to represent a core business concept such as customers, products, employees, vendors, and others. A link connects two or more hubs to represent the relationship, transaction or event between two or more business concepts. The links and hubs don't contain descriptive data, that's actually stored in the satellite tables. 
A satellite contains the attributes that provide the descriptive context for the hubs or other links. User will query a hub, which will link to a satellite table containing the query's relevant attributes. Let's go back to our e-commerce example and model the data as a Data Vault model by following three key steps. First, you need to model the hubs, which contain business keys. To identify the business keys, you can ask yourself, what is the identifiable business element? How do users commonly look for data? A business key can be a column or a set of columns that the business uses to identify and locate the data, and it must not be a key generated in or tied to a particular source system. 
This way, you can easily integrate these business elements from different source systems. In a e-commerce example, the business concepts you can model as hubs or a customer, order, store, and item. The business keys are a customer ID for the customer hub, order ID for the order hub, store ID for the store hub, and skew for the item hub. These keys uniquely identify each business element, and I assumed that the store ID, customer ID, and order ID are keys generated by the business rather than being tied to the source system. In addition to the business keys, a hub must also contain three additional standard fields. The hash key, which is a calculated column consisting of a hash of the business key column and use as a hubs primary key. The load date, which represents the date on which the business key in a given row was first loaded into the hub, and the record source, which represents a source from which the unique row is obtained. 
The next thing you need to model are the links. You use a link table to connect two or more hubs to capture the relationships between the business keys of the hubs. Let's say you want to model the event of a customer placing an order, you can add a link table to connect the order hub to the customer hub to show which customer placed the order. Then you can add a link table to connect the item hub to the order hub to show which items were ordered. Finally, you can add a link table to connect the order hub to the store hub to show which store the order was placed in. Each table must contain the primary and business keys from its parent hubs, the load date of a row, and the source for the record. For each table, the primary key consists of a hash calculated based on the business keys of the parent hubs. 
With link tables, you can easily add new relationships or update a current relationship without having to re engineer the Data Vault. For example, let's say the company decided to completely change a sliding of business and instead of selling items at the sites to sell services. In this case, you can create a new service hub table and a new link that connects this service hub to the order hub, and then stop using the old item hub and item order link. Moreover, the link tables can model a many to many relationship. Let's say the organization changes the rule and now allows more than one customer to contribute to the same order. You can use a link table to model this new many domain relationship between the customer hub and the order hub. You don't need to change anything in the design. 
Now, to give meaning and descriptive context to the hubs and the links, and the third step, you can create satellite tables. In this example, I created a satellite table for the customer hub and added some customer information, such as customer name and customer zip code. You can add satellites for the store, order and item hubs as well. I also created a satellite table for the item order link to add context to this relationship, namely the quantity of that item placed in the order. Each satellite table must also contain the record source, and for the primary key, it should consist of a hash key of the parent hub and the load date. There you have an overview of the Data Vault model. Data Vault provides a flexible design that decouples the structure of your data from the source systems, enabling you to easily adapt as a business evolves. 
With that, we covered the basics of a three most popular data modeling approaches for data warehouses Inmon, Kimball, and Data Vault. However, the coverage hardly does justice to their respective complexity and nuance. In the resource section, I have listed some books from the creators of each modeling approach that I highly recommend you read to further understand how and why data modeling is central to batch analytical data. In recent years, an approach known as one big table has emerged to model data for analytical use cases. Join me in the next video to explore one big table as the final data modeling approach will cover this week. 


#### One Big Table
The modeling approaches we've looked at so far this week, in particular, Kimball and Inmon were developed when data warehouses were expensive on premises and heavily resource constrained with tightly coupled compute and storage. While batch data modeling has traditionally been associated with one of these strict approaches, more relaxed approaches such as what's known as one big table, or obt for short, are becoming more common. With one big table, you throw all your data into a single wide table, which is exactly what it sounds like, a very wide collection of many columns typically created in a columnar database. A wide table can potentially have thousands of columns, whereas tables in relational databases typically have maybe a dozen or so columns, and a column may be a single value or contain nested data. So y tables with one big table are highly denormalized and flexible. Here's an example of a wide table, which is the denormalized table you saw in an earlier video. This is just a small example, and the wide tables you'll encounter with one big table can have way more columns, and as you can see, this table combines various data types and each row represents a customer order. 
You can think of one big table as the denormalize extension to Kimball's approach, where you have facts and dimensions represented in the same table. By doing so, you can free the data analyst from performing any complex joins, or any joins for that matter. Moreover, you can run the same analytical queries faster on wide tables than on highly normalized data, or even on data modeled as a star schema, where you might still need to join dimension tables with the fact table. The wide table simply contains all of the data you would otherwise need to join together in a more rigorous modeling approach, which can have a huge impact on scan performance. This one big table approach is becoming more common because of the low cost of cloud storage. Also, many organizations are choosing to design flexible schemas in their source and analytical systems by using nested data. And with one big table, you can store this nested data all together in one table without having to worry about the optimum weight represented in storage. 
Wide tables are usually sparse, meaning the vast majority of entries in any given field may be null values. So it's extremely expensive for you to store and read a white table with a bunch of null values and a traditional row oriented relational database, because the database allocates a fixed amount of space for each entry, and the database must read the contents of each row in its entirety when reading from the database. But with the emergence of columnar databases, you can read only the columns selected in a query and so reading nulls is essentially free. So columnar storage helps optimize the storage and processing of wide tables. The biggest criticism towards the one big table approach is that as you blend your data, you lose the business logic in your analytics. Another downside is that you need complex data structures like arrays to store nested data. These structures have poor performance when it comes to updates and aggregation of elements. 
And so I suggest using a wide table when you have a lot of data that needs more flexibility than a traditional data modeling approach might provide. When it comes to modeling your data, there's no one size fits all solution, so make sure you understand the trade offs between the possible approaches when it comes to flexibility, data integrity, and ease of use by downstream stakeholders to choose the best approach for your use case. In the next lab, you'll practice taking normalized data and modeling it into a star schema and one big table, which is a common task you'll have to do as a data engineer. But before you head to that lab, I have included a demo on dbt and an exercise for you to practice writing the SQL queries that you will use in the lab to model your data. I'll see you in the demo. 


#### Demo: Transforming Data with dbt (Part 1)
Interactive Transcript - Enable basic transcript mode by pressing the escape key
You may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key
In this video, we'll use dbt to model some normalized data you've seen previously into a star schema. This is something you'll do in the lab. Behind the scenes, I've set up a local postgres database with five tables, order items, orders, customers, items, and stores. I created these tables under a schema labeled staging schema. I also inserted a few rows into each table, now I'll use dbt to create a new schema that I'll label as star schema. Under which I'll create the fact order items table and the dim stores, dim items, and dim date dimension tables. An earlier video, we already discussed the SQL statements you can use to create each table in the star schema. 
Dbt provides an easy way to create these tables by wrapping each SQL statement with another SQL statement that will create the tables under the new schema. Dbt also helps you document and validate your data within your database or data warehouse. Let's first install dbt and set up the environment. Dbt provides two types of environments, dbt core, which is an open source command line tool that you can install locally in your own environment and communicate with your databases through adapters. Dbt Cloud, which runs dbt core in a hosted environment with a browser based interface. In this video we'll go with the dbt core option, which is what you'll use in the lab. Here in the terminal, I'll create a new virtual environment and then I'll activate it. 
Then I'll install dbt core and the dbt-postgres adapter. Dbt core will use this adapter to connect the postgres database. Depending on the type of database or data warehouse you're using, you can choose other adapters. You can find the list of adapters in the dbt documentation. After dbt core and dbt postgres have been installed. I'll create the dbt project folder that will contain the files used to define the new tables. I'll run the dbt init command. 
Specifying the name of the project to be dbt tutorial. Then dbt will ask you to specify the type of database. Since I've only installed dbt-postgres, I just have one option, so I'll type one. Then dbt will ask you to enter the database information to automatically create the file that contains the connection details to the database. For now, I'll stop this process by typing control C. I'll show you how to manually create this file later. After you run dbt innate, a folder labeled dbt tutorial will be created in your working directory. 
I'll open an IDE to go through the files in this folder and run the dbt commands. Here are the folders and files created in the dbt tutorial project folder. The model sub folder is the main directory where you'll spend most of your time. In this directory, for each table you want to create in your star schema, you'll create a.SQL file that contains a SQL statement that defines the corresponding table. In the same directory, you can add a Yaml file to define some model configurations and some other testing and documentation information. When you initialize your project, some example files will be generated under the models directory. Feel free to take a look at them, but we won't use them in this video, so I'll delete them when I create the files for our star schema. 
You can use the analyses sub folder to include any SQL statements that are not part of your models directory and that could be a part of your own exploration. When you run dbt, only the SQL statements that are in the models sub folder will be executed. In the Macros sub folder, you can store pieces of SQL code that you want to reuse multiple times in your models directory. In the seed sub folder, you can have some CSV files that you want to load into your data warehouse or database using dbt. In the snapshots sub folder, you can record the changes in your tables over time. Finally, in the test sub folder, you can create SQL statements to perform some specific tests on your data. In this video in lab, we'll just work with the models sub folder. 
In addition to these sub folders, there's also this dbt_project Yaml file that's also automatically generated by dbt innate. This is an important file that contains the configurations of your project, so let's take a look at the file. In this file you specify the name and version of your dbt project. Next, you select the profile for your project. The profile should contain the details for connecting to the database, and you need to define the profile within a Yaml file called profiles.Yaml. Here, dbt automatically labeled profile with the project name. I'll leave it as it is, but you can definitely choose another name for the profile. 
Next, you'll define the directory configurations that tells dbt were defined the sub folders of your dbt project. With clean targets, you can specify the directories to be removed when you run dbt clean. The target directory contains the compiled SQL files of your models that will be created after you run dbt, and the dbt packages is a directory that you can create to store third party packages if your project depends on them. Finally you can specify the default configurations for your models, this includes whether you want to create the models as tables or views in your database. Views mean that you only save the SQL statements without creating the actual tables. You can specify this information using the materialized key. You have defined multiple sub directories inside of the models directory, you can organize the model configured by the sub directories. 
In this example, we're telling dbt to create all models defined under the example sub directory as views. Later, I'll delete this configuration and replace it with the configurations for the actual tables of the star schema. Now let's create the profiles file to specify the connection to my local postgres database. Inside the dbt tutorial project, I'll create profiles.Yaml. You can specify in this file, more than one profile and each can correspond to one data warehouse. Here, I'll create one profile labeled as dbt tutorial. Make sure that the name of the project matches the names you specify in the project configuration file. 
The profile can consist of multiple targets where you can specify different connection details under each category. For example, you might need to create two targets, one for development and another for production. List these targets under the outputs key, I'll specify one target that I'll call Dev. To populate this target, I'll specify the database type. Which is postgres in this case, then I will specify the database credentials. For a postgres target, I'll specify the hosts, user name, password, port number, and database name. In the dbt dbt documentation, you can find a sample profile for each possible target type. 
Finally, I'll specify that this dbt project will run on one thread, and that the default schema is a star schema. To wrap up the dbt tutorial profile, I need to specify the default target. Which is Dev, because that's the only target I have right now. To verify the connection to the database, I'll go to the terminal. I'll make sure that I'm in the dbt project directory, and I'll type dbt debug. If there's no issue with the credentials, you should see that the connection is okay. Since the profiles at Yaml can contain sensitive credentials, you can move this file to the hidden.dbt file. 
That's created by dbt inside your home directory. If dbt does not find the profiles in your project directory, it will automatically check the.dbt folder. Let's move the profiles files to the.dbt folder and then verify the connection. At this point, I created a profiles file and successfully connected to a local postgres database. Join me in the next video to create the SQL files used to create the tables. 


#### Demo: Transforming Data with dbt (Part 2)
In the previous video, I connected to a Postgres database, and now we're ready to create the SQL queries for the tables. Under the models subfolder, I'll first remove the example subdirectory that was automatically generated. I'll then create a subdirectory that I'll label as star schema models. Starting with the dim stores table, I'll create a SQL file dim stores.sql. DBT will use this file name to assign it to the table that will be created in the database. In this file, I'll type the SQL query for the dim stores table. In front of the table name, I'll type staging, which is the name of the schema under which I created the stores table inside the Postgres database. 
In the same subdirectory, you can also create a YAML file to configure your schema for documentation and testing purposes. Here, I'll create a schema.yml file. To document the information about the model, I have to specify the key models. Then I'll specify the name of the table and then the name of the columns. You can add a description about the table. For each column, you can also add a description and specify any of the generic tests provided by DBT. Here, for the store key column, I'll add a description, saying that the store key is the primary key for the dim stores table. 
I'll also add two tests to verify that the values of store key are unique and not null. There are two other generic tests. You can use accepted values to check if a column takes in values from a list of accepted values and relationships to verify how two columns are related to each other. You can also create your own custom tests. I'll create the SQL files for each of the dim items, dim date and fact orders tables, and then type the corresponding SQL query inside each file. In the schema file, I'll add a description for each table, and for each primary key, I'll add a description in the same tests. Make sure to pay attention to the indentation when working in a YAML file. 
Before we run DBT to create the tables, let's update the default configuration for the models in the DBT project YAML file. Here, I'll remove the configuration for the example and replace it with the name of the subdirectory we created under the models directory, which is star schema models. Then I'll specify table as a value for the materialized key. This ensures that all the tables are actually created in the database. We can also specify the schema key. For example, if I specify analytics for the schema, the schema value will be appended to the default schema name in the profiles.yml file. This way, the tables will be stored under star schema analytics. 
In this example, I'll remove the schema key to ensure that the tables will be created under the schema star schema. Note that you can also specify these configurations under the models directory, which will override the default configuration of the DBT project file. Now that we've defined the tables for the star schema, let's run DBT to create the tables. In the terminal, I'll make sure that I'm in the project folder, and then run the DBT run command. If you have more than one subdirectory under the models directory, you can select which models you want to execute using the -S or --select options and then specify the name of the subdirectory. For example, I can execute star schema models by typing DBT run -S star schema models. But here, since I only have one subdirectory, I can remove this option. 
To verify that all tables have been created from the terminal, I'll establish a connection to my database server. I'll choose the database, and then I'll check the schemas in the database. Then I'll check the tables inside the star schema. You can see that all four tables have been created. Let's check the rows in the fact tables. Now I'll exit the connection and perform one last DBT command to test and validate the data. You can see that all tests were completed successfully. 
DBT has other features, which you'll use in this lab. For example, instead of using the MD5 hash function to generate surrogate keys, you'll use a utility function to create your own surrogate keys. You'll also use a DBT date package to generate a complete date dimension table that has all of these fields. Now it's your turn to try out the lab where you're going to implement another star schema. Before you jump into the lab, make sure to complete the next practice quiz to familiarize yourself with the normalized tables you'll see in the lab and how you'll transform them into a star schema model. You'll also create the SQL statements for the fact and dimension tables. Then you'll be ready to implement the SQL statements in the lab using DBT. 


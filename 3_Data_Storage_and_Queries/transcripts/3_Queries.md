# Queries

#### Overview
Welcome to the third and final week of this course. In previous weeks, you learned how data is stored in storage systems such as databases and optic storage, and how storage abstractions add additional management layers on top of storage systems. This week, we're going to look at how the way data is stored and managed has a direct impact on the speed of data retrieval, meaning how fast you can query the data, as well as how queries can impact the performance of the storage systems themselves. In your work as a data engineer, you'll be writing queries to extract data from internal and external systems, as well as setting up data storage solutions that your stakeholders can query directly. You need to understand how your data storage and management choices can impact query speed and system performance. To do that, you need to understand how queries actually work in detail. A query is a statement that you write in a specific query language to retrieve data or act on data. 
For instance, in the previous course, you wrote SQL statements to interact with a relational database management system or RDBMS. But queries are not only limited to tabular data. In another lab in that course, you use SQL like statements to retrieve data from Amazon object storage. In the first week of this course, you use Cypher to query relationships and node properties from a Neo4j graph database. Query languages or declarative languages, meaning that when you write your queries, you only have to describe to the DBMS what data you want to retrieve or what you want to do with the data without worrying about the exact steps that are needed to execute your query. These details are abstracted from you and handled by the DBMS. So with the DBMS handling the details, it might be tempting to think that you don't have to understand exactly how queries are processed behind the scenes. 
But if you don't really understand how queries work, one day, you might end up running a query that brings down a critical database for a few days or more. Believe me, that's no good. Even if you're already good at writing efficient queries yourself, understanding how queries are processed can help you model the data so that it's easier and faster to retrieve by your stakeholders. This week, we'll explore the journey a query takes to be executed from the moment you write a query to when it's parsed through the creation and implementation of an execution plan to finally when the results are returned or the desired action is executed. Then we'll cover techniques you can use to improve SQL query performance, like creating a database index that can help optimize a search for specific records in a database. We'll be focusing on SQL this week because it's an extremely popular and well established language. Many of the techniques that we'll cover are applicable to other query languages, as well. 
We'll also look at aggregations, revisit our discussion for row versus column storage, and discuss queries on streaming data. In the labs this week, you'll get hands-on experience with advanced SQL statements, compare the execution time of an analytical query performed on a row storage versus columnar storage, and finally, use Amazon Managed Service for Apache Flink to perform time-based windowed queries on streaming data. Join me in the next video to examine the life of a query. 


#### The Life of a Query
While running a query might seem like a simple task where you write code, run it, and get the results. A lot's going on behind the scenes. Multiple components inside your database management system work together to translate your query into a set of detailed action steps. Even the simplest select query statement goes through these pages in order to be executed. Let's go through the life of a query so you can see how it's executed in a database. Here's a typical architecture for a database management system. In Week 1 of this course, we discussed the details of the storage engine. 
Here, I'll be focusing on the other components. When you send your query to the database, your request arrives through the transport system, which hands a query over to the query processor. The processor has two main components, the Query Parser, and the query optimizer. The Query Parser breaks down the query into query tokens, which are the basic building blocks of the SQL query, including keywords like select and from, table and attribute names, operators, and so on. Then the Parser checks for proper syntax and validates the query by ensuring that all table and attribute names referenced in the query actually exist in the database. Control checks are also performed to ensure that you or whoever ran the query have the appropriate access to these attributes. Then the Query Parser converts the SQL code into byte code, which expresses the steps needed to execute the query in an efficient machine readable format. 
The byte code is then passed to the query optimizer, which analyzes the query and devises an execution plan to retrieve the results from the storage layer. Since a query can be executed in multiple ways, the query optimizer attempts to find a suitable strategy that uses available resources as efficiently as possible. To do this, the query optimizer generates various execution plans based on factors like the types of operations required, the presence of indexes, and the data scan size. Then it calculates a cost value for each plan, which can include several components such as the IO costs for transferring data from disk to memory, as well as a computation and memory usage cost. In the end, the query optimizer picks the least expensive plan, and so query optimizers are fundamental to your query performing as well as it can. Once the execution plan is created, the execution engine carries out the sequence of operations outlined in the plan and produces a query results. While all the details of executing a query are generally abstracted from you, you can access the execution plan of any query statement. 
To understand its performance before it's executed or troubleshoot the cause of a slow query after it's executed. For instance, with relational databases, you can add the explained command before your SQL statement to display the sequence of steps the database will take to execute the query. This will also show you the various resource consumption and performance statistics in each query stage. Let's take a look at an example. Here's the customer table of the DVD rental database that you saw in a lab in the previous course. Let's say that I sort of the data in a postgress database, and I want to select all records from the customer table. I can add the explained command in front of this simple select statement to fetch the execution plan created by the postgress query optimizer. 
The return plan specifies that it's going to perform a sequential scan, meaning a full table scan. It also shows two cost values. The first is the start up cost for any processing needed before outputting the results and the second is a total cost for the execution to retrieve the query result. Additionally, the plan also outputs the estimated number of rows that will be returned and the expected row width and bytes. In this example, the full table scan will have a start up cost of zero cost units, but a total cost of 14.99 cost units to return 599 rows. Let's say you want to run this select query again, but this time, you decide to use this were clause to filter the records in the queries so that only records with customer ID equaling three will be returned. The return plan now specifies that it's going to use the index on the customer ID column to look for the specified row that matches the wear condition instead of scanning the entire table. 
Recall from Week 1 that an index is a special data structure that you create as a way to keep some metadata on the side to help locate the data you want more efficiently. You'll notice that with this index based strategy, the total estimated cost is less than the cost of the full table scan. We'll get into the details of the database index in another video. You can use this explained feature whenever you want to understand your query's performance. This feature is not exclusive to relational databases, you can also use it with no SQL databases as well as data warehouses or Data lake Houses. Join me in the next video for a quick introduction to the first lab this week. 


#### Advanced SQL Queries (Part 1)
In the first lab of the previous course, you use SQL quarries to apply crud operations on a relational database, where CRD stands for create, read, update, and delete. You learned how to create a table and new records. You use select statements to read a set of records, as well as update and delete existing records. You also get a chance to apply a predicate to filter your data using the were clause, combine data from different tables using joins, and apply aggregate functions such as count sum average Min and Max. In the next lab, you'll work with more advanced eQL statements. These include select distinct SQL functions to manipulate strings and dates, case statements, SQL Boolean expressions, common table expressions or CTEs, sub queries, and SQL window functions. Before you jump into the lab, let's go through these advanced SQL statements. 
Here's the entity relationship diagram or ERD of the data you'll be working with. It contains the same information as the DVD rental database you worked with in Course 2, but here it's organized into what's known as a star schema. You'll learn more about this data model in the next course. The middle Fact table called Fact Rental contains information for each rental transaction made by a customer, such as the rental date, return date, amount paid, the ID of the rented firm, it's category ID, the idea of staff that serve the customer, and so on. Other surrounding dimension tables contain more detailed information about the customers, films, film categories, actors, as well as the stores and the staff. To walk you through these advanced eQL statements, I'll only be focusing on the rental fact table and the dimension tables for the customer, staff, film, and film category. Let's say you're interested in knowing which staff member served which customer? 
From the fact rental table, I'll select the staff ID and the customer ID. This queria returns all pairs of staff and customer IDs. But since the customer could be served by the same staff member multiple times, the result can contain repeated pairs of staff and customer IDs. You can add the SQL keyword distinct to the select statement here to ensure that the result contains only unique pairs of staff and customer IDs. Now, let's include the first and last name of the staff member, which you can find in the staff dimension table. I'll join the fact rental table with the Dim staff table based on the staff ID column. I'll add the staff first name and staff last name to the select statement here. 
You can concatenate the first and last names into one string. Depending on the database management server, the syntax for string concatenation might look different. Here I'm working with a MS SQL database where I can use the Concat function to combine the first and last names. But this concat function will return the full name of the staff member without any spaces in the name. To make it easier to read, let's add a single space between the first and last name. I'll label this column as staff name. Other than concatenating two strings, you could also apply other string manipulation functions such as lower to convert the string to lower case or upper to convert it to uppercase. 
You can also use the substring function to extract a part of a string. For example, to return the first letter of the last name, I'll apply the substring function to the staff last name. This function expects two arguments, the start position and the number of characters to extract. Since I only want the first letter, the start position and number of characters would both be one. Here are the results with the updated last name. Now, let's say you want to check whether a customer made an on time payment, meaning that they paid for the DVD rental before they returned the DVD. In the fact rental table, you can compare the payment date and return date columns for individual records. 
But to make this easier, let's use the SQL case statement to create a column that contains a one if the payment date is before the return date and a zero otherwise. The statement starts with a case keyword and finishes with the end keyword. In between these two keywords, you can use the one keyword to specify the condition and the then keyword to specify the result you want to associate with that condition. After you list all the conditions, you can use the ls keyword to specify the result to return if none of the listed conditions are met. Let's use the statement to create the indicator column. From the fact rental table, I'll select the customer ID, the rental ID, and then I'll write the case statement and specify the on time payment condition, which is when the payment date is less than the return date. Then the result is one, else the result is zero. 
Then I'll finish the statement with the end keyword. I'll label this column as on time payment. Let's limit this query to only show the first five rows of the result. Are the results of this query? All these five customers have zero in the on time payment column, meaning that they have paid for the DVD rental after they have returned the DVD. Let's filter these results so that you can only see the results for customers who are located in the United States and Canada, and for any rentals that occurred between May 24th, 2005, and July 26th, 2005. To get the country of the customer, I'll join the fact rental table with the customer dimension table based on the customer ID. 
You'll need to use the were statement to filter the results based on country and then by date. First, let's check if the country of the customer is the United States, or if the country is in Canada. Or you can write this expression using the in operator and check if the country of the customer is in the list containing the United States and Canada. I'll go with the in operator here, which could be handy when you're interested in more than two options. Next to check the date, you can use the between operator to check if the rental date is between 2005-05-24, which is May 24th, and 2005-07-26, which is July 26th. All right end between these two Bulion expressions here since I want both the country and the date conditions to be true. Now I'll run the query to see the results. 
We discovered the select distinct statement, some SQL string functions, bleion expressions, and the case statement. Join me in the next video, to go over some more advanced SEL techniques that you'll see in the lab. 


#### Advanced SQL Queries (Part 2)
In the previous video, you saw how you can apply the select distinct statement and some SQL string functions to query the unique pairs of staff and customer IDs from a DVD rental database. Then you use the case statement to create a column that indicates whether a customer paid for the DVD rental on time, and you filtered the results using Boolean expressions in the WHERE clause. In this video, I'll show you how to apply some advanced SQL techniques, including common table expressions, subqueries, SQL window functions, and some SQL date functions to your queries. Let's say you want to perform some additional computations on top of each of the previous two queries. For instance, in the first example, you might need to know the total number of customers that were served by each staff member, or in the second example, you might want to compute for each customer the average of the on-time payment column to get the percentage of on-time payments for each customer. In both examples, I don't want to store these results in a separate table. I just need to be able to reference these temporary results to perform my computations. 
You can use common table expressions or CTEs to define these temporary result sets that can be referenced elsewhere in the query. Let's take a look at the first example and compute the total number of customers that were served by each staff member. To define the CTE, you start with the with keyword. Then you specify a name for the CTE, which we'll call staff customer pairs, followed by as keyword and the query that represents the temporary results enclosed between two parentheses. Within these parentheses, I'll write the query you saw in the last video that got you the unique staff and customer pairs. Once you define your CTE, you can query it in the same way, you query any table. I'll write a regular query to select the staff name and count the customer IDs from the staff customer pairs CTE and then group by the staff name. 
You can see that there are only two staff members, and they both served the same number of customers. Now, let's move on to the second example to compute the percentage of on-time payments for each customer. I'll define the CTE called customer payment info and write the query you saw in the last video to get the on-time payment indicator column from this customer info CTE. I'll select the customer ID and compute the average of the on-time payment indicator column to get the percentage of time each customer paid on time. I'll then group by the customer ID. The results show zero as the percentage of time each customer paid on time. You can verify that this is true for all customers by defining this as a CTE and then finding the maximum of the percent on-time payment column. 
After the first CTE, I can type comma and specify the name of the second CTE as customer percent on-time payment. Then I'll surround the second query with parentheses. After these two CTEs, I'll select the max value of the percent on-time payment column from the customer percent on-time payment CTE. Indeed, I do get a max value of zero. It looks like no customer paid on time, and you might want to investigate this further. For now, let's continue exploring more advanced SQL statements. With CTEs, you saw that you can define your temporary results using the with keyword and you can query from the CTE similarly to how you would from any table. 
You can also incorporate some temporary results within your main query using subqueries. Let's focus on the film dimension table. You can see that each film has a certain length, and let's say you're interested in getting the IDs of the films that have length greater than the average length. You can start by getting the average length of the film by selecting average length from dim film. This query returns a single number, which represents the average of the film length. You can then incorporate this query as a subquery within the main query to return the IDs of the film with length greater than this average. Here, I'll select film ID and length from the dim film table, where length is greater than the average. 
I can get this average by writing the subquery that returns the average length of the films and enclose it within parentheses. You can see that the return movies have length greater than the average length. The last type of query I'd like to go over are SQL window functions. This type of query allows you to apply an aggregate or ranking function over a particular window or set of rows. Similar to performing aggregation using group by but instead of considering all rows at once, it applies the aggregation to a subset of rows, and it also doesn't group rows into a single output row. Each row remains separate. Let's start with a ranking function. 
To define the window, you use the over clause, which expects two pieces of information. The column, you want to partition the rows by, and the column, you want to rank the rows by. To rank each row in each window, you can choose from several ranking functions such as rank and row number. Let's go over an example. First, I need a query that I can use as a CTE to apply the window function. Let's write a query that computes the average duration and days that a customer spent on a film category. I'll focus on the rental fact table and the category dimension tables. 
From the fact rental table joined with the dim category table based on the category ID, I'll select the customer ID and the name of the film category and then I'll compute the average of the difference between the rental date and return date using date diff, which is a function that finds the difference in days between two dates. Finally, I'll group the results by customer ID and category name and order by the customer ID and the average rental days. Here are the results which show for each customer the average number of days spent on each category. Now let's add a column to this result that shows for each customer the rank of each category based on the rental days. First, I'll define this query as a CTE called customer info. Then I'll select the customer ID. Category name. 
And average rental days columns from the CTE. Then I'll add the ranking column using the rank function. I'll define the window using the over keyword and specify that I want to partition the rows by the customer ID and order them by the rental days in descending order. I'll label this column rank category. Then finally, I'll order the results by the customer ID and rank category. You can see that the records are grouped by customer ID. Within each customer ID, the film categories that have the highest rental days are also shown. 
The rank function assigns the same rank to the rows where there's a tie. The row number function, on the other hand, assigns different ranks when there's a tie. For the same window function, if instead of the rank function you use any aggregate function such as the sum of the average rental days, it will return the running sum over each window. In the results here, you can see that you have the customer IDs and the category names like before, but now the running sum column shows the total average rental days for this film category along with all the film categories before it. There are other window functions such as lead and lag that you can check out in the optional part of this lab. Now it's your turn to try out the lab. After that, join me in the next series of videos to see how some of these SQL statements are processed behind the scenes and explore strategies you can use to improve their performance. 


#### Index Deep Dive
Interactive Transcript - Enable basic transcript mode by pressing the escape key
You may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key
Back in week one, you saw that with relational databases, you can create an index to help certain queries run faster. In fact, when the query optimizer analyzes a query, it considers if an index is present and whether using an index based plan can reduce the query cost. So in this video, we'll dive a bit deeper into the details of the index structure. As a data engineer, understanding how indexes are implemented can help you design better index structures and optimize your queries to improve SQL performance. An index is a separate data structure that has its own disk space and contains information that refers back to the actual table. It stores the data from one or more columns in a well defined order. So just as you would use an alphabetical index at the end of a book to quickly find pages related to specific topics, instead of searching through the entire book, a database management system can query data by searching an ordered index rather than scanning the whole table. 
And in this way, the results are returned to you much faster. So, revisiting the example from week one, you can see here that the entries from the country column are stored alphabetically in an index structure. When you want to query the orders that took place in a certain country, say you want to select all records from the order table where the country is USA, the database can perform a binary search over the index to locate the rows in the actual order table that have USA in the country column. But how is the index actually implemented? In this example, I showed the indexed entries in a table as a way to explain the concept of an index. However, the data in an index is not actually stored sequentially as a table. It's instead divided into blocks that are doubly linked together to enable forward and backward reading from any block, the data stored within each block is sorted. 
Then the index blocks are linked together in a way that maintains the logical order of the entire index. Moreover, the location of the blocks doesn't matter because they are properly linked together. This structure facilitates the update of the index when new data is inserted or old data is deleted. For example, let's say an order from Thailand was added to the table. Since Thailand is alphabetically between Mexico and USA, you would add it in the third index block you see here and shift the rest of the data accordingly. And if that record were to be deleted, then you would remove the corresponding country from its index block. This whole time, the index blocks remain doubly linked. 
Now, in order to efficiently locate these index blocks, another structure known as the balanced search tree, or B tree, is built on top of the index blocks. The index blocks represent what is known as the leaf nodes for the tree. Then building up from these leaf nodes, you have internal nodes, also known as branch nodes, that act as apparent nodes for groups of leaf nodes. So for example, this internal node is the parent node of these three leaf nodes. The internal node stores three entries, Canada, Mexico and USA, which represent the last country from each leaf node sorted in alphabetical order. This way, if the database is looking for a country whose name is between Canada and Mexico, it will traverse down to the second leaf node. Here. 
If the name is between Mexico and USA, it will traverse down to the third leaf node. This pattern of how nodes are grouped and linked is repeated up the tree until it reaches the first node or root node of the tree. So let's say you want to query orders that were placed in Canada. To locate these rows, the database starts at the root node, then looks for the country Canada. Canada is not in the root node, but since it's alphabetically between Brazil and USA, the database chooses the second internal node, which you know will contain countries that are alphabetically between Brazil and USA. The database searches this internal node for Canada and finds that it is the first entry. So the database chooses the first path and reaches a leaf node where it finds all the records within the leaf node that represent orders placed in Canada. 
Since the entries are not unique within the country column, the database also needs to horizontally traverse across the leaf nodes to find all entries that correspond to Canada. So in summary, to retrieve data that has an index structure, the database would need to traverse the b tree first, since the tree is maintained to be balanced, meaning that the number of children nodes are evenly distributed between the parent nodes all the way up to the root node of the tree. Traversing the tree is always an efficient operation that takes o log n time, but if the index does not contain unique elements, once the database has located the appropriate leaf node, it needs to then traverse horizontally across a chain of leaf nodes to retrieve all rows with the desired index value. If there are lots of repeated values, traversing a chain of nodes could end up being almost like scanning the entire table, in which case a query optimizer will choose to scan the full table instead of using the index. So when you create an index, you need to carefully choose a suitable column or columns to build your index on. The general strategy is to create index structures that will improve the performance of your most performance sensitive queries. You also don't want to overload the tables with too many indexes because maintaining balance in many tree structures whenever the data gets updated may actually degrade the performance of your database. 
To better understand how indexes can impact the cost of a query, let's take as an example the payment table of the dvd rental database you saw in the first lab of this week. First, let's check the execution plan of a SQL query that doesn't involve indexes. Here I'm going to select records from the payment table where the rental id is one. You can get the query plan by adding the explain keyword in front of this query. From the return plan, you can see that the query optimizer chose to do a sequential scan, meaning a full table scan. Now let's create an index for the rental_id column. I'll start with the keywords create index and give this index the name rental_idx. 
I want to create this index structure on the rental_id column of the payment table, so I'll type on payment, then in brackets, rental_id. Ill add the explain keyword in front of the same queries as before to check the execution plan. This time, the query optimizer recognized the presence of the index and chose the index based strategy because it has a much lower cost. So here, by adding an index, I was able to reduce the query time by more than a factor of 30. Ive included a few more examples about indexes in the reading item that follows this video. Be sure to check those out to better understand how indexes can really improve the performance of your queries. So far, we've been discussing the concept of indexes for traditional relational databases, but the same concept also exists for columnar storage. 
For instance, when you create a table in Amazon redshift, you can declare one or more of its columns as sort keys. Then, without creating a separate data structure, redshift directly sorts the rows of the data according to the sort key and then stores the data on disk. For example, when you declare the country column as the sort key for this table, all the rows of the table will be reorganized based on the country column, like this. This is what you see when sorting by a column or group of columns in a spreadsheet. The reorganized version of each column is then sorted on disk. By the way, other cloud data warehouses, such as BigQuery, refer to the sort key as the cluster key, but it's the exact same concept. By properly creating an index in row oriented databases or specifying a sort or a cluster key for columnar storage, you can enhance the performance of certain queries by reducing the amount of rows that need to be scanned. 
After taking a look at the additional index examples in the next reading item. Join me in the video after that to learn about another best practice when querying your data, which is to only query and retrieve the data that you actually need. 


#### Retrieving Only the Data You Need
In the previous video, you saw that by creating a proper index for a table, you can avoid doing a full table scan, which can be very costly. It turns out, however, that you can actually do even worse than simply doing a full table scan. That's when you write a query that not only scans the entire table, but also returns everything. When you run SELECT * with no predicates, meaning no wear clause to filter the results, the DBMS need to scan the entire table and retrieve every row and column. This could be costly for the source database because large amounts of data needs to be transferred from disk, and the data might need to be further processed. I remember a time when a new analyst at a large grocery store chain that I was consulting for, ran a SELECT * command on a production database, and that brought down this entire critical inventory database for three whole days. The company had to spend a lot of money fixing this problem, and things didn't look good for that poor analyst. 
Running SELECT * on your Cloud pay-as-you-go database or data warehouse can also be very expensive. You'll be charged for reading all the bytes from the entire table and for utilizing any compute resources while the query is running. This is why, in general, you should avoid running SELECT * with no where clause to filter the results. As a rule of thumb, you should query only the data you need. If you'd like to explore the data, consider using a technique called pruning to exclude irrelevant data from being scanned in your query. One of the most common pruning techniques is row-based pruning, where you filter out rows that don't meet your wear condition. For example, like you saw earlier, you can select all the records from the payments table where the rental ID column is one. 
When filtering your results, you can further improve your query performance by using indexes in traditional row-oriented databases, or by using sort or cluster keys in columnar storage, such as BigQuery or Amazon Redshift. As you saw in the previous video, you can create an index called rental_idx on the rental ID column and the payment table to speed up this query. Pruning techniques also include column-based pruning where you specify in the query statement, only the columns you need. For example, instead of selecting all records, you can select only the customer ID and rental ID columns from the payment table. This way, the database won't have to scan all the other irrelevant columns. There's also partition pruning, where you only scan specific partitions that contain relevant data instead of scanning the entire table. This type of pruning is possible when you work with a data store that allows you to partition your data based on a partition key, such as date or location. 
For instance, with big query, you can take this table here that's not partitioned and partition the records based on the order dates. You can then further order each partition by the country. Let's say user queries this data and filters by a specific order date of April 1st, and country value of USA. The database only has to scan the records in the April 1st partition, then look for records with USA as a country. To avoid incurring any surprising expenses or degrading the performance of the source data storage, you should always make sure to only read the data that you need. Another factor that has a huge impact on query performance is the way you join data from different tables. We'll discuss the challenges that come with table joins in the next video. I'll see you there. 



#### The Join Statement
Using joins is one of the most common ways to combine datasets, allowing you to transform data and create new datasets within your data pipeline. Additionally, your end users might also use joins to combine the data you serve to them. However, joins are one of the most time-consuming query operations, so it's critical that you understand how your end users will need to combine data with joins when you're modeling the data. To quickly recap how joins work, let's consider these two tables. The orders table contains information about each order placed in an commerce company, and the customers table contains information about each customer. These two tables are related to each other through the customer's ID and are part of a normalized model that has data stored in separate tables to reduce redundancy. To simplify the process of finding information about an order and the customer who placed it, you can combine the data from the orders and customers table using a SQL JOIN statement. 
You'll select all the records from the orders table that's joined with the customer's table on the ID column from the customers table, and the customer ID column from the orders table. This way, the corresponding customer information from the customers table is combined with the orders information based on the customer's ID. For example, for these three orders that all have a customer ID of one, you'll append the customer information for the customer with ID of one. Then for this next order that has a customer ID of two, you'll append the customer information for the customer with ID of two and so on. This type of join is known as the inner join, where it combines data from only the rows that share a matching customer ID in both tables. To help you understand why the join operation is one of the most time-consuming query operations, let's go through three common methods for implementing joins. Most database query optimizers will use one of these methods when devising an execution plan for join statement. 
The default method is the nested loop join, which works like a nested for loop. Starting with the first row in the orders table, the database takes note of the customer ID, then it scans every row in the customers table and only retrieves the rows with this matching customer ID. It combines the information from this row of the orders table with a retrieved rows of customer information. This is repeated for every row in the orders table and the combined results are returned at the end. The second method is the index-based nested loop, which is a variation of the first method. This method can be used when an index exists for at least one of the join attribute, which in this case, could be the ID column in the customers table or the customer ID column in the orders table. Let's assume an index exists for the ID column in the customers table. 
You might have a B-tree structure with a root node, internal nodes, and leaf nodes that looks something like this. To execute this join, the database retrieves a first row from the orders table, and then uses the ID index to locate all rows in the customers table that have a matching ID of one. From the root node, since one is less than 10, it'll go down to the first internal node. Then since one is less than three, it'll go down to the first leaf node. Finally, it finds unique ID of one and its corresponding row addressed from the customers table. The database will then retrieve the data from this row in the customers table and join it with the first row from the orders table. It will repeat this for every row in the orders table and then return the combined results. 
The last method is the hash join method. This method uses a hash function to map the rows from each table to buckets based on the value of the join attribute, which is a customer's ID in this example. With this method, the database first scans all rows of the smaller table, which is the customers table in this case, and sends each row to a particular bucket. For example, we could have a hash function that maps Row 1 from the customer's table to the first bucket, Rows 2 and 3 to the second bucket, and Row 4 to the last bucket. Then the database scans through the rows of the larger orders table, sending each row to a bucket based on the same hash function. For example, it could send Rows 1, 2, and 3 from the orders table to the first bucket, Rows 4 and 5 to the second bucket, then Row 6 to the last bucket. Finally, within each bucket, the database combines the rows from the customers table and the orders table that share matching customer IDs. 
In the first bucket here, since the row from the customers table and the rows from the orders table, all have a customer ID of one, the database would combine all these into three rows. In the second bucket here, the database would combine the rows with the customer ID of two, and then separately combine the rows with the customer ID of three. Then finally, in the third bucket, it'll combine the two rows that both have a customer ID of four. At the end, it'll return the combined results.This method can be much faster because scanning these smaller buckets in parallel can be much faster than scanning the entire table. In all of these methods, the join operation requires a database to scan a considerable number of rows from each table, and that can occur multiple times. That's what makes a join operation one of the most time consuming query operations. Aside from being able to write efficient join queries yourself as a data engineer, you should model and serve data to your end users in a way that enables them to easily join the data when needed. 
The data model and schema you choose affects the number of joins or end users need to perform to get the data that they want. In general, normalized schemas result in less redundancy in your data, but require more complex join statements to combine the data. Here are two schemas you've seen in the first slab of the specialization. The first is a normalized schema of the input data that you ingested in your pipeline. You then ran a glue op to transform the data into the star schema before serving it to the data analyst. Let's say the data analyst is interested in computing the total number of products sold for each country. If you kept the data in its original normalized schema, then the data analyst would need to join the customers table with the orders table and then join the orders table with the orders details table. 
On the other hand, in the star schema, the data analyst would only need to perform one join by combining the fact_orders table with the dim_locations table. Another alternative is to combine the relevant attributes into one big table. Then the data analyst won't need to perform any joins at all. Don't worry if data modeling is new to you. We'll get more into the details of data modeling and the pros and cons of each approach in the next course. But aside from modeling your data, another challenge you'll likely encounter when working with joins is when two tables have a many to many relationship. For example, the payments and orders table shown here have a many to many relationship. 
A payment can be associated with many orders and an order can be paid over several payments. Each payment is associated with a customer, and let's say that customer number 1 made five different payments. Each order is also associated with a customer, and let's suppose that customer number 1 made five different orders. You can join the payments table and the orders table based on the customer number to get this table. That sounds simple, right? The problem is that this table might not represent what you think it should. At first glance, you might think that this table shows the payment information and their associated orders, but there's actually an error in the join logic here. 
By joining the table on the customer number, you are blindly joining the payments table with the orders table without considering if a given payment is correctly mapped to its corresponding order. Because of this error, every row in the payments table that's associated with Customer 1 is mapped to every row in the orders table that's also associated with Customer 1. This creates five times five or 25 rows in the output. Now, suppose that there are many other repeats in the customer number column. This leads to a scenario known as row explosion, where the query returns more rows than expected. Row explosion can generate enough output rows to consume a massive amount of database resources and can actually cause queries to fail occasionally. Avoid this problem, make sure that you check your query plan to see if it correctly describes what you intend the join to do. 
If this query will be run frequently, then consider adding a table to your model that correctly maps a payment to its corresponding order number. Understanding how joins or process can help you design more efficient join queries as well as properly model the data for your end users. I hope you'll join me in the next video for a look at aggregate queries. 


#### Aggregate Queries
Interactive Transcript - Enable basic transcript mode by pressing the escape key
You may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key
When building a system for analytical workloads, you have to support aggregating large datasets. Aggregate queries are used to compute a summary value of a column, such as the sum, average, maximum, minimum, and count of the values in that column. Here's a query where you select the min price from the orders table. In role oriented databases, aggregate queries like this one can be computed by a full table scan, where you scan each row in the table, looking for the row with the minimum value in the price column. Or you can speed up this query by using an index structure if it's available. So, for example, if you have a b tree index on the price column like this, then the query optimizer can decide to use the index and traverse this tree to reach the leftmost leaf node to return the minimum price. When working with aggregate queries, you can also use a GROUP BY clause to group query results by specific columns and return the summary values within each group. 
So, for example, you can add GROUP BY country to your query to get the minimum price for orders placed in each country. In this case, the table must be first partitioned into groups where each group includes just one country. Then the minimum price is computed for each group. The partitioning is usually executed using a sorting algorithm or a hash function, and could be avoided if an index exists on the grouping attribute, which is the country column in this case. In any case with aggregate queries, you are performing operations on columns rather than rows. Remember that with row oriented databases, you store all the values within the same row next to each other on disk. This means that to get the price value from each row, you have to transfer all the rows in their entirety from disk to memory, so you end up transferring more data than you need to execute the analytical query. 
This could work just fine with small data sets, but with large data sets, performing analytical queries and role oriented storage becomes much slower. On the other hand, columnar storage stores all the values within the same column next to each other on disk. So in this case, if you needed all the values from the price column, you just need to transfer the data from the first price value through the last price value that's stored on disk. So performing analytical queries on columnar storage is much more efficient because you only need to transfer the relevant columns of data for your analytical query from disk to memory. In the upcoming lab, you'll get a chance to compare the query performance between row and columnar storage. You'll run an analytical query on a role oriented database hosted on Amazon RDS, and then you'll run the same analytical query in Amazon Redshift, which is a cloud data warehouse that leverages columnar storage. And finally, you'll compare the execution times of both of these queries. 
In the next video, Morgan will walk you through the details of Amazon Redshift. Then after that, I'll give you a quick walkthrough of the lab. 


#### Amazon Redshift Cloud Data Warehouse
Interactive Transcript - Enable basic transcript mode by pressing the escape key
You may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key
You've been learning about queries, indexes, and various strategies and tips for writing efficient SQL queries. Now, let's talk about some architectural factors that affect query performance for data warehousing workloads running on Amazon Redshift. Here, we'll look at how Redshift queries data and some considerations to keep in mind for table design that can optimize query performance. Redshift is designed to be a highly efficient data warehousing solution, and it achieves this through a combination of different internal architectural features, including columnar data storage, massively parallel processing, and data compression encoding schemes. Redshift is a columnar database, which means that it stores data column-by-column together on disc rather than row-by-row. This storage method is particularly efficient for analytical queries and OLAP workloads that require aggregating data over many rows, but only need to access a few columns for any particular query. By storing data in this columnar way, Redshift can quickly scan and retrieve the necessary data, significantly speeding up query performance. 
This is part of the reason why Redshift is especially well suited for data warehousing and large scale data analytics, where fast query performance over large data sets, and cost efficiency are important. Columnar storage also allows for better data compression. When you run a query, Redshift reads the compressed data into memory and decompresses it as needed, which means it can use more memory for actually analyzing the data, and that means your queries can run faster. You can save on storage space and cut down on the amount of data being read from the disc. Redshift also uses massively parallel processing or MPP. Which you've already learned a bit about last week with Joe. Let's quickly go over that again. 
Redshift is made up of a cluster that has multiple compute nodes and a leader node. Data associated with a workload is distributed across these compute nodes, and each node is responsible for storing a portion of the data, as well as processing queries on that data. Compute nodes are partitioned into what are called slices. A slice uses a portion of the compute nodes memory and disc space to process a portion of the data assigned to the node. With MPP, these compute nodes work together to handle query processing with each slice running the query on different portions of the data. When you submit a query to Redshift, the leader node parses the query, develops execution plans, and generates a series of necessary steps. It then compiles the code needed to perform the task and distributes it to the compute nodes for execution. 
Each slice processes its assigned portion of the data in parallel. Once a compute node is done with the work, it sends the results back to the leader node, which then aggregates the results into the final result set. This parallel approach ensures that queries are executed quickly. However, the performance of your queries may also depend on the number of nodes or node types you have in the cluster. Now, all that being said, to optimize for efficiency and performance, you can't rely on these built in factors alone. There are multiple key factors to consider related to table design. When you create your table, you can optionally define a sort key and distribution style, which will heavily influence overall query performance. 
Let's dive into the details around distribution style first. I've alluded to this one already when we talked about how MPP works with Redshift. The data gets divided amongst the compute nodes, and how that division happens is controlled by defining a distribution style for the table. There are two main goals when defining an appropriate distribution style. The first is to have a uniform or even distribution of the data across the nodes. Uneven distribution, otherwise known as data distribution skew, may result in some nodes doing a lot more work than other nodes. Since your query would be waiting on one node to churn through a large amount of data, it can lead to poor performance. 
Having an uneven distribution of data like this means that you can't take full advantage of the massively parallel processing capabilities Redshift offers. The other goal is to minimize data movement across the nodes. If a query running on a node involves joining tables or aggregating data that is distributed across multiple other compute nodes, some of the data may need to be redistributed between nodes over the network. This cross node data movement can result in increased network traffic, which in turn may slow down query performance and increase your query cost. To minimize this, it's important to carefully select a distribution style for your tables. A reasonable data distribution across the nodes ensures that related data is collocated on the same node. Reducing the need for this cross node communication. 
This optimization helps balance the workload and improve query efficiency by keeping as much processing as possible localized to each compute node. When you create your table, you can choose from the distribution styles, auto, even, key, or all. Key will let you choose a specific column, and then use that columns values to distribute the rows of data among the nodes. The leader node will distribute rows with the same key value to the same node. You may be a tempted to define a specific key for every table you create. But this requires some thorough analysis of the dataset, and you might not know exactly which column would make the most sense. In this case, you may instead want to use auto, which will let Redshift assign an optimal distribution style based on the size of the table data. 
If you can't provide a value for distribution style, it will default to auto, or you can use the even distribution style. Redshift will have the leader node distribute the rows across the nodes using a round robin approach, regardless of the values in any particular column. Even distribution is most appropriate when a table won't really need to have joins run against the dataset. Then there is also the all option. With all, a full copy of the entire table is distributed to every node. When using even or key, Redshift places a portion of the table's row on each node. But when you use all, Redshift ensures that every row is collocated for every join that the table participates in. 
This is nice for cases where you frequently join smaller tables with much larger tables. By having a full copy of the small tables on every node, you eliminate the need for data shuffling across nodes during join operations. However, the all distribution multiplies the storage required to store the table data by the number of nodes in the cluster. It takes much longer to load, update, or insert data into multiple tables. For this reason, Using the all distribution is really only appropriate for relatively slow moving tables. Now let's move on to sort keys and how that impacts Query performance. Redshift stores your data on disc in sorted order according to what you define as the sort key. 
When you submit a query to Redshift, the query optimizer uses the sort order to determine the most optimal query plans. The sort key you choose for a table impacts query performance because it determines how the data is physically organized on disc. When your queries filter or join data based on sort key, Redshift can more efficiently locate the relevant data, reducing the amount of data that needs to be scanned from disc. Choosing an appropriate sort key can minimize disc read operations and speed up overall query execution. For example, if you frequently query a sales table by order date, defining the order date as the sort key allows Redshift to efficiently scan only the necessary portions of the table that match the date range in your query. Similarly, if you often filter by customer ID, setting customer ID as a sort key can optimize those queries. You can think of this like how OLTP databases use indexes to speed up queries. 
In a similar way, OLAP databases like Redshift, use sort keys to speed up queries. Those are some considerations to keep in mind for table design. Up next, Joe will walk you through the upcoming lab where you'll compare performance between Row and Columnar databases, and I'll see you again soon. 


#### Additional Query Strategies
Interactive Transcript - Enable basic transcript mode by pressing the escape key
You may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key
In addition to knowing how queries are processed behind the scenes, understanding strategies for working with complex queries, such as query caching and comment table expressions, as well as other database maintenance techniques like vacuuming can help you improve the performance of your queries. Let's assume you're working with the DVD rental database. You saw in the previous course, and you want to calculate the total amount spent on three film categories, the family, drama and comedy categories. You can start by selecting the sum of the payment amounts from the payment sable. Then to get the names of the film categories from the category table, you'll need multiple joins. You need to join the payment table with the rental table based on the rental_id. Then you'll join the results with the inventory table based on the inventory_id. 
Next, you'll join with the film table based on the film_id. Then you'll join this with the film category table based again on the film_id. Finally, you'll join this with the category table based on the category_id. Once you've done all these joints, you'll be able to select the category name. Since you only care about the family drama and comedy categories, you'll filter the results using this ware clause. Then you'll group the results by the category name so that you can calculate the aggregated total spend for each film category. You can also order the results by the amount to sort the total spendings from lowest to highest. 
Running this query frequently on a large database containing this data could be very costly. To avoid rerunning the same query repeatedly and incurring significant charges, many databases, especially Cloud all app databases, allow you to cache query results to make them available for instant retrieval later on. By leveraging query caching, you can reduce a load on your database and enhance the user experience for queries that are executed frequently. Another piece of advice for writing complex queries is to prioritize readability, just like you would with any code. Readable queries are less likely to contain errors, simpler to debug and easier to collaborate on. You can enhance the readability of your queries by using Common Table Expressions or CTEs to create a temporary result set that you can reference elsewhere in your query. This is a concept you practiced in the first lab of this week. 
For example, let's say you want to get the names of the actors that acted in the film, Rocky War. You can create a CTE or a temporary table called selected_film that selects the film_id from the film table where the film title is Rocky War. Then you can create another CT called film actors_id that selects the actor_ids from the film actor table. Where the film_id is the one you got from the selected film CTE. Finally, you can write a main query to select the first and last names from the actor table, where the actor_id is the one you got from the film actor_id CTE. Compared to writing many nested sub queries, CTEs can help you structure complex queries in a more readable way, which makes it easier to understand the flow of your queries. Along with optimizing the queries themselves, you also want to optimize the use of DBMS resources to execute the query as efficiently and quickly as possible. 
Certain databases are designed to allow concurrent access to data while the data is being updated. When you delete or update a record in these databases, they create new records while physically retaining on disc the outdated data as pointers to the last state of the database. This can also help the transaction roll back to its previous date in case of any failure. However, as these outdated records accumulate and are no longer needed to be referenced, they can lead to table bloat. This is where the data space occupied on the physical disc far exceeds the actual data size. In addition to the wasted disc space, the database has to skip over many blocks to retrieve the required data, slowing down queries. Since a query optimizer relies on the internal statistics of the data on disk to generate query execution plans, outdated records can also lead to suboptimal or inaccurate plans. 
Similarly, indexes can also become inefficient as they accumulate entries for outdated data. And so to free up space for new records and allow for better query and index performance, you should remove these dead records using a process called vacuuming. You can vacuum a single table, multiple tables, or all tables in a database. Vacuuming is more critical for relational databases, such as Postgres and MySQL, because large numbers of transactional operations can cause a rapid accumulation of dead records. When you work in these systems, you need to familiarize yourself with the details and impact of vacuuming. With that, you now have more strategies to optimize the performance of equeries. We covered lots of ground with querying batch data. 
I'll see you in the next lesson to take a look at quering, streaming data. 



#### Queries on Streaming data
So far, we've been discussing how to query batch data, but as streaming data becomes more prevalent, you might find yourself needing to aggregate and join together streaming data as well. When querying your streaming data, you must adopt query patterns that reflect the real-time nature of this data. Let's say you want to ingest data from this streaming system and process a stream of data as soon as you receive it. You can use stream processing systems, such as Apache Flink and Spark Streaming, which enable you to apply SQL queries, even complex ones, continuously over your stream of data. Streaming platforms such as Kafka also support querying data in Kafka streams. With these systems, you can continuously aggregate the streaming data by applying something called a windowed query, which allows you to bound your queries using a window and then apply operations such as aggregation, adding, or removing data over that window. Let's take a look at three common types of windows, session, fixed time, and sliding windows. 
A session window is ideal for handling events that arrive at irregular times. It groups events that occur at similar times and filters out periods of time when there are no events. When using this type of window, you need to specify the maximum time gap allowed between events to identify when one session ends and another one begins. For example, let's say you're analyzing website clicks for each user and decide to set the time gap of an activity to be 5 minutes or more between your session windows. In here, you'll have three session windows because they are each separated by 5 minutes or more of user inactivity. Note that session windows are unique to each key, so in this example, each user gets their own set of session windows. Doing analytics on these windows could, for example, allow an analyst to do something like follow up with an email that has a coupon for a product that was viewed by the user in their last session window. 
To determine the session boundaries, the processing system starts with a new session window when the first event occurs. In this case, the first website clicked by a user. Then the system continues to accumulate arriving events for that user, as long as no events happen within 5 minutes of the previous one. Once there's a 5 minute inactivity period, the system closes the window, sends the consumer any specified aggregations like max, min, or average values, and then flushes the data. If no events for the user arrive later, the system starts a new session window. And so with session windows, the windows can extend to be of any size as long as events keep on arriving close to each other. Alternatively, you could aggregate the data for events over windows of fixed size, known as fixed time or tumbling windows. 
For example, here you have 3 fixed time windows, each lasting 20 seconds. The system processes all data arriving within each window, and then sends the aggregations as soon as the window is closed. This can be useful if you like to compute, for example, the total number of clicks that happen every 20 seconds. This is similar to traditional batch ETL processing, where you might run a data update job every day or every hour. However, the streaming processing system allows you to generate windows more frequently and deliver results with lower latency. With session and fixed time windows, the windows are non-overlapping. But with sliding windows, you can group events into windows of fixed time length that can overlap. 
For example, here you have 3 60 second overlapping windows generated every 30 seconds. This type of windowing can help you calculate things like a moving average within a time interval. In addition to aggregating streams of data, you can also join multiple streams, or combine a stream with batch historical data. The conventional way of joining multiple data streams is to transform each stream into a table and then join the tables in the database. But streaming processing systems are increasingly supporting direct stream-to-stream joining. So for example, you might want to join a stream of web browsing data with streaming data from an ad platform. Since those streams can be produced at different event rates and have different latencies, typical streaming join architectures rely on streaming buffers that can retain those events for a certain period of time. 
Events from the streams get joined in the buffer and are eventually emitted after the buffer's retention period passes. Aside from joining two streams of data, you might also want to join streaming data with batch historical data that's stored in a database or object storage in order to produce an enriched stream of events. For example, you might want to enrich product browsing events from an e-commerce website with product details and user demographic information. To do this, you might use a serverless function or a processing system to look up the product and user information in an in-memory database or object storage, then add the required information to the event, and finally output the enhanced events to another stream. For the last slab of this week, you'll work with the streaming data that you saw back in course 1. But now you'll apply time-based windowed queries to process this data. In the next video, Morgan will give you an overview of the Amazon Managed Service for Apache Flink that you'll use to accomplish this. 


#### Deploying an Application with Amazon Managed Service for Apache Flink
You just heard from Joe about how you can use tools like Apache Flink to query streaming data, and in the upcoming lab you will have an opportunity to get hands on with Apache Flink. When you're looking to run Apache Flink with AWS, you have multiple options to choose from. You can run Apache Flink on Amazon EMR as a yarn application. Or you can self host Apache Flink in a containerized environment using Amazon Elastic Kubernetes service or Amazon Elastic container service. These are what you might call the do it yourself options, but you can also choose to use a managed service like Amazon managed service for Apache Flink. And that's what I'm going to talk about here. Amazon managed service for Apache Flink runs Apache Flink on AWS. 
It provides the underlying infrastructure for your Apache Flink applications and creates a hosted, serverless environment for them to run in. It handles a lot of the heavy lifting for you, including provisioning, compute, setting up and managing AZ failover for resilience, automated scaling, and application backups. To give you a sense of how this works, I'll run through a demo on how to set up Amazon managed service for Apache Flink, and we'll cover some of the core concepts along the way. Here I am in the AWS management console and I will type Flink in the search bar and then select Amazon manage service for Apache Flink. Manage Apache Flink creates the hosted environment for you to run your Apache Flink applications. But you would write your applications locally, just like you would with any other programming project using the Apache Flink framework. Under get started, I can choose to either create an application or I can choose studio notebooks. 
Creating an application is what you would choose if you wanted to host your Apache Flink application and run it in a production environment. This involves defining the necessary resources, configuring the application settings, and deploying your Flink job. AWS then handles the underlying infrastructure, scaling and operational aspects, allowing you to focus on your application logic. On the other hand, studio notebooks are ideal for development and interactive data exploration. They provide a browser based interface powered by Apache Zeppelin that is integrated with Apache Flink. And it allows you to run stream processing applications using standard SQL, Python, and Scala. Using this interactive approach, you can experiment with your flink code, test different scenarios, and visualize results quickly and interactively. 
This is particularly useful during the development phase or for an ad hoc data analysis task. For the first part of this demo, I'll create a streaming application. Then later I'll create a studio notebook. On the next page, you need to select whether you want to start from scratch or from a blueprint. I am going to select a blueprint which will create all the resources you need to get started using AWS cloudformation. It will set up an Amazon Kinesis data stream as the source we want to analyze. Then it will set up the resources needed to send demo data into the stream. 
>> And deploy an application that will read from the stream, process the data, then send the process data to Amazon s three to be stored. I will select deploy blueprint and cloudformation will now deploy the necessary resources. This will take a few minutes to complete, so we will come back when it's done. All right, the cloudformation template is now done deploying and our demo application is now running. This demo application sends sample data for stock ticker prices through the kinesis data stream, simulating stock market data coming in real time. And then deploys a flink application that does a simple data transformation and then writes the data to S3. This blueprint gives you a link to the GitHub repo where you can explore the Apache flink application it deployed and this one was written in Java. 
A lot of this code is boilerplate and used to get things set up. So I want to direct your attention to the part of the program that actually does the transformation. In this run app with Kinesis source method, there is logic that sets up the source stream to be Amazon Kinesis data streams. Then it runs the transformation logic. This code reads stock data from an Amazon Kinesis stream into a flink data stream, filters the data to keep only stocks with a price of $1 or higher, and then writes the filtered data to an s three file. Now that you understand what the application is, let's head back to the AWS console where we can then select the open Apache Flink dashboard button. This opens the Apache Flink dashboard where there is one job running. 
If we select running jobs in the navigation, there is the blueprint job called Kinesis data streams to S3 Flink streaming app. Clicking that to view more details. We can then see a graph of the operators we defined for this job. We are reading the stock data from the stream using a flat map to filter out stocks with the price lower than $1, and then writing the filter data to an S3 file. The graph visually represents the flow of data through these operators, making it easier to understand the processing steps and how the data is being transformed and stored. If I select this operator here, you can see that 10,000 records have been processed. All right, so that was an example of running an Apache Flink application with a data processing and analysis job, pulling data from Amazon Kinesis data streams. 
As I mentioned earlier, another way you can use Flink on AWS is through studio notebooks, which can be great for data exploration or ad hoc analysis. So that's what I'd like to show you next. Join me in the next video to deploy a studio notebook using Amazon managed service for Apache Blink. 


#### Deploying a Studio Notebook with Amazon Managed Service for Apache Flink
In the previous video, we deployed a Flink application to read data from an Amazon Kinesis data stream, perform some transformations, and then write the data to Amazon S3. Here, we'll set up a similar deployment, but this time using a studio notebook. But first, I wanted to pause for a moment to just say a bit more about how Flink is working behind the scenes. The way Flink was able to connect to the Kinesis data stream and write to S3 was by using what are called connectors. Connectors provide code for interfacing with various systems, which include things like databases, message queues, and cloud storage services. For instance, you can connect to Amazon DynamoDB for working with NoSQL data, or use Amazon Kafka for stream processing with Apache Kafka topics. Flink connectors also support relational databases via JDBC, which allows for integration with databases like MySQL or PostgresQL running on Amazon RDS. 
These connectors enable Flink to interact with different data sources and destinations. Helping you with the creation of real-time data processing pipelines across different platforms and services. Now, let's create a studio notebook together using Amazon Managed Service for Apache Flink so you can see the difference between deploying a notebook versus an application. Back in the AWS console, I am on the managed Apache Flink dashboard, and this time, I will select Studio notebook and then I will select Create. I, again, will use a blueprint, and this blueprint will send demo data to a managed streaming for Apache Kafka or MSK topic, which will then be read by managed Apache Flink. We will then interact with the data using the studio notebook. I will deploy the blueprint and then come back when it's done deploying. 
Now, the demo notebook is ready for us to interact with the MSK topic. This demo is sending demo data around stock ticker prices to an MSK topic, and we can use Flink to interact with this data in real time. To open up the notebook, I can select Open in Apache Zeppelin. This is a browser-based notebook that enables collaborative, interactive data analytics. Zeppelin supports code written in languages like SQL, Python, and Scala. And you can visualize your data with different formats like tables and charts, as well as integrate with tools and frameworks like Flink. This is a popular tool with data scientists, analysts, and engineers to collaboratively develop and share insights from their data. 
So this is the Zeppelin notebook created by the blueprint. In Zeppelin notebooks, you use paragraphs to define units of work. Each paragraph can do things like run code, show text, or perform data visualizations, and they can be executed independently. This allows you to break down your data analysis or data processing tasks into more manageable steps, making it easier to develop, test, and debug your workflows. You can use different interpreters within these paragraphs to do things like run SQL queries, execute Python scripts, or interact with big data frameworks like Apache Flink and Spark. The first paragraph here is creating a user-defined function for generating stock ticker data. This first line that says %flink is specifying that the code will be executed using the Flink interpreter. 
Then we have some code that defines and registers a custom Flink function named RandomTickerUDF. That returns a random stock ticker symbol from a predefined list and provides a random value for the ticker price. This function can then be used in Flink SQL queries to generate random ticker values. Then we have another paragraph which defines a source table using a connector for the MSK topic. Then we also have a query that is doing a select star from this table and displaying the results. We could change this display to be other types of visualizations like a line or bar chart as well. Then finally, this last paragraph that the blueprint provided allows you to run the initial data generation again by using that function we defined earlier. 
This is here so that you can perform time window-based queries. Because if the data generation has already occurred and there is no data currently running through the stream, you would end up with empty results. So you can run this to run the sample data again so you can test out your time-based queries. You'll see an example of using window-based queries in the upcoming lab. Behind the scenes, this is using an AWS Glue database to store information about the data sources and destinations your scripts are using. We used a blueprint, so the creation of this database and the connection information was set up by the CloudFormation template. However, in the real world when you create your studio notebook, you would need to specify the AWS Glue database that contains your connection information to your data sources and destinations. 
Then when you want to access your data sources and destinations, you would specify the relevant Glue tables contained in the database. And the tables provide access to the Glue connections that define the locations, schemas, and parameters for your data sources and destinations. That is it for our quick rundown on Amazon Managed Service for Apache Flink. Coming up next, you'll get to put all of this knowledge to good use in the lab. As always, have fun and I'll see you later. 


#### Summary
Congratulations on finishing this third course of the data engineering specialization. So that's three courses down and one to go. In this course, we covered a lot of ground for the most complex stage of the data engineering storage. So nice work. In the first week of this course, we looked at the first two layers in the storage hierarchy, raw storage ingredients and the storage systems that are built from those raw ingredients. And in the labs, you compared the file, block, and object cloud storage options and got a chance to practice with a graph database. In week two, we looked at the evolution of storage abstractions from data warehouses to data lakes, and finally data lake houses. 
You set up partitions and a data catalog for data lake and got some practice creating a data lakehouse using AWS, lake formation, and Apache Iceberg. In the third week of the course, we dug into the details of the life of a query and how filtering, joining, and aggregating queries are a process behind the scenes. You got hands-on experience with advanced SQL statements, compared to the execution time of an analytical query performed on row-based storage versus columnar storage. And finally, use Amazon managed service for Apache Flink to perform time-based windowed queries on streaming data. With three courses completed so far in the specialization, you've acquired many of the core skills you need to become a successful data engineer. In the final course, we'll focus on how you can model and serve your data for analytics and machine learning use cases. I'll see you there. 




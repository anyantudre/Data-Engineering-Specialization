# Storage Abstractions

#### Data Warehouse - Key Architectural Ideas
Interactive Transcript - Enable basic transcript mode by pressing the escape key
You may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key
In your work as a data engineer, you'll find yourself ingesting data from many different source systems. One very common source you'll ingest from will be a database that's part of an OLTP system, where the data is structured in a way that's optimized for transactional workloads. OLTP systems have been around for decades. In the early days of data analytics, it wasn't uncommon for teams to be running analytical queries directly on a production OLTP database. As I've already mentioned a couple of times, running analytical queries on your production databases can be a recipe for disaster. And beyond that, attempting to do analytics directly on row based transactional data can be very inefficient and costly. Back in the late 1980s, Bill Inmon came up with the notion of a data warehouse to solve this problem. 
He described a data warehouse as a subject oriented, integrated, non volatile and time variant collection of data in support of management's decisions. This foundational definition underscores the role of the data warehouse as a central data repository designed to facilitate reporting and analysis. Let's break down this definition. Subject oriented means that a data warehouse organizes and stores data around key subjects or domains of the business, such as customers, products, sales, or finance. With a data warehouse, your focus is on modeling the data to support decision making rather than on transaction processing and recording. Integrated means that a data warehouse brings together data from different sources and ensures that it's stored in a consistent way with a predefined schema. Non volatile means that the data in a data warehouse is read only and cannot be deleted or updated in the general sense. 
This is useful for historical data analysis, where the principle of non volatility requires a data warehouse to preserve historical records. When initially loading data from source systems into a data warehouse, the data is captured and loaded as a snapshot. When subsequent changes and the source systems occur, either a new snapshot of the data is loaded into the data warehouse or just the changes are loaded. Byther way, the existing snapshot of data is not deleted or changed, it's kept in the data warehouse. This leads to the time variant characteristic, meaning that a data warehouse stores current and historical data. Data users can look at this historical data to observe trends across multiple subjects to support their business decisions. This is something you can't do with the OLTP source systems, which typically don't support preserving historical data or historical data analysis. 
Although technical aspects of the data warehouse have evolved significantly. This original definition still holds true today. Traditionally, you'll load data into a data warehouse from various sources by using an ETL pipeline, which is the extract transform and load ingestion pattern you saw previously. The typical process here, would be that you first move the extracted data into a staging area outside the data warehouse, which could be a three object storage, for example. There you would apply transformations to the data to clean it and standardize it. This is also where you structure the data according to a particular model to make the data useful for downstream users. We'll get into data modeling in detail in the next course. 
Next, you load the transform data into the data warehouse. Data warehouses are designed to serve the broader organization. But you can also serve a narrow set of users by loading the data into what are called data marts. You can think of each data mart as a more refined subset of the data warehouse that's designed to serve the specific needs of a single department or business function like sales, marketing, or finance. Unlike the comprehensive schema of a data warehouse, the data in a data mart often follows a simpler or de normalized schema that provides a more focused view of only the subset of data that's specific to a department. With data marts, you can also perform an additional stage of transformation beyond that provided by the initial ETL pipeline. Which can improve performance for analytical queries that require complex joins and aggregations. 
This CTL process is commonly used to keep data in your data warehouse in sync with your production databases as the source databases are continuously updated. When you extract data from production databases, instead of extracting all the data, you can use a change data capture or CDC process to identify and capture only change events like insertions, updates or deletions, and deliver those changes to your data warehouse. By only extracting incremental changes, you can minimize your impact on the performance of the source systems. So data warehouses were a departure from traditional OLTP systems. By extracting data from production databases, modeling to support analytical workloads, and then loading it into a separate data warehouse, you can direct the load away from the production systems and provide a better end user experience with improved analytical query performance. This made data warehouses a standard for online analytical processing or OLAP data architecture. The earliest implementations of data warehouses were based on a single monolithic server that limited their performance. 
As the amount of data grew steadily throughout the 1990s, traditional data warehouses couldn't keep up. Then the emergence of massively parallel processing or MPP systems enable data warehouses to scale. Data warehouses that implemented MPP were able to scan large amounts of data in parallel, achieving high performance analytical queries. But the systems were complex to configure and required effort and time to maintain. In the early 2010s, modern Cloud data warehouses, like Amazon Redshift, Google Big Query, and Snowflake, emerged and represented a significant evolution from the on premises data warehouse architecture of the past. The Modern Cloud Data Warehouse architecture separates compute from storage and expands the capability of MPP systems. This allows for scalable and efficient processing of large datasets and makes data analytics more accessible and cost effective to smaller organizations. 
In the next video, we'll take a closer look at the characteristics of Modern Cloud data warehouses. I'll see you there. 



#### Modern Cloud Data Warehouses
As a data engineer, you'll likely work with cloud data warehouses. In this video, we'll take a closer look at the factors that give cloud data warehouses greater processing power than traditional on premises data warehouses. Understanding these factors will help you design more efficient data warehouses and better manage scaling, performance and costs. Like I mentioned in the previous video, data warehouses typically implement Massively Parallel Processing, or MPP, which uses multiple processors to crunch large amounts of data. With cloud data warehouses, instead of needing to appropriately size an MPP system and spend millions of dollars upfront to set up the system, you can just spin up compute clusters on demand, scaling it up over time as data and analytics demand grows, or delete the clusters when they're no longer needed. Let's take a moment to look at the specific MPP architecture for Amazon Redshift. Other cloud data warehouses, like Google, Big Query and Snowflake use a similar structure but different implementation details. 
In redshift, a collection of computing resources is called a cluster. Each cluster is composed of one or more compute nodes that are managed by a leader node. These nodes have their own CPU, memory, and disk space. A compute node is further partitioned into node slices, where each slice contains a share of the node CPU, memory, and disk space. When you load data into redshift, the leader node manages how the data is distributed across the node slices. Then, when a client application sends a query request to the data warehouse, the leader node parses a request into a series of steps and forms an execution plan. Then, based on the plan, the leader node compiles the code and distributes it to the appropriate compute node slices that contain data that's relevant to the query. 
The slices work in parallel to complete their workload, then the compute node sends the intermediate results back to the leader node. Finally, the leader node aggregates the results and sends a final result back to the client application. As your workload increases, you can spin up more compute nodes or upgrade the node type to one with higher computing capacity. So in this sense, cloud data warehouses expand the capabilities of MPP systems and allow you to easily scale your data infrastructure to handle even petabytes of data in a single query. With the increase in processing power that comes with MPP, cloud data warehouses can also support the ELT or Extract, Load, and transform ingestion pattern. After you extract data from source systems, rather than transforming and modeling the data outside the data warehouse, you can load the raw, unprocessed data directly into a staging area within the data warehouse. Then you can leverage the massive computational power of the cloud data warehouse to transform the data. 
This results in faster ingestion so you can quickly provide your downstream stakeholders with data. Another change we've seen with cloud data warehouses is a shift from row based to columnar architecture. As you saw last week, column storage, along with data compression, facilitates higher performance of large scale analytical queries. Finally, in many cloud data warehouses, data is stored in object storage, allowing virtually limitless storage. This means that you can separate compute and storage, allowing you to manage and scale these resources independently to optimize for both cost and performance. Now, cloud data warehouses still have all the attributes of traditional data warehouses, namely that the data you store can be highly structured and modeled to enable analytical queries. Combining these attributes with the high processing power that comes with MPP, column storage, and the separation of compute and storage is what makes cloud data warehouses highly efficient at storing and processing data for high volume analytical workloads. 
Now, what if your company wants to store in query unstructured data like text, images, audio files, or videos? Modern data applications are no longer limited to analytics and reporting, they will often involve machine learning or exploratory analysis use cases that require direct access to a variety of unstructured data, not just data that's accessed by running SQL queries. That's where the data lake storage architecture comes into play. So join me in the next lesson to dive into the data lake paradigm. 


#### Data Lakes - Key Architectural Ideas
Imagine you're a data engineer at an e commerce company where you're tasked with bringing together structured data from a sales order database, semi structured customer records from a CRM, and customer reviews, sort of text, video, and audio files. You start with a data warehouse, but then you quickly realize that the semi-structured data and unstructured data don't fit into a fixed schema, and the data is coming at you at very large volumes. So instead of imposing tight structural limitations on your data, why not simply route all of your data, structured and unstructured, into a central repository? This is exactly the concept of a data lake that emerged in the 2000s up to the early 2010s. The name data lake itself came a little later, but the idea of a central repository for sorting large volumes of structured and unstructured data was a motivation for this new paradigm. Unlike data warehouses, data lakes don't require you to decide on a fixed schema or predefined set of transformations ahead of time. Instead, data lakes follow a schema on read pattern where the reader determines the schema when reading the data. 
The first generation of data lakes, which I like to call Data Lake 1.0, made solid contributions towards this promise by combining different storage and processing technologies. For storage, Data Lake 1.0 started with HDFS or Hadoop Distributed File System, but as the cloud grew in popularity, it became more common to see data lakes built on top of cloud based object storage. Like Amazon S three, this extremely cheap and virtually limitless storage capacity allows you to store massive amounts of data of any size and any type, creating a central source of truth for all the data in an organization. In terms of processing, when you need to transform or query the data, you could pick from many different technologies, including MapReduce, Apache Pig, Spark, Presto, and Hive, among others. Despite the promise and hype, Data Lake 1.0 had many serious shortcomings. The biggest downside was that data lakes in the 1.0 era commonly became data swaps, a place for organizations to dump their data with no proper data management. Without things like data cataloging and data discovery tools, users would struggle to find the data they needed and had trouble understanding how one piece of data related to another. 
Even if you could locate the data you needed, there was no guarantee on the data integrity or data quality. That is, you couldn't tell if the data was up to date or accurate. On top of that, the original data lake concept was essentially write only. Simple data manipulation language or DML operations you commonly use in SQL, like deleting or updating rows, were painful to implement and generally required users to create an entirely new table. This made it very difficult for organizations to comply with data regulations such as GDPR, which required them to be able to delete user records when requested to do so. Without schema management and careful data modeling. It was also very challenging to process the data that was stored in data lakes. 
The data was not optimized for querying in the same way that structured data in a data warehouse was. For example, common data operations like joins were a huge headache to code as Mapreduce jobs. But even with all these shortcomings of Data Lake 1.0, many organizations, especially tech companies like Netflix and Facebook, which is now known as Meta, that are heavily data focused, they found significant value in data lakes. These companies had the resources to build successful data practices and create their own custom tools for processing the data. But for many organizations, Data Lake 1.0 was an expensive disappointment. The good news is that in recent years, many tools and practices have emerged to help businesses better organize and query the data stored in a data lake. So join me in the next video to explore the characteristics of next generation data lakes. 


#### Next-Generation Data Lakes
In response to the shortcomings of the original data lake 1.0 that I described in the previous video, engineers developed approaches to more efficiently manage and find data stored in their data lakes. In this video, we'll take a look at some of these approaches, specifically data zones, partitioning, and data catalogs. To better manage data stored in a data lake, you can organize it into different zones, where each zone houses data that has been processed to varying degrees. Although there are no set rules for the number of zones or the naming of these zones, a common design pattern is to have three zones. The first zone is usually known as a landing or raw zone. When you load raw data into a data lake, it lands in the zone so that you can have a permanent record of the raw data ingested from source systems. Then, after you apply transformations to clean, validate, and standardize the data, as well as remove or mask any PII information, a copy of this transform data will be written to a second zone, commonly known as the cleaned or transformed zone. 
Next, you model the data by imposing business logic and applying further transformations to it. Then you write the transformed data to a third zone, known as a curated or enriched zone. Data in this zone should abide by the organization's standards and is ready for consumption. You typically store data in the cleaned and curated zones using open file formats like Parquet, Avro, or ORC to make storage on disk more efficient. Since these formats are open source, they also allow a wide range of analytics engines and machine learning systems to directly access the data. Like I said, the number and naming of these zones can vary. You might design a data lake with only the raw or curated zones, or maybe you need to perform more complex transformations to the data to comply with strict regulations, in which case you might have four, five, or more zones to handle intermediate storage stages. 
In any case, organizing data into different zones allows you to apply appropriate data governance policies on each zone and ensures that data users are consuming data at the appropriate level of quality and readiness for their specific needs. Now, to improve query performance with data lakes, you generally want to take the data from cleaned or curated zones and sort it as partitions. Data partitioning is a technique where you divide a dataset into smaller, more manageable parts based on some sort of criteria, like time, date, or location recorded in the data. That way, when you query the data, the query engine only needs to scan the partitions that contain the data relevant to the query, resulting in faster query performance. Finally, to address a challenge of data discoverability with data lakes, you can create a data catalog, which is a collection of metadata about the datasets. This centralized metadata allows data users to search for database on things like the data owner, data source, partitioning information, business definitions for the columns, and much more. The catalog also records and maintains a schema of the datasets, including changes over time. 
And so the data catalog is a critical feature that provides everyone in the organization a common understanding of the data structure and meaning. So with these enhanced data management features and search capabilities, you can use a data lake to store, manage, and process large amounts of data of many different types. Despite these efforts to make data lakes more organized and searchable, historically, organizations still needed multiple storage systems to meet their business needs. They wanted to leverage the low cost storage of data lakes to store large amounts of data for machine learning applications. And they also wanted to leverage the superior query performance of data warehouses for analytical use cases. So they would first ingest and process data in a data lake so that there's a single source of truth for all the data. Then they would take a subset of the data that needs to be queried frequently and loaded into a data warehouse to support low latency query performance. 
But this solution is expensive because you have to continuously move data with an ETL pipeline from your data lake into your data warehouse, which have higher storage costs. And each step of the ETL process can introduce bugs or failures, causing issues with data quality, duplication, and consistency. To solve these challenges, a new storage architecture known as the Data Lake House was created. This new architecture aims to combine the benefits of a data warehouse and a data lake into one unified architecture. Before we take a look at this new architecture, I'll walk you through the upcoming lab, where you'll get some hands-on practice partitioning data and creating a data catalog to improve data retrieval from a data lake. Then, after the lab, join me in the next video to take a closer look at the data Lake House architecture. 


#### The Data Lakehouse Architecture
You can think of a data lakehouse architecture as a data lake with additional features built in to create an experience that is similar to a data warehouse. It aims to combine the flexible and low cost storage benefits of a data lake with the superior query performance and robust data management of a data warehouse. This enables analytics and reporting applications, as well as machine learning and big data processing use cases. Let's take a closer look at the key architectural components and characteristics of a data lakehouse. At its core, a data lakehouse is very similar to a data lake. It uses a single storage layer built on top of object storage to store large amounts of data of any type. You can organize a storage layer into different zones, like you saw in the previous video, to facilitate data governance and ensure better data quality. 
Databricks, which is the company that first introduced the notion of a data lakehouse, refers to a storage layer organized by data zones as the medallion architecture, labeling the raw data zone as bronze, the clean data zone as silver, and the curated data zone containing modeled and enriched data as gold. Transform data is stored in the silver and gold zones, and written in an open file format, typically parquet for more efficient storage and allow analytics and query engines of all kinds to access the data directly. In addition to the data lake characteristics, lakehouses include the data management features found in data warehouses. They enforce schemas at the storage level to ensure that the data you load adheres to specified formats and quality standards, and they also support schema evolution. Data lakehouses typically adhere to ACID principles, meaning that transactions are atomic, consistent, isolated, and durable. This enables your data users to concurrently read, insert, update, and delete data, while ensuring it's reliable for analytical processes. Lakehouses also have built-in data governance and security features, such as robust access controls, data auditing capabilities, and data lineage tracking. 
You can also connect to a data lakehouse using connector APIs, then use SQL to perform incremental updates and deletions to your datasets. These are critical features that enable compliance with data regulatory and privacy rules. Since lakehouses retain old versions of files and metadata, you can also roll back to or access any version of your historical data as needed. By integrating the best capabilities of data warehouses and data lakes, data lakehouses provide a unified architecture that supports everything from SQL applications to business reporting to machine learning. Since the inception of the data lakehouse, various cloud and software providers, along with open source organizations, have been creating new products to help organizations move toward a data lakehouse architecture. Join me in the next video to take a closer look at some of the details of data lakehouse implementation. 


#### Date Lakehouse Implementation
Ever since the inception of the data lakehouse, we've seen an interesting fusion happening with Cloud data warehouses and data lakes. Cloud data warehouse providers have started integrating features typically associated with data lakes and meanwhile, data lake technologies have started embracing characteristics typical of data warehouses, like enforcing and managing schemas, as well as SQL functionality. In this video, we'll look at Data Lakehouse Implementation using open table formats. In the next video, Morgan will walk you through how you can implement a lakehouse on AWS. When it comes to lakehouse implementation, a number of open table formats have been developed to support the idea of a more transactional data lake. These open table formats include Databricks Delta Lake, Apache Iceberg, and Apache Hudi, which stands for Hadoop Update, Delete, Incremental. Open table formats like these are specialized storage formats that add transactional features to your data lake house. 
This allows you to easily update and delete individual records in the storage layer that's built on top of an optic storage data lake, while supporting the ACID principles traditionally found in a data warehouse. How do open table formats work exactly? Well, in short, they provide a logical abstraction that says on top of your store data. When you perform an operation on your data tables, such as inserting, updating, or deleting a record, the open table formats track those changes and store them as a series of snapshots that reflect the state of the data at a given time. These snapshots enable a feature known as time travel, where you can any previous version of a table by specifying a timestamp, as well as roll back the table to a previous version to recover from any incorrect changes made to a table. This also supports schema and partition evolution, meaning that you'll still be able to query the data, even if you make schema changes, like adding or deleting a column, or changing how you partition the datasets. Since open table formats are open source, they also enable different quarry engines to access the data stored in the data lake house. 
You can use whatever processing tools you like that are suitable for your use case without having to duplicate the data and restructure it into another format. In your work as a data engineer, you might encounter Databricks Delta Lake, Apache Iceberg, and Apache Hudi. These technologies all offer the same features of schema evolution and time travel, but they usually differ in terms of the underlying implementation details. For example, here's how Iceberg works. Similar to a data lake, there's a data catalog and a data storage layer, which contains files written in Parquet for efficient storage but then there's a metadata layer that says between the catalog and the storage layer. Whenever you update or create a data file, Iceberg creates a new manifest file to keep track of these data files and additional details about the metadata of each file. Then a new manifest list is created to keep track of the information about the location of the manifest files, which snapshot, each manifest file is a part of and partitioning information. 
Finally, a new meditative file written in JSON format is created that contains a new snapshot that points to the new manifest list. These files include information like the current table schema, partitioning information, snapshots, and which snapshot is the current one. Within the Iceberg catalog. There's a pointer for each table stored in the lake house that references the table's most current metadata file. This pointer gets updated whenever a new metadata file is created. When you run a query, the pointer in the catalog tells a query engine which metadata file is the current one based on the table that is being queried. The query engine then retrieves a manifest list for the current snapshot in the metadata file, then the relevant manifest files, and finally, the relevant data files. 
This metadata layer helps Iceberg determine which data files need to be read and it ignores files that are not relevant to the query. This significantly speeds up query performance. When it comes to choosing between a data warehouse, a data lake, or a data lakehouse, it's really about choosing the right storage abstraction to support your organization's needs. If you're at an early stage company that only needs to process small amounts of structured data for analytics and reporting, you might be able to get away with connecting your BI tools directly to a read replica of your production database. However, as the volume and sources of data grow, you'll want to use a Cloud data warehouse to bring together structured and semi structured data from multiple sources and allow your data users to query current and historical data for analytics and reporting without adding too much extra load on your production databases. If your organization requires massive amounts of data, especially if it's unstructured, say for machine learning applications, then you might want to consider implementing a data lake architecture to save on storage costs. In this case, you can choose to evolve your storage architecture into a data lake house by adding on data management and discoverability features to enable both machine learning applications and low latency analytical quarries. 
As you've seen, the technical architectures of Cloud data warehouses and data lakes have started to converge. I think this trend of convergence will only continue. The data lake and the data warehouse will still exist as different architectures, but in practice, their capabilities will blend together. So in the future, instead of choosing between a warehouse or a lake, you will have the option to actually choose a converged data platform based on your specific circumstances and data use cases. Next up, Morgan will walk you through how you can implement a data lake house in AWS using AWS lake formation and after that, I'll give you a walk through of the upcoming lab where you'll get a chance to create your own data lakehouse architecture. 


#### Lakehouse Architecture on AWS
You've learned a lot already from Joe about the differences between data lakes, data warehouses, and data lake houses, and you've completed a lab using AWS Glue and Amazon S3 to set up a simple data lake. It's common for organizations to start with a simple data lake like this and then evolve over time to use a more mature solution like a data lake house. Now it's time to discuss how you can architect a data lake house using AWS services, including AWS lake formation, and Amazon Redshift Spectrum. At its core, AWS lake formation is designed to simplify the process of building and managing data lakes. Traditionally, setting up a data lake or lake house involves a lot of manual steps, defining storage, setting up access controls, cataloging data, and managing permissions to data assets. This can be complex and time consuming, and lake formation automate some of these tasks, making it simpler for you to get started. Here's how it works, you start by identifying your existing data sources, like Amazon S3 or relational and NoSQL databases. 
Then you can use lake formation to move that data into your data lake. After that, you use lake formation to crawl through the data, catalog it, and get it ready for analytics. Finally, you can give your users secure self service access to this data with their preferred analytics tools. It's a streamlined way to make sure everyone in your organization can easily find and use the data they need. You may recognize some of these tasks as things you've done before with AWS Glue and that is because lake formation is actually built on top of AWS Glue and IAM, so it's using features of Glue you are already familiar with like glue jobs, workflows, and crawlers to perform these tasks. When you're using lake formation, you can create things like workflows, but also manage these features directly in the Glue console. With a typical data lake or lake house, there are many AWS services interacting with each other and with end users accessing different datasets. 
Along with that comes a fair amount of overhead for managing permissions and so how part of lake formation helps automate the creation of data lake is managing complex fine grained permissions. Lake formation also provides fine grained access control on the data stored in S3, and the metadata in the data catalog. You can centrally manage permissions and IAM policies to streamline the process of governing and sharing your data internally and externally for both analytics and machine learning applications. Now that you've got a high level understanding of lake formation, let's review a diagram of an example of data lake house architected with AWS services. We'll dive deeper into various aspects of this architecture as we go along. First, you have the data sources, which from a data engineer's perspective, are often out of your control. These are things like databases, file shares, SAS applications, and more. 
Then you have the ingestion mechanisms, which would ingest data into the data lake house. As you have learned in previous courses, there are various services you can use for data ingestion, including Amazon Kinesis Data Streams, Amazon Data Firehose, AWS Data Sync, AWS database migration service, and Amazon AppFlow. Also, AWS lake formation can manage some data ingestion tasks through AWS Glue. All that ingested data needs to be stored somewhere, so n ext to the ingestion layer, there is the storage layer that uses Amazon Redshift and Amazon S3. Above that, sits the processing layer, where you would read data from the Lake house storage layer and transform it for downstream consumers. You would use services like Amazon EMR or AWS Glue, Amazon manage service for Apache Flink, or SQL data processing on Amazon Redshift. Then there's the catalog layer that uses lake formation to provide a central catalog to store and manage metadata for all datasets hosted in the storage layer. 
In this layer, you can also use lake formation to manage permissions and provide fine grained access control. Then finally, the rightmost layer here for the lake house architecture is the consumption layer, which provides AWS services you may use to consume the data, including but not limited to Amazon Sage maker for machine learning use cases, Amazon QuickSite for business intelligence and data visualizations, and Amazon Athena and Amazon Redshift Spectrum for querying data in the lake house. We will spend more time exploring Amazon Redshift Spectrum in the next video. Since you have already learned a lot about data sources and ingestion use cases, I won't focus on those more right now. But I would like for you to join me in the next video for a closer look at the storage, processing, catalog, and consumption layers of these data lake house architecture. I'll see you there. 


#### Implementing a Lakehouse on AWS
In the previous video, I gave you a high-level overview of how various AWS services can be combined into a series of layers in a data lakehouse architecture. As a reminder, here's how that architecture looks. You have the source and ingestion layers on the left over here, then the storage processing and catalog layers in the middle, and finally, the consumptions layer over here on the right. I'd like to start off with a focus on the storage and catalog layers. Using S3 for storage is common for data lakes. For data lakehouses, it's common to use both S3 and Redshift as the storage layer. Typically, S3 provides storage for structured, semi-structured, and unstructured data, whereas Amazon Redshift stores highly curated, structured or semi-structured trusted data that fits into predefined schemas. 
This dual storage approach leverages the cost efficiency and scalability of S3 for large, structured, and unstructured datasets, while utilizing Redshift for high performance analytics on more structured datasets. Part of the reason for creating a data lakehouse is that you want to be able to analyze the data that is stored across S3 and Redshift at the same time. You could, of course, write ETL jobs that move data from S3 to Redshift on a recurring basis, but creating a data pipeline for this effort might be costly over time and can lead to data redundancy. It also creates an opportunity to introduce bugs or issues into the data. Anytime you are moving and transforming data, you are opening the door for mistakes to be made, which can impact data quality and availability. With that being said, it would be nice if you could somehow integrate your data lake with your data warehouse natively. This is where Amazon Redshift Spectrum comes in, which works as an integration between S3 and Redshift for querying data. 
This is represented here in the storage layer, but it is a key part of the consumption layer for a data lakehouse on AWS. Redshift Spectrum allows you to run queries on data stored in S3 without having to load it into Redshift first. This is great because it eliminates the need for complex ETL pipelines to move data between your data lake and your data warehouse. This helps you make a data lakehouse possible by integrating these two data storage systems. Using something like Redshift Spectrum to be able to query data in both the data lake storage and the data warehouse storage is definitely a preferred method over moving data out of S3 using ETL processes into Redshift for querying. More on this when we get to the consumption layer. Next up, we'll move on to the layer above the storage layer, which is the catalog layer. 
A central data catalog is used to provide metadata for all datasets in your lakehouse in a single place and make it easily searchable. This is extremely important for self-service discovery of data in your lakehouse. You learned earlier about a data swamp. You don't want your data users to be wading through murky waters. Instead, you want to be able to provide clarity about the data in the lakehouse. In this case, we're using Lake Formation, which uses Glue behind the scenes to create the data catalog to store metadata for all datasets hosted in the lakehouse. Lake Formation coordinates Glue crawlers to identify datasets, and then it persistently stores metadata, including schema information, partition information, and data location in the Glue data catalog. 
Now, it's important to remember that it's common for datasets in the storage layer to have evolving schemas and increasing data partitions over time, so populating the metadata catalog isn't a one-and-done job. It's something that has to be maintained and kept up to date. To automatically keep the catalog up to date, you can configure AWS Glue to periodically crawl through the lakehouse storage layer to discover new or updated datasets and extract their metadata, which is then stored in a table in the catalog. While Lake Formation and Glue are great for managing and cataloging your data, handling, evolving schemas and large datasets efficiently is where Apache Iceberg tables may come into play. You learned in an earlier video about iceberg tables and how they make it easier to make changes to your data schema without disrupting existing processes or underlying data. This is made possible in part through schema and data versioning, which allows users to track changes to data over time. With versioning, you can use the time travel feature to access and query historical versions of data and analyze changes to data between updates and deletes. 
Lake Formation also supports Iceberg tables, and you can create Iceberg tables that use Parquet format in the AWS Glue data catalog. Next up, we'll move on to the consumption layer. I want to spend some more time covering Redshift Spectrum and Amazon Athena. You'll have an opportunity to use Athena in the upcoming lab. Though you won't be using Spectrum, it is commonly used in data lakehouses on AWS, so you should understand the basics. Redshift Spectrum enables Redshift to present a unified SQL interface for data consumers that can accept and process SQL statements where the same query can reference and combine datasets hosted in the data lake or S3, as well as the data warehouse storage or Redshift. This means that by using Redshift Spectrum, you can reduce data latency. 
In other words, by querying data in place, you can get insights faster without waiting for data to be moved or transformed. Redshift Spectrum queries use massive parallelism to run queries against large datasets, and a lot of the processing occurs in the Redshift Spectrum layer. That means most of the data remains in Amazon S3. After your Redshift Spectrum tables have been defined, you can query and join the tables just as you would with any other Amazon Redshift table. Multiple Redshift clusters can also query the same dataset at the same time in Amazon S3 without the need to make copies of the data for each cluster. Using Redshift Spectrum, you can do things like keep large volumes of historical data in the data lake and ingest a few months of hot data into the data warehouse using Redshift Spectrum, or you can create enriched datasets by processing both hot data from Redshift and historical data from S3 without needing to move data in either direction. You can also more easily offload volumes of large historical data from the data warehouse into S3, which provides more cost effective data lake storage while still being able to easily query the data as a part of Amazon Redshift queries. 
Then there is Amazon Athena. Athena makes it possible to query data in S3 directly using standard SQL. There is no need to load the data into another system to query it using SQL. Instead, you can create tables using Athena and query it directly. Athena is serverless, so there is no infrastructure to set up or manage, and you pay only for the amount of data scanned by the queries that you run. You will get a chance to try this out in the upcoming lab. Athena also supports something called federated queries, which allow you to query data that's outside of S3. 
It supports a wide range of data sources for federated queries, including Redshift. The Amazon Athena Redshift connector enables Athena to access your Amazon Redshift tables. You can write queries that pull in data from your data warehouse. For consumption using SQL, you can query your datasets in S3 and Redshift using Athena and/or Redshift Spectrum, which both can use the schema stored in the Lake Formation catalog and apply it by following the schema on read approach you learned about earlier in the course from Joe. That's an example of a data lakehouse architecture on AWS using AWS Lake Formation and other services, including Amazon Athena and Amazon Redshift Spectrum. In the upcoming lab, you'll get hands on with some of these services. Have fun, and I'll see you soon. 


#### Summary
This week, we walk through the evolution of storage abstractions from traditional data warehouses to modern Cloud data warehouses, and then to data lakes and finally, data lakehouses. You saw how the data lakehouse architecture aims to combine the advantages of both the data warehouse and the data lake to support the growing data needs of many organizations. Understanding the key concepts of each architecture enables you to choose the most appropriate storage solution based on your organization's needs. Remember that modern Cloud data warehouses can be used to store data for analytical workloads and reporting use cases. They enable low latency query performance by leveraging the massively parallel processing power of Cloud computing. But data warehouses typically come with higher storage costs. On the other hand, data lakes are built on low cost object storage to store large amounts of structured and unstructured data, supporting applications that require massive amounts of data, such as machine learning and big data processing. 
But without the proper data management features or data discovery tools, your data lakes can easily become unusable data swamps. In the first lab of this week, you saw how you can mitigate this challenge by creating a data catalog for the data sets stored in your data lake, and you partition the data to improve data retrieval. Finally, data lakehouses combine the scalable, low cost, and flexible storage capabilities of a data lake with the structured querying and data management features of a data warehouse to provide a unified platform that supports both low latency analytical workloads and machine learning. In the lab assignment, you created a data lakehouse using LakeFormation and Iceberg tables. As I said earlier, existing data warehouse technologies are increasingly incorporating features that allow them to also function like a data lake, and data lake technologies are incorporating features that allow them to also function like a data warehouse. In your work as a data engineer going forward, I think it's likely that you'll see the distinction between data lakes and warehouses and lakehouses start to fade away in favor of a set of tools that allows you the flexibility to optimize your storage solutions to the needs of your organization. Then next week, we'll dive into queries. 
You'll see how queries work under the hood and we'll explore strategies for improving query performance. I'll see you there. 


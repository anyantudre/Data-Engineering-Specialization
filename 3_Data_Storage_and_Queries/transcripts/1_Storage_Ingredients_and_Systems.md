# Storage Ingredients and Storage Systems
Interactive Transcript - Enable basic transcript mode by pressing the escape key
You may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key
Welcome to this third course of the data engineering specialization. In completing the previous two courses, you've already accomplished a lot. In the first course, you got a high level overview of the field of data engineering, including the data engineering lifecycle. A framework for thinking about your work as a data engineer, and also practice building data engineering solutions. In the second course, you took a deep dive into source systems, ingestion, DataOps, and orchestration. With these two first courses, you've already built up a powerful toolkit of knowledge and skills for your work as a data engineer. I'm here again with Joe Rees, your instructor for these courses. 
It's great to be with you again and maybe say a bit about what learners will see next in this course. >> Sure thing. Thanks, Andrew. So the first two weeks of this course are all about data storage. Then in the third week, we'll talk about queries. At that point, I can imagine you might be wondering a whole course on storage and queries? Well, is this really necessary? 
Believe me, while storage inquiries might seem straightforward, at first glance, they are anything but straightforward. Would you agree with this, Andrew? What do you think about storage inquiries? >> I think storage inquiries are not straightforward at all. >> No. >> There are many decisions in how to store your data that will affect how you can use the data efficiently afterward. For example, what type of storage from long-term storage that's cheap, but it's practical to access only intermittently to data storage using a relational database or a key value store to storing it in memory. 
And then also the design of the database schema, and maybe even deciding which data centers that is, in what parts of the world you decide to store your data. All this can drive both performance as well as impact how you have to affect with registry concerns such as data sovereignty. For example, I was recently just working on a project with a semiconductor company that couldn't let the data leave their country. So we had to architect a system that we wrote here in the United States that ran only in that country and shared only aggregate results with our US team. And that's just one example, the needs of different companies are so varied. This has led to a wide range of tools and frameworks that have been developed to handle these very different needs. >> That's right, so lots of complexity with storage and queries. 
So as you've seen in previous courses, you can think of data storage as a sort of hierarchy, where you have the raw ingredients of storage at the bottom. Including the physical elements of storage like magnetic disks, solid state drives, and RAM, as well as the less tangible elements of storage things like CPU compression, networking, and serialization. On top of that, you have storage systems built from these raw ingredients, things like databases and object storage. And on top of that you have storage abstractions built from these storage systems. So things like data warehouses, data lakes, and data lake houses. Then, when it comes to querying the data you have in storage, there are a wide range of tools and techniques you need to be aware of as a data engineer to ensure your systems are working efficiently. >> That sounds great, I'm excited about all this that you cover in this course. 
And I think you find that in your work as a data engineer, having deep expertise in storage solutions and queries will help you be more successful at every stage of the lifecycle. >> Great, let's get started. 

#### Overview
Interactive Transcript - Enable basic transcript mode by pressing the escape key
You may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key
Storage is arguably the most complicated component of the data engineering life cycle. That's because you'll store data many times as it moves through the life cycle, and the storage solutions you choose will impact everything from the cost, to the performance, to the end user experience of your data systems. Data storage really spans all the stages of the lifecycle from the source systems that are out of your domain of control as a data engineer to ingestion, transformation, and finally serving data to your end users. In this course, we'll focus on the storage you'll directly handle as a data engineer, from ingesting data on through to serving. To choose the proper storage solutions for your data architecture, you need to know the characteristics of your data, including things like its type, format, size, and how it will be accessed and updated by various stakeholders at different points in time. Recall that you can think of storage as a sort of hierarchy. At the bottom layer, you have the raw ingredients that comprise any storage system. 
These are components like the magnetic disks, solid sate drives, and memory that use a physically stored data, along with the processes needed to store and transmit data, such as networking, serialization, and compression. You typically don't interact directly with these physical storage components or processes. Instead, you interact with the storage systems that are built from these raw ingredients. A storage system consists of an internal management system that organizes your data and allows you to interact with the stored data. You already learned about some of these storage systems, such as databases and object storage in the context of store systems in Course 2. Back then, we mainly discuss OLTP systems for processing transactions, which focus on performing read and write queries with low latency. However, storage systems that support transactional processing are different from those needed to support online analytical processing or OLAP systems. 
These are systems that focus on applying analytical activities on data like aggregation, and summarization to make business decisions. Nowadays, we have storage systems like graph and vector databases that are more specialized and can support machine learning analytical use cases. At the top of the storage hierarchy, storage systems are assembled into storage abstractions, including Cloud data warehouses, data lakes, and data lakehouses. In Week 1 of this course, we'll focus on these first two layers, the raw ingredients, and storage systems. You'll dive deeper into the characteristics of physical storage technologies and look at the technical details of serialization and compression algorithms. Then we'll explore the cloud storage paradigms of block, object, and file storage. We'll also cover distributed storage systems. 
Finally, we'll discuss the details of data storage in various types of databases. You'll compare the performance between row and column-oriented databases to understand their use cases in OLTP and OLAP systems. Then you'll explore how data is stored in NoSQL graph and vector databases. Similar to what you did in Course 2, when exploring relational and NoSQL databases as source systems, in Lesson 2 of this week, you'll gain hands-on practice using the Cipher language to query a Neo4j database, which is a graph database with vector search capabilities. The theme of this first week is to assess the trade offs between storage costs and performance at the raw ingredient and storage system levels so that you can start to make informed storage decisions when designing your data architectures. In Week 2, we'll focus on the top layer, storage abstractions. You'll learn to choose the appropriate abstractions for storing the data your stakeholders need through the ingestion, transformation, and serving stages. 
In the last week of this course, we'll look at how queries work to retrieve stored data, how the different storage solutions affect query performance, as well as techniques for improving query performance. Even though this course has only three weeks, we have a lot to cover. Join me in the next video to get started with the raw ingredients of data storage. 



#### Storage Raw Ingredients- Physical Components of Data Storage
Like I said before, the raw ingredients used to physically store data are at the heart of all data storage systems. As a data engineer, you need to be aware of the characteristics, performance, data durability, and cost of these raw ingredients in order to select the storage system that is appropriate for your end use case. As data works its way through the data pipeline, it frequently passes through persistent storage mediums or disk like magnetic disk drives or solid-state drives. Also passes through volatile memory, like RAM and CPU cache. In this video, I'd like to compare these different raw ingredients to help you understand the differences between them. Magnetic disks often referred to as hard disk drives or HDDs, use rotating platters coded in magnetic film to store data. Magnetic disks work like old record players, where you need to move the stylus or needle back and forth to locate the right track on the record. 
As the record spins, the stylus reads the vibrations from the grooves to generate music. With magnetic disks, each platter contains circular tracks that are broken up into storage units called sectors. Together, the track and sector number create a unique address to organize and locate data. When you perform a write operation, a read/write head magnetizes the film to physically encode binary data at a particular address by changing the magnetic field to point in one direction to store a bit representing a one, and in the opposite direction to store a bit representing a zero. When you read the data, that same read/write head detects the magnetic field at the specified address and outputs a bit stream. Solid-state drives or SSDs, on the other hand, store data as electrical charges in flash memory cells. A charged cell represents a one bit and an uncharged cell represents a zero bit. 
Since they eliminate the mechanical parts of magnetic disks, SSDs can read and write data much faster through purely electronic means. How do magnetic disks compare to SSDs in terms of performance? The latency or total time it takes to fetch data on a magnetic disk depends on the seek time, which is the time it takes for the read/write head to physically locate the appropriate track and the rotational latency, the time it takes for the write sector to rotate under the read/write head. Both of these mechanical operations have physical limitations. At the time of this recording, a typical commercial magnetic disk drive rotates at around 7,200 revolutions per minute, which implies an average of four milliseconds of latency when fetching data. A magnetic disk drive can only perform a maximum of several hundred IOPS or input/output operations per second. It's much faster to read data through electrical charges in an SSD. 
Newer SSDs can typically perform up to tens of thousands of IOPS with a data fetch latency of about 0.1 milliseconds. This makes SSDs better for random access, meaning they can read or update data from any location very quickly. In terms of the actual data transfer speed, magnetic disks can read and write up to 300 megabytes of data from disk to memory or RAM in a second, while SSDs can be more than 10 times faster than that. You can achieve even better read and write performance through distributed storage systems and parallel processing. For example, you can distribute data across many magnetic disks and clusters and read from these clusters simultaneously. In this case, your transfer speed will primarily be limited by the network performance and less so by the physical limitations of the disk itself, or you can scale SSDs by slicing storage into partitions with numerous storage controllers running in parallel to handle more data transactions simultaneously. With a parallel processing approach, SSDs can reach a transfer speed of many gigabytes per second. 
We'll dive into distributed storage and parallel processing later this week. Now that we've compared magnetic disks and SSDs in terms of their performance, let's consider their costs. Commercial magnetic disks are typically 2-3 times cheaper than SSDs for storing the same amount of data. That's why even with slower data transfer speeds and higher latency, magnetic disks still form the backbone of the bulk of data storage systems. When considering the choice between these two storage mediums, my advice is to choose magnetic disks as a more cost efficient option if you require infrequent data access in blocks of one megabyte or more at a time, and where your applications don't need super fast read and write speeds. On the other hand, SSDs are commonly used in commercial deployments of OLTP systems because they allow relational databases to handle thousands of transactions per second. However, SSDs are not always the best option for analytical storage because of their higher costs. 
Let's switch gears to take a look at the volatile memory ingredients, namely random access memory or RAM and CPU cache. In order for the CPU to process data, you need to transfer the data from persistent disk storage, such as SSDs and magnetic disks to RAM. RAM is typically attached to a CPU, so it's very fast and can better match the processing speed of CPU than that of disk storage. Now, the exact performance metrics for RAM move fast, no pun intended, and they can change very quickly. But at the time of this recording, RAM offers a data transfer speed of about 100 gigabytes per second and a very low data fetch latency of about 0.1 microseconds, which allows it to perform millions of IOPS. Now, these metrics can vary significantly based on hardware and configuration specifications, but RAM is not all powerful. Because it's attached to a CPU, it's more expensive. 
As of now, RAM is usually 30-50 times more expensive than SSD per unit of stored data. It's also volatile, meaning that if you were to lose power, the data stored in RAM could be lost in less than a second. You usually only use RAM to store the code that the CPUs execute and the data that's code directly processes, and nothing that needs to persist over time. This makes RAM good for caching, data processing, or indexing, which we'll discuss later in this course. Several databases treat RAM as a primary storage layer because it allows very fast read and write performance. In these applications, you should always keep in mind the volatility of RAM, even if data memory is replicated across a cluster, a power outage that brings down several nodes could cause data loss. Another type of memory that's even faster than RAM is CPU cache, which is located directly on the CPU processing chip. 
You want to use CPU cache to store frequently accessed data for ultrafast retrieval during processing. Because of its location, it has a data fetch latency of about one nanosecond, and a super high data transfer speed of about one terabyte per second. Caches are not only used for CPU cache, but you can also use them in multiple applications to sore frequently and recently accessed data in a fast access layer. For example, you can use a browser cache to store downloaded web resources so that you can load a web page faster. You can also use a database cache to store the results of frequently used queries. With that, we cover the performance and cost trade offs between the different ingredients that physically store data. As a data engineer, having a solid understanding of these trade offs can help you evaluate different storage technologies to ensure that they meet the performance requirements of your data processing workloads. 
Join me in next video, where we'll take a look at other components and processes like serialization and compression that are critical for storing data and modern data systems. 


#### Storage Raw Ingredients - Processes Required for Data Storage
Interactive Transcript - Enable basic transcript mode by pressing the escape key
You may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key
So far, when talking about the storage hierarchy, we've been focusing on the bottom layer, the raw storage ingredients that are used to physically store data. But the bottom layer also consists of other components and processes that are required for storing and transmitting data in modern data systems. And so in this video, I'd like to take a moment to go over how networking, CPU serialization, and compression play a part in your storage systems. In the cloud era, storage systems are increasingly distributed. This means that your data can be split up, replicated, and spread out across many connected servers to enhance read and write performance, data durability, and availability. So you can consider networking and the CPU required to handle the details of servicing read and write requests, things like aggregating read results and distributing writes across many servers as part of the raw ingredients of storage solutions. You'll learn more about distributed storage systems later on. 
Regardless of whether you're storing data on a single server or across a distributed storage system, when you store data in a file or database or send it over to a network, you need to transform it into a different format. That's because data stored in memory has different representation than the data that's stored in disk. In system memory, you will store data in data structures that are optimized to be efficiently accessed and manipulated by the CPU. But that format is not suitable for persistent storage in disk or transmission over a network. So you need to use a process known as serialization to translate the data into a standard format, usually a sequence of bytes that can be efficiently stored or shared over a network. And when you want to read the data, you'll use a process known as de-serialization to reconstruct the original data structures from the serialized format. You can serialize data using a row or a column-based approach. 
In row-based serialization, you encode and sort tabular data record by record so that a consecutive sequence of bytes represents one row of data. If you're encoding semi structured data, you encode the data object by object or document by document, so that data representing a single object or document is represented as a consecutive sequence of bytes on disk. This is ideal for transactional operations where you need to access data from an entire row. In column-based serialization, you encode and store data column by column so that a consecutive sequence of bytes in a serialized format represents a column. And if you're encoding column oriented semi structured data, the values for a single key across all the objects is stored as a consecutive sequence of bytes. This is perfect for analytical queries where you need to perform operations on specific columns. As a data engineer, you'll likely encounter a wide range of data serialization formats. 
From human readable text formats like CSV, XML, and JSON that are widely used to exchange data between systems and applications, to binary formats like avro and parquet that are even more efficient for storing and querying data. CSV is a popular row-based format that's actually quite prone to error because it doesn't support a defined schema. So it's up to the application to define the meaning of each row and column. If an application adds a new row or column to its data, you have to handle that change manually. So if possible, you should avoid using this format in your data pipelines. XML or extensible markup language was popular when HTML and the Internet were new, but it's now viewed as a legacy format because it's generally slow to serialize and deserialize for data engineering applications. XML has largely been replaced by the JSON format for plain text object serialization. 
Nowadays, JSON is viewed as a new standard for data exchange over APIs, and it's a very popular format for data storage. In terms of binary formats, parquet is a column based format that's designed for efficient storage and big data processing, and avro is a row based format that uses a schema to define its data structure, and it supports schema evolution. You'll learn more about row versus column-based storage in databases and about the popular parquet format later this week. The decisions you make as a data engineer around serialization and how you store data in files and databases can impact overall query performance. Recently, as I was working with a data team, we discovered that by simply switching the serialization format from CSV to parquet, they were able to improve job performance by a factor of 100. Now, let's say that you've serialized your data so that it can be stored on disk or transmitted over a network. As your data volume grows, you might want to enhance storage efficiency and speed up the transmission of your data. 
Data compression is a way to reduce the number of bits needed to represent the data, and it's a critical component for modern data applications that require increasingly large datasets. With compression algorithms, instead of directly encoding the data into a sequence of bits, you use sophisticated mathematical techniques to identify redundancy and repetition in your data, then re encode the data in a more efficient way. For example, traditional compression algorithms that you can apply to text-based data formats such as CSV, JSON, and XML, identify the characters that occur most frequently and encode them differently than the characters that occur less frequently. Instead of mapping each character to a sequence of bits of fixed length, these algorithms match common characters to shorter bit sequences and less common characters, so your compressed data file takes up fewer bits in total to store on disk. And the ratio of the compressed file size relative to the original uncompressed file size is called the compression ratio. In addition to reducing disk space, compression also improves query performance because it reduces the input and output, or I/O time needed to load the necessary data from disk to memory when processing a query. In recent years, engineers have created a new generation of compression algorithms that prioritize speed and CPU efficiency over compression ratio. 
These algorithms are frequently used to compress data in data lakes or columnar databases to optimize for fast query performance. In the optional reading item that follows this video, I've included some examples of these new compression techniques. Now that you've seen how these components make it possible to store data, let's move on from the bottom layer of the storage hierarchy to the next layer, the storage systems that are built from the raw storage ingredients. Again, the next reading item about compression is optional. After that, we'll explore three cloud storage options, file storage, block storage, and object storage. I'll see you there. 


#### Compression Algorithms Compression Overview
To encode data into a sequence of bits, you can use raw encoding which relies on the data type (boolean, integer, double, character) to map the data item into a sequence of bits of fixed size. This is the raw uncompressed method of encoding. Compression algorithms look for redundancy and repetition in the data values, then re-encode data to reduce the overall number of bits that represent data in storage systems. For example, one way to compress textual data is to map the most frequent characters to codes that use less number of bits than the codes mapped to the less frequent characters. In this way, the total number of bits representing a text could be less than the total number of bits used in raw encoding. 

Compression algorithms utilize more sophisticated mathematical techniques to identify and remove redundancy; they can often realize compression ratios of 10:1 on text data. Note that we’re talking about lossless compression algorithms. Decompressing data encoded with a lossless algorithm recovers a bit-for-bit exact copy of the original data. Lossy compression algorithms for audio, images, and video aim for sensory fidelity; decompression recovers something that sounds like or looks like the original but is not an exact copy. Data engineers might deal with lossy compression algorithms in media processing pipelines but not in serialization for analytics, where exact data fidelity is required.

Traditional compression engines such as gzip and bzip2 compress text data extremely well; they are frequently applied to JSON, XML, CSV, and other text-based data formats. In recent years, engineers have created a new generation of compression algorithms that prioritize speed and CPU efficiency over compression ratio. Major examples are Snappy, Zstandard, LZFSE, and LZ4. 

Compression in Column-Based Formats
Some algorithms are generic and can be used in both row-stores and column-stores to compress data using a general-purpose algorithm: LZO (1996), LZ4 (2011), Snappy (2011), Brotli (2013), Oracle OZIP (2014), and Zstd (2015) - (
source
). However, some algorithms are specific to column-stores since they use the fact that consecutive values from the same column are stored together on disk. Compression algorithms benefit from repetition and redundancy in data, and values from the same column can have this characteristic.

Consider the following table:


If data is stored in rows, it means that you need to store the values of product sku, quantity, price, customer id, store ID, and state all together. Since each value represents a different feature, the algorithm might not encounter a lot of repetitions. On the other hand, if data is stored in columns, then the product SKUs are all stored together, same thing for store ID and state. Each column can have lots of repeated values: let’s say you have millions of rows where each row represents an order, then the number of distinct values in the product column will be much less than the total number of rows: maybe 10,000 distinct products.  This column characteristic allows the compression algorithm to detect the common patterns in data easier, as well as represent the data more efficiently. 

In addition to reducing disk space, compression also improves database performance, meaning it helps the database process queries faster since less data is read from disk into memory, and from memory to CPU.


Examples of Compression Algorithms Used on Column-Based Formats
Run-length Encoding

Run-length encoding (RLE) compresses a run of the same values in a column to a more compact representation. Each run is replaced with a tuple that has 3 elements -- (value, start position, runLength), where each element is represented with a fixed number of bits. 

For example, here's the product sku column data from the table mentioned earlier:

34 34 34 63 32 32 32 67 67 67

With RLE, you'll get this result: 

(34, 1, 3), (63, 4, 1,), (32, 5, 3), (67, 8, 3)

RLE can be used in column-oriented systems where the columns have few distinct values, meaning that you'll likely have runs of the same value stored together.


Bit-Vector Encoding (or bitmap encoding)

With this algorithm, each distinct value is associated with a sequence of bits where the length of the sequence is the same as the number of records/rows in the column: a ‘1’ in the i-th position means that the distinct value appears in the i-th row of the column, and ‘0’ otherwise.

For example, here's the product sku column data again:

34 34 34 63 32 32 32 67 67 67

With this algorithm, this data would be represented by four sequences of ten bits (i.e. the number of rows):

bit-string for value 34: 1110000000

bit-string for value 63: 0001000000

bit-string for value 32: 0000111000

bit-string for value 67: 0000000111

Bit-vector encoding is most useful when columns have a limited number of unique values (such as states in the US, store ID, product ID). However, it can be used even for columns with a large number of distinct values, especially if it is combined with another compression such as RLE (to further compress it). 


Optional Resources
Other compression algorithms: 
The Design and Implementation of Modern Column-Oriented Database Systems

Compression encodings supported in AWS

There is an additional file format (Avro) that you may encounter as a data engineer, especially when working with streaming systems such as Kafka. Avro is a row oriented binary file format that encodes semi-structured data in a way that is more efficient than Binary Json. To learn more about Avro, check the following two resources:

Schema evolution

Parquet vs Avro


#### Cloud Storage Options: Block, Object and File storage
As a data engineer, you're faced with a large number of cloud storage options. These storage systems exist at a level of abstraction above the raw ingredients that you saw previously. In this video, we'll look at the three common types of cloud storage systems. These are block storage, file storage, and object storage. You'll need to consider the performance and scalability trade-offs between these options when choosing the best system for your use case. Let's start with the file storage systems, which organize files into directory trees, similar to how the folders might be organized on your laptop. Each folder contains metadata for its files and subfolders, detailing the names, owners, last modification dates, access permissions, and location pointers to the actual files and subfolders themselves. 
So to locate a file on disk, you give the operating system a path to follow, like this one. To locate the hierarchical structure from left to right, your operating system starts at the root directory, indicated by the forward slash, then it finds the user directory, then the Matthew Housley subdirectory, and finally it locates the file named output.txt. In your work as a data engineer, you use file storage when you need to provide centralized access to files that need to be easily shared and managed by multiple users or host machines. You can use a managed cloud file storage service, like Amazon Elastic File System, or EFS. This service provides you, your applications, and your stakeholders access to shared files over a network without the hassle of managing networking, scaling disk clusters, or configuration. File storage is often built on top of block storage, where the complexity of the underlying storage mechanisms is abstracted from you. Despite being a more accessible and understandable storage format, file storage systems don't have the highest read and write performance because they need to keep track of the file hierarchy. 
Block storage, on the other hand, provides the performance and flexibility needed for high-speed transactional data access. Block storage divides files into small, fixed-size blocks of data that you can store on magnetic disks or SSDs. This allows you to precisely allocate storage space for any given piece of data. Each block has a unique identifier, like the address for that block, which helps you efficiently retrieve and modify data in individual blocks, providing higher performance and lower latency than file storage. You often design block storage systems based on a distributed architecture, spreading blocks of data across multiple storage disks, which leads to higher scalability and stronger data durability. This makes block storage the backbone of most modern storage solutions, including your local file systems, transactional databases, and virtual machine storage. When you store a file in block storage, the storage application writes the data into multiple blocks and records the block's identifier into a data lookup table. 
When you request a specific file, the application retrieves the data from the relevant blocks and merges them into the original sequence for you to read. This is all abstracted from you, so you can locate any block by its unique identifier and update the block without having to replace the entire file. This makes block storage ideal for use cases where your data is accessed and modified often. For example, transactional database systems generally access disks at a block level for high random access performance. This enables OLTP systems to perform small and frequent read and write operations with low latency. Block storage is also used to provide persistent storage for virtual machines, like EC2 instances. When you create EC2 instances, you automatically attach a root storage device that's backed by a block storage volume to each of your instances. 
You can install the operating system, file system, and other computing resources in the block storage volume. With EC2, the default storage is Amazon Elastic Block Store, or EBS, and you can choose from various EBS volume types depending on your use case. For example, some volumes are built on high-performance SSDs and are great for latency-sensitive workloads, while others use cost-efficient magnetic disks to store data that's infrequently accessed. Since block storage volumes are typically attached to compute instances, scalability is limited by how much you can scale your compute resources. And so block storage typically caps out at a few terabytes. Object storage, on the other hand, decouples the data storage layer from the compute layer, so it can scale to petabytes of storage or more. In a cloud environment, where your storage capacity is limited only by budget, with object storage, you'll likely run out of money before you run out of object storage space. 
And so cloud object storage allows you to process data with ephemeral clusters and scale these clusters up and down on demand. These ephemeral clusters exist behind the scenes, and you don't need to worry about them. This is a big factor in making big data available to smaller organizations that can't afford to own hardware for data jobs that they'll only occasionally run. Let's review what you learned about object storage in the context of store systems back in Course 2. Unlike a traditional hierarchical file storage system, you store files as immutable data objects in a flat structure in an object storage system. You organize objects into top-level logical containers, like an S3 bucket, and each object is assigned a unique identifier, or key, that you can use to find the object within its container. So an object identifier in S3 might look something like this. 
The first part refers to the bucket name, which must be unique across all of AWS, and dataExample.json is the key pointing to the particular object. Once you write the data initially, the object becomes immutable. Even if you want to change one character of a 1GB file, you'll have to rewrite the entire object instead of just making a small change like you would in block storage. This might seem like a constraint, but it actually removes the overhead of supporting chain synchronization. So you can distribute the objects across many storage nodes that each contain their own disks, eliminating the need to propagate data changes across all these nodes. This allows object stores to scale horizontally and support extremely performant parallel reads and writes across many disks. Each node holds shards of objects, which are replicated across multiple nodes for durability. 
This high scalability and durability makes object storage ideal for the storage layer of cloud data warehouses and data lakes. It allows these storage abstractions to accommodate massive volumes of data in a cost-efficient way. But since objects are immutable, object storage is not good at supporting transactional workloads where many small update operations need to happen with low latency. Instead, object stores are great for storing data needed in massive OLAP systems that focus on read-heavy analytical workloads rather than write-heavy transactional operations. In modern data engineering applications, object storage also plays a crucial role in machine learning pipelines that require large amounts of unstructured training data such as raw text, images, videos, and audio. And so file, block, and object storage all have wide use cases. If your focus is on simple data sharing and ease of management with low performance and scalability requirements, then file storage systems might be the most straightforward solution. 
To support transactional workloads that require frequent read and write operations with low latency, you should choose block storage. And if you need to perform analytical queries on massive datasets, you can choose object storage for its high scalability and parallel data processing capabilities. Cloud providers typically offer different storage tiers within each of these three common storage options. Join me in the next video to take a look at how to decide on an appropriate storage tier by considering the hot, warm, and cold classification method used to categorize data based on access frequency. 


#### Storage Tiers - Hot, Warm, & Cold Data
Most Cloud data storage services offer different storage tiers. You can choose from depending on your costs, access speed, access frequency, and compliance requirements. When choosing a storage tier, I recommend you consider the hot, warm and cold data classification method that's based on how frequently data is accessed and used. On one end of the spectrum, you have hot data, which is data that's accessed frequently and requires fast retrieval times. For example, in a product recommendation application, you need to frequently access the product catalog and the purchase history of your users. You might also want to store the results of frequently run queries in a cache so that you can quickly serve customers with product recommendations. To serve fast read access, you typically want to store hot data in systems that utilize high performance storage mediums like SSD and memory. 
The storage cost for hot data is typically more expensive, but the retrieval time and compute resources required to access the data is relatively low because you store data in a way that optimizes for fast access. Warm data is accessed less frequently than hot data, but still needs to be readily available. For example, this could be data that's used for regular reports and analysis that don't need to be updated in real time. You typically want to store warm data in lower cost storage systems that utilize slower magnetic disks or hybrid storage systems. Compared to hot data, the storage costs for warm data is lower, but it typically takes more time and compute resources to retrieve the data. Then on the other end of the spectrum, you have cold data. This is data that's rarely accessed and is often archived. 
For example, you might decide to archive project documentation or keep old emails for compliance purposes. You want to store cold data in the most cost effective storage tier that's built on low cost magnetic disks. So the storage cost for cold data is the most inexpensive, but compared to warm data, it'll take you longer and require more compute resources to retrieve this data. In general, the storage price goes down as you move from high performing storage with fast access, to lower performing storage with slower access. If you store all of your data in hot storage, you'll be able to access your data very quickly, but at a tremendous storage price. If you store all your data in cold storage, then you'll save on storage costs, but it'll be at the expense of long retrieval times and high compute required for data access. You'll typically want to choose a combination of storage tiers for your various storage needs. 
As an example, when storing data in Amazon S3, you might store your frequently accessed hot data for real time transactions in what's called the S3 express one zone or the S3 standard tiers. You might store warm data that needs to be accessed say weekly or monthly for finetuning a product recommendation system in the S3 Standard infrequent access or S3 one zone infrequent access tiers. Finally, you want to archive historical cold data in one of the S3 glacier tiers. When designing your storage solutions, aside from access frequency, you also want to consider things like the scalability and durability of your storage solutions. In the next video, we'll dive deeper into distributed storage systems and discuss the trade offs associated with that architecture. I'll see you there. 


#### Distributed Storage Systems
Interactive Transcript - Enable basic transcript mode by pressing the escape key
You may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key
As your data storage needs increase and the data access patterns become more complex, you'll inevitably outgrow storage on a single machine and move to distributing data to more than one server. In fact, distributed storage is the default way to store data in the Cloud, whether you using block, file, or object storage. In this video, I'll build on what you've previously learned and dive deeper into the details of how a distributed storage architecture works. In a distributed storage system, you distribute and replicate data across multiple servers known as nodes that are connected by a network. Groups of nodes make up what's called a cluster, and these clusters collectively make up the distributed storage system. Each node contains storage mediums, such as magnetic disks or SSDs to physically store the data. The storage capacity of your distributed system is the total capacity of all the individual nodes, and each node typically has processing capabilities to handle data management, replication, and access control. 
Storing data this way allows you to easily scale your storage systems horizontally, meaning you can add more nodes to clusters to accommodate growing data volumes and tackle increased workloads. In the single machine storage architecture, you can only achieve vertical scaling, which means you can only upgrade the storage capacity of a single server. By spreading data across multiple nodes and replicating it across clusters, you can also ensure a higher level of fault tolerance and data durability. Which is to say your data will persist over time even in the event of failure to one or more components of the system. This goes hand in hand with high availability. If a node becomes unavailable due to hardware or software failure, network outages or other disruptions, you can still access the data from another node that's not impacted, maybe because it's in a different geographical location. In terms of performance, distributed storage systems divide large processing tasks into smaller sub tasks that are handled by individual nodes. 
This helps the system process many read and write operations in parallel. Since data is replicated across multiple nodes, the system can also serve read requests from the nearest or least congested replica node so you can access the data faster. Because of these advantages, many storage solutions, including object storage, Cloud data warehouses, Hadoop Distributed File System or HDFS, Apache Spark, and many more rely on distributed storage architecture. As a data engineer, you'll find there are two common ways to distribute data across multiple nodes. Through replication or partitioning. With replication, you keep a copy of the same data on several different nodes, potentially in different geographical locations. This redundancy results in higher availability and helps improve performance. 
Partitioning, also known as sharding, splits a big dataset into smaller subsets called partitions or shards, and then different partitions can be assigned to different nodes. In practice you'll likely use a combination of these methods to distribute data. You partition a large dataset and distribute the shards to different nodes. Then you'll replicate those nodes to create a good level of redundancy. Most databases can automatically partition and replicate your data in a way that's abstracted from you, or you can specify replication and partition parameters for more control over your distributed storage system. One challenge with distributed storage is that it takes time to replicate changes across nodes. When you're trying to access data from a node that's being updated, you can either wait for the update to complete before accessing the data, or you can access the recent data that's currently available in that node. 
This trade off is summarized by something called the CAP theorem, which states that any distributed system can only guarantee two out of three properties, consistency, availability, and partition-tolerance. Strong consistency means that every read reflects the latest write operation. Note that this is different from the consistency component in the ACID principle you saw in Course 2, which says that any changes to the data made within a transaction must follow the set of rules or constraints defined by the database schema. The ACID consistency principle ensures that the database will transition from one valid state to another, which is a condition that is facilitated by the strong consistency property. Availability means that every request will receive a response, even if it's not necessarily the most recent data. Partition-tolerance means that the system continues to function even when the network experiences disruptions or failures that isolate some nodes from others. Since some distributed system is safe from network failures or unforeseen disruptions, network partitioning usually has to be tolerated, which is to say, building systems that guarantee partition-tolerance is usually a given. 
You usually have to choose between consistency or availability because remember, the CAP theorem states that any distributed system can only guarantee two out of the three properties. This means that in the scenario where you're trying to access a node that's still being updated, you can either design your system to cancel the request, which decreases availability, but ensures consistency, or you can configure the system to proceed with a read operation, which provides high availability, but risks inconsistency. Database systems such as an RDBMS that are designed to be asset compliant will choose consistency over availability. Whereas NoSQL database systems that are not ACID-compliant, we'll typically choose availability over consistency. In contrast to the asset principles, there's actually a set of principles called BASE that you can use to design and evaluate your distributed data systems. Base stands for, basically available, meaning that consistent data is available most of the time. Soft-state, meaning it's uncertain whether the transaction is committed or uncommitted, and eventual consistency, meaning at some point, reading data will return consistent values. 
As a data engineer, you need to understand how your database handles consistency, which can be determined by the database technology itself, the database configuration parameters, or the consistency configuration at the individual query level. Once you understand the technical limitations and business use cases for your data, you might need to negotiate consistency requirements with other stakeholders. Let's revisit the scenario from Course 1, where you needed to serve sales data to the data scientists so they can update an analytics dashboard for the marketing team. The software engineers have set up a read replica of the production database, so you can ingest, transform, store, and serve the required sales data to the data scientists. Suppose that the database is implemented using Amazon RDS Aurora, which is a distributed relational database service. With this database, there's a main database instance that supports strict read after write consistency, meaning strong consistency. But if the data scientist values immediate access to sales data, even if it's not the most up to date data, then you can set up read replicas and RDS. 
These read replicas will track all changes made to the main database instance and update their own copies of the data. Then on a query by query basis, you can decide to read from the main database instance that supports strong consistency, or you can read from one of the read replicas that supports the eventual consistency. Next up, I've included an optional reading item about database partitioning that you can go over if you want and it covers how a database is distributed across different nodes. After that, I'll walk you through the lab where you'll get a chance to work with the object block and file storage systems in the Cloud, which all rely on the distributed storage architecture. You'll also compare these systems to memory storage systems that store data primarily in RAM rather than on disc. I'll see you in the next video. 


#### [Optional] Database Partitioning/Sharding Methods
Let’s take a closer look at one approach for implementing distributed storage, specifically for databases, known as database sharding.

Say you want to distribute the following dataset  across multiple nodes.

Customer ID

Name

Country

10023

Sanjay

IND

27181

Jane

USA

98221

Mo

IND

10134

Abdul

CAN

33410

Mina

USA

30191

Sam

USA

…



You need to split the dataset into partitions or shards, where each shard contains unique rows of data and the shards will collectively make up the whole dataset. Then you can distribute these shards across the nodes in your system. You can use a database sharding method or rule to construct a shard key that indicates how the data will be partitioned. 


Common Sharding Methods
Range-based sharding. 
This method splits the rows based on a range of values. For example, let’s say you want to group the rows based on the first letter of the customer's name. One shard might hold customers whose names start with A through J, another shard might hold names starting with K through R, then a third shard could hold names starting with S through Z. Then the shard key you see in the following tables tells the database which node to distribute each row of data to. This is a straight-forward method but can result in unbalanced shards, meaning unbalanced nodes.

Name

Shard Key

Starts with A to I

A

Starts with J to S

B

Starts with T to Z

C

Hashed sharding 
This method uses a mathematical formula called a hash function to determine how to partition your data. For example, you can simply assign alternating hash values of 1 or 2 to each row to separate the rows into two shards. Then this shard key tells the database where to distribute the data for each row. This method can result in a more even distribution of data across nodes.

Geo sharding
This method partitions data based on geographical location. Then you can store the customer's information in nodes that are physically located in that location. By reducing the physical distance between the shard and the customer, you retrieve data faster.

Other methods
There are also other methods that split the data based on meaningful attributes, for instance, the customer’s occupation or favorite color.


Resources (optional further readings)

What is database sharding?

Designing data intensive applications 
- Chapters 5 and 6 (Replication and partitioning)



#### How Databases Store Data
I mentioned in the previous courses. One of the most common types of source systems you'll interact with are databases, specifically relational databases, and you'll use databases throughout all the stages of the data engineering life cycle. In this video, let's dive deeper into how databases store data. Databases typically come with a software layer known as the database management system or DBMS that facilitates your interactions with the database. This is true for the relational databases you saw in Course 2 and for non-relational databases, such as graph and vector databases that you'll explore in more detail later in this lesson. A DBMS consists of a transport system, query processor, execution engine, and storage engine. You'll learn how all these components work together to process a query in the last week of this course. 
But for now, let's focus on the storage engine. The database storage engine does the heavy lifting for you when it comes to physically storing data on disk, including serialization, arrangement of data, and indexing. You will likely work with modern storage engines that are optimized to support the performance characteristics of SSDs and can handle modern data types and structures like variable length strings, arrays, and nested data. As organizations started to apply analytics on large scale data. Modern storage engines have also evolved to offer robust columnar storage support for analytical applications. When you want to retrieve data from a database by writing a query, speed is typically very important. Suppose you're quering a very large relational table with millions or even billions of rows, and you're particularly interested in the rows related to certain countries, say you want to find the average price of products purchased in the USA. 
You can write a SQL query to select the average value of the price column from my table and filter the results where the country column equals USA. Note that I'm using a SQL query here because I want to query a relational database. In the lab at the end of this lesson, you'll use a different query language to query a graph database. Now, back to this example, to execute the query, the query processor would have to scan the entire table each time to find the record satisfying this filter condition. Turns out that you can speed up data retrieval by using a special data structure called an index. You can think of indexing as a way to keep some meta data on the side to help you locate the data you want more efficiently. In most relational database management systems, indexes are typically used for primary table keys and foreign keys, but you can also apply indexes to other columns to serve the needs of specific applications. 
In this example, we can create a separate index table that consists of two columns. One that includes the country value sorted in alphabetical order. And the other that consists of the memory addresses referencing the corresponding rows in the original table. Then when you execute a query to find the average price of products purchased in the US, the query processor can use binary search on the Dt index table to locate the rows that have a country code of USA. This is much faster than scanning all the rows in a linear fashion to look for a specific country code. If you're familiar with computer science, you'll see that you're effectively reducing the time complexity of the retrieval operation from ON to O Log N. If you're not familiar with this notation, don't worry about it. 
It's most important that you understand how locating a specific country code from a sorted list is faster than scanning all rows. This is one example of indexes, and there are many other types of index structures depending on the database type and your use case. Now, I want to quickly mention in-memory databases that use RAM as the primary storage layer. Although RAM offers excellent transfer speeds and low latency, it's also extremely volatile. In memory databases are generally used in applications that require ultra fast data retrieval, such as caching applications, real time bidding, and gaming leader boards. But these databases should not be used for retention or persistent storage purposes. For instance, you might use a key value store like Memcached to cache database query results or API calls, where it's acceptable for data to be lost if the machine is restarted. 
You might also encounter a popular memory based storage system called Redis, which is a key value store that supports more complex data types. Redis has several built in persistence mechanisms, including snapshotting and journaling. You can use this key value store for extremely high performance applications that can tolerate a small amount of data laws. Now that you've seen how database engines store data and optimize the retrieval of data within databases, let's take a closer look at two common approaches for storing structured and semi structured data and databases. Join me in next video, to compare the storage patterns and query performances of row storage and column storage. 



#### Row vs Column Storage
There are many types of databases used to sort the kinds of data you'll be working with as a data engineer. In previous courses, you've already seen how relational databases, key value stores, and document databases store data. And later this week, you'll learn how data is stored in other types of databases, like graph and vector databases. In this video, I want to focus on two common ways to store structured tabular data in semi structured data that's commonly used in data engineering. First, you can store this data in a row-oriented storage system, or second, you can sort it in a column-oriented storage system. As a data engineer, you'll choose between these two storage patterns based on your data access pattern, which is to say how the users and the system access your data. Traditional relational database management systems use row oriented storage to store data row by row. 
Each row represents a complete record. If you zoom in on the physical storage medium, you'll find that each row, or in the case of semi structured data, each object is stored as a consecutive sequence of bytes on disk. This way of storing related data next to each other makes row storage perfect for OLTP systems that need to perform read and write operations with low latency. Say you want to query the data to locate a particular record based on the id column, since all the data for that record is stored together. Once you have located that specific id, you you can efficiently read and update that data. But what if you want to perform an analytical query that requires you to operate on the values of an entire column? Analytical queries focus on summarizing or aggregating columns to answer questions like what was the total revenue? 
Which product was sold most frequently? Or what was the average quantity? Let's take a look at how these queries perform on row storage. Let's say your row storage contains data with 1 million rows, 30 columns, or each entry is 100 bytes. Suppose that the second column represents price, and you want to find the sum of all these prices. So I'll write a query that selects the sum of the price column from a table called mytable. To execute this query, you need to first transfer all the rows one by one from the persistent disk storage to ram. 
Then these rows will be sent to the CPU for processing, where the price from each row will be extracted and added together. So the total data size you need to transfer to ram is 1 million rows times 30 entries per row times 100 bytes per entry, which is 3GB. If the disk you're using has a data transfer speed of 200 megabytes per second, how long would it take to read all the data to memory. Well, that would take 3GB, which is 3000 megabytes divided by 200 megabytes per second, which is 15 seconds to read all the data from disk to RAM, that's not too bad. So the row storage pattern works fine for performing analytical queries on small data sets in the short term, but it's not scalable. Imagine that instead of 1 million rows, you actually had 1 billion rows of data, so your data size would be 3000gb at the same transfer speed. That would take you just over 4 hours to transfer all the rows with all their columns from disk to ram. 
So to accommodate such large-scale data transfers, engineers designed another storage pattern, the column-oriented storage or column-oriented database, which is a type of NoSQL database. When you store data in column storage, the data from each column is stored together on disk, so all the data from the first column is stored together. Then the data from the second column is stored together, and so on. And so this allows you to read full columns of data all at once, rather than having to scan each row for the data in a single column. Let's take a look at the analytical query performance of column storage using the same example as before. Say you want to again find the sum of all the values in the price column of a large dataset containing 1 billion rows, 30 columns, where each entry is 100 bytes. Since column storage stores the data from each column together, to execute this query, you'll only have to transfer the data from the second column from disk to RAM. 
That means the size of the data you need to transfer is the 1 billion entries from the second column times 100 bytes per entry, which is 100gb. Suppose we have the same data transfer speed as before, 200 megabytes per second. Then how long would the transfer take? Well, it will take 100gb, which is 100,000 megabytes divided by 200 megabytes per second, which is just over eight minutes. Comparing this to the 4 hours it would have taken with row storage, you can see that column storage is much more efficient when it comes to analytical queries on large datasets. That's why column-oriented database are more suitable for OLAP systems that focus on applying analytical activities on data. However, for exactly the opposite reason, column storage is terrible for transactional workloads because you can't easily access the individual data rows. 
If you want to read a particular record, you have to reconstruct the row by reading data from several columns. And if you want to update one field in a particular record, you need to deserialize the columns modify them, then write it back to storage. As a data engineer, it's important that you understand the difference in how row storage and column storage work so that you can choose the right approach for your use case. Namely, using row storage for transactional workloads like the production database for your sales platform, and using column storage for analytical processing. In the next two optional reading items, you can learn more about the parquet format that combines both row and column-oriented storage approaches, followed by a reading on wide column databases. While Parquet is a popular and common format in data engineering today, wide column databases are less common, but can be well suited for transactional workloads. In the video after that, we'll take a look at graph databases that have gained attention alongside the growth of machine learning and generative AI applications. 
I'll see you there. 



#### [Optional] The Parquet Format
Overview

Column-oriented storage is suitable for analytical workloads where you want to apply aggregating operations on columns. But it is not suitable for reading or writing/updating rows. On the other hand, row-oriented storage is suitable for transactional workloads that require read and write to be performed with low latency. But it is not suitable for efficient analytical workloads.

Parquet and ORC (optimized row columnar) are  file formats that combine both approaches by following a hybrid approach that tries to get the best from both worlds. The hybrid approach relies on partitioning rows into groups where each row group is stored in a column-wise format.


 Although they are similar, ORC is generally less popular than Parquet (ORC was very popular for use with Apache Hive - a data warehouse), and it enjoys somewhat less support in modern cloud ecosystem tools. So let’s focus on Parquet.


A bit more detail about Parquet

With a Parquet file, the data is horizontally partitioned into row groups, where each row group has a default size of 128 megabytes.  A row group consists of a column chunk for each column in the dataset. Each column chunk is divided up into pages, where each page contains the encoded values for that column chunk, metadata like the minimum, maximum and count of the values, along with other data (repetition and definition levels) used to reconstruct the nested structure of the data. Parquet can be used to store tabular data as well as nested (i.e., semi-structured data like json) data. 

Another advantage of Parquet is its portability. So you’ll get better performance with Parquet when interoperating with external tools, unlike proprietary cloud data warehouse columnar formats, that require deserialization and re-serialization for compatibility. 

To learn more about the Parquet format, feel free to check out 
this video
 by Databricks.

Resources (optional further readings)

Parquet, ORC and Avro

Parquet documentation


#### [Optional] Wide-Column Databases
A wide-Column database is a type of NoSQL database that's like a combination of a relational database and a document store. It structures the value part of a key-value pair into more key-value pairs. 

In a key-value database, the data is represented as follows:


On the other hand, a wide-column database stores data in tables that are like two-dimensional key-value maps:


Each row usually describes a single entity, and is identified by its row key. Within a row, data that’s related is modeled into column families that contain columns with unique column names. The actual data is stored within cells, which are uniquely identified by the combination of row key, column family, and column name (e.g. row key 1, column family 4, column key 3).  


For example, let’s suppose you want to store customer and purchase information in a wide-column database like this:


You could use the customer_id 12345 as the row key that points to two column families -- customer information with 3 columns (first_name, last_name, and phone_num) and purchase information with 2 columns (invoice_num and store_id). 

Each cell value is versioned and uniquely identified by its version number, typically its timestamp. Let’s say this customer changed their phone number or made more than one purchase, then these cells will contain multiple values, each identified by their timestamp. 


Now, let’s say you have another customer, with customer_id 45678, who you don’t know the phone number for and who hasn’t made any purchases. Then their row key will point to the customer information column family that contains only the customer’s first_name and last_name columns without the phone_num column. Since this customer hasn’t made any purchases, you don’t have to point to a purchase information column family. When this information becomes available, you can add that to the wide-column database. Since the wide-column database doesn’t enforce a strict table schema, adding columns becomes very flexible, and a column is only written if there’s data for it. 

A wide-column database typically stores column families separately on disk. So it will store the customer information separately from the purchase information. Then the data within a column family is stored in a row-oriented fashion. So, for the purchase information, you would store all the data for a specific row key next to each other on disk, storing all the invoice_num values together, then the store_id values together, before moving on to the next row key, and so on.



Examples of wide-column databases include 
HBase
, 
BigTable
, 
Apache Cassandra
, and 
Amazon keyspaces
 (for Apache Cassandra)


#### Graph Databases
Interactive Transcript - Enable basic transcript mode by pressing the escape key
You may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key
Graph databases store data within a mathematical graph structure using nodes and edges. Nodes represent your data items, which are typically entities such as people, products, or locations, among other things, and edges represent the relationship or connection between these data items. With modern applications, you might encounter graph databases if you're building a data system for a use case that involves complex connections between data entities. For instance, if you work at an e commerce company, you might use a graph database like this one to store data about the relationships between users and products. You could, of course store this data in a relational database like this one on the right. But graph databases treat relationships as first class citizens, so you can very easily see the relationships between entities using a graph database compared to using a relational database. When you want to query the data in a graph database, meaning you want to query the relationships between nodes and edges, you can traverse the graph structure. 
For example, let's say you want to recommend to a user the products that were purchased by their friends. Starting with user one, you can traverse a graph by following the edges labeled friend to identify all the friends of this user. Then, for each friend, you can follow the edges labeled purchased to create a list of the products that these friends have bought in the past and that you want to recommend to User1. If the same data was stored in a relational database like I have here on the right, then to create the list of recommendations for user one, you would join the friendship table with the purchase table where the friend and friendship matches with the user in the purchase so that the joined results contain rows of users along with their friends and products purchased by their friends. Then you would filter this query so that you would only get information for user one. Then you could select the distinct products to get a list of the products that you should recommend to user one. Now you can imagine that if you wanted to recommend even more products to a user, say by considering the products that were purchased by the friends of their friends, then querying a relational database like this would require even more joins and that can get out of hand very quickly. 
Aside from product recommendation applications, there are many other use cases for graph databases, including modeling social networks, representing network and IT operations, simulating supply chain logistics, and tracing data lineage. Another not so obvious use case is in fraud detection, say in e-commerce transactions. You can build a graph to model the relationship between entities such as customers, the products they purchased, the credit card used for the purchases, and their IP addresses. Then you can identify suspicious activity by comparing these relationships with known fraudulent ones. For example, suppose that you know that many fraudulent transactions include a credit card number that's already associated with one being used by a new user at a new IP address. Then, by analyzing the relationships between users credit cards and IP addresses in this graph, you can flag all users who are using a credit card that's already associated with another user, but now from a new IP address as being suspicious. You can also use graph databases to create knowledge graphs to connect data from disparate sources for a variety of use cases, like to enhance the accuracy and reliability of chatbots. 
So, for example, here you could create a knowledge graph to connect product, customer, and shipping data across an e-commerce company. When a user chats with a chatbot, you can search the knowledge graph for relevant information to provide additional context to the underlying large language model through a technique called retrieval augmented generation or RAG. This technique gives the LLM access to fresh data that's relevant and specific to the e-commerce company itself, improving the query results. We won't be getting into specific generative AI topics in these courses, but it's a very exciting time for generative AI. And as a data engineer, I would encourage you to learn more about these topics by checking out the links to additional resources at the end of this week. And so with all these potential use cases for graph databases, you need to develop some experience working with them. Nowadays, you can choose from many different graph databases including Neo4j, ArongoDB, and Amazon Neptune. 
With these databases, you'll use specialized languages like Cypher, Gremlin, and Sparkle to query data. In the lab at the end of this lesson, you'll get to play around with Neo4j and the Cypher query language. Aside from being a graph database, Neo4j also has integrated vector search capabilities commonly found in vector databases. So before you jump into the Neo4j lab, in the next video, I'll show you a bit more about vector databases and how vector search works. I'll see you there. 


#### Vector Databases
Another type of database that's gained popularity with the rise of machine learning applications is the vector database. Vector databases enable you to efficiently query data based on semantic similarities. This is called similarity search, and it has applications from recommendation systems to anomaly detection to text generation. For example, if you wanted to recommend a product to a customer, you might query a vector database to identify products that are similar to those that the customer has purchased in the past. Or you can query a vector database to identify transactions that are dissimilar to other transactions in order to detect anomalies and potential fraudulent activities. These databases are optimized for storing and processing vector data, which consists of numerical values arranged in an array. So you could use a vector database for storing any kind of data that is in the form of numbers in an array. 
For example, an array of numbers representing the amount of rainfall recorded each day over the course of a year. Or you could store numerical data that could be rearranged into a vector, like image data, where you can unroll the RGB dimensions of the image into an array of numbers. However, the importance of vector databases today is really about storage and retrieval of what are called vector embeddings. The core idea with vector embeddings is to take an item like a text document or an image and capture its semantic content using a vector. The way this works is that you pass the original data, say, a piece of text, through a machine learning model that has been trained to convert text to vector embeddings. You can then convert an entire database of documents or other text content into such embeddings and store them in a vector database. The advantage of storing vector embedding representations of other types of content is that it's much easier and much faster to find and retrieve similar items based on their vector representation than it would be to compare across the original data items. 
So, for example, suppose you have an item like a piece of text, and you want to find items in your vector database that are similar to that text. Then to query the database, you first need to compute the embeddings for the query item. Then the database can measure the similarity between any two vectors and return those that are most similar. Vectors that are similar to one another semantically, will be closer to one another in the high dimensional vector space the vectors are represented in, and by closer I don't only mean closer to one another in terms of the euclidean distance you see here, which measures the length of the line segment between the endpoints of two vectors. You can also determine the closeness of two vectors by using other types of distance measures. Like the cosine distance, which determines the closeness based on the angle between the two vectors. Or by Manhattan distance, which is based on the distance between the vectors measured along their axes, as well as other metrics. 
And so there are many algorithms available to you for performing a similarity search over a database of vector embeddings. Let's take a closer look at one of the more popular algorithms, k-nearest neighbors, or KNN search. Say you want to find the k most similar items to a given item. The KNN algorithm will do an exhaustive search over all the vector embeddings of all items to compute the distance between those items and the given one. You can imagine that as the size of the vector database increases, this algorithm becomes less efficient. There's also the challenge of what's called the curse of dimensionality. Which is to say, because the high dimensional vector space may be sparse, distance measures might not reflect the accurate distance between them. 
To overcome these challenges, you can use a different set of algorithms known as ANN, which stands for approximate nearest neighbors. These algorithms rely on finding a good guess for the nearest neighbors to a given item rather than calculating the exact distance from all the items. So even though they may result in slightly less accurate results, they are a lot more efficient. In fact, vector databases are built to support ANN algorithms to enable you to perform efficient similarity search. When storing your vector embeddings in a vector database, the database applies an ANN algorithm to represent your data in a data structure that enables faster search. Then, when you query the vector database to perform a similarity search based on the given item, the database uses its specific ANN algorithm to traverse the data structure to return the items that are approximately the closest. I included an optional reading item about a popular ANN algorithm known as hierarchical navigable small world, or HNSW, after this video. 
Feel free to read more about the algorithm if you're interested to learn more. Then in the next video I'll walk you through how to use the cypher query language to query data in a Neo4j graph database to get you set up for the lab. 


#### [Optional] ANN Algorithm: Hierarchical Navigable Small World (HNSW)
Hierarchical Navigable Small World (HNSW) is a popular ANN algorithm that underpins many vector databases and is considered to be among the best performing ANN algorithms. 


Image source 

This algorithm relies on building a hierarchical graph representation of the embeddings: each layer consists of a graph representation of the data, where each node represents an embedding, and each edge represents the degree of similarity between two nodes. The layers are constructed in a way so that the top layer contains more of the longest links and the bottom layer contains more of the shortest links. Moreover as you move up from the lowest layers to the highest layer, the number of nodes decreases. 

Given a query point (e.g. the green node in the image above), the algorithm starts at the entry node of the top layer (i.e. the red node) and navigates through the graph of that layer, each time choosing the neighboring node that is closest to the query point. It stops at the node that does not have any neighboring nodes closer to the query point. At this point, the algorithm shifts to the current node in the next lower layer and begins searching again. It repeats the process until it finds the nearest node at the bottom layer.

The way this algorithm organizes the data reduces the time and computational resources needed for these searches when compared to the k-nearest neighbors algorithm.


#### Neo4j and Cypher Query Language (Part 1)

0:48
Play
Volume

0:00 / 4:17

1.5x
Settings
Full Screen
Neo4j and Cypher Query Language (Part 1)

en
​
Interactive Transcript - Enable basic transcript mode by pressing the escape key
You may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key
Graph databases such as Neo4j allow you to model your data as a graph and interact with it similar to how you would interact with relational databases. In the upcoming lab, you'll explore Neo4j, and so here, I'd like to show you a particular graph model called property graph model, which you can implement in Neo4j. You'll also learn how to interact with such a graph using the Cypher query language. In relational databases, you can represent your data with a relational model like this that describes the tables, the column names and each table, and how the tables are related to each other. Similarly, in Neo4j, you can model your graph data using the property graph model as shown here. Note that this diagram describes at a high level the graph structure, but it doesn't show the actual data. The model describes what types of node exist in the graph and how these are linked together. 
In this example, the model consists of five types of nodes, customer order, supplier, product, and category. The type of node is referred to as the node label. So for example, if a node represents a category of products, it should have category as its label. The edges between each node are called relationships and each relationship has a type which is shown in the text here next to each arrow. Each relationship type has a source node and a target node. So for example, the supplies relationship has the supplier node as its source node and the product node as its target node. Here's an example of some actual data that follows this graph model. 
You can see five customer nodes shown in pink. For each customer, you can see the orders they purchased, what products each order contains, to which category each product belongs, and who supplies each product. You can also see additional information associated with each node, such as the iD for the order node, the customer ID for the customer node, and the name of the product for the product node. This information is known as the node properties. You can associate more than one property with each node to further describe the entity it represents. Specifying the node properties is part of describing a complete graph model, hence the name property graph model. So in this example, each customer is associated with this set of properties, including their address, contact name, customer Id, and so on. 
Here's an example showing the values for those properties for the customer QUEDE, and here are the properties associated with the other node labels. You can specify the properties not only for nodes, but also for each relationship. So for example, each relationship of type orders which maps an order to a particular product, has this set of properties, properties, discount quantity, and unit price. Here's an example showing the values for the properties of this orders relationship there are several ways you can create a graph database in Neo4j. One way is to write a set of instructions to Neo4j, specifying the details of the graph model such as the nodes, their labels and properties, as well as the relationships between the nodes, their types and properties, and where to find the data for the nodes and their relationships, which could be in some CSV files. Neo4j will create the actual graph with the given data. Then, you can perform queries to interact with the graph and visualize the query results. 
Youll need to use the cypher query language to create a graph database or to interact with the data in Neo4j. So in the next video, we'll go through some examples of how you can read information from a graph database using the Cypher query language. Then in the lab, you'll practice with more CRUD operations to create and delete nodes from a graph database. In the lab, you'll follow the instructions to open the Neo4j desktop browser that looks something like this. I've also created a database following the graph model example you saw at the beginning of this video. To learn more about how you can create the graph database and the reading item that follows this video, I included a link to the CSV files that I use as well as the Cypher instructions that you can use to create the same graph you see here. Now that you've seen what a property graph model looks like, join me in the next video to go over the Cypher query statements you'll use to interact with the graph data in the lab. 



#### Neo4j and Cypher Query Language (Part 2)
Let's explore the graph you saw in the previous video. You'll use the match statement to retrieve information you want from a graph. This statement enables you to specify the pattern that you want neo four j to search for and return to you. It's similar to the select statement you use with a relational database. It always has this format, match pattern return result where you need to specify the pattern end result. So let's say you want to retrieve all the nodes. In cipher, you use parentheses to denote a node. 
Then inside the parentheses, you can specify the information of the node you want to retrieve. So I'll write match parentheses. Since I want all the nodes to be returned, I don't need to specify any further information. I just need to use a variable like n to denote the node, and then I'll ask neo four j to return n. This statement returns all the nodes. Now let's say you want to get the total number of nodes. Again, I'll use match (n), but in the return statement, I'll write return count (n). 
You can also explore what node labels you have in the graph by using the labels function. In the return statement, I'll write match n, then return distinct labels n. The distinct keyword ensures that the returned labels are not repeated. So the nodes in this example have these labels, which is consistent with the graph model you saw earlier. If you want to count the number of nodes that have a specific label, say the ones with an order label, you can specify the label name inside the match pattern, like match n colon order, then return count n. You see that the graph contains 99 order nodes. You can also explore the properties associated with each node type by calling the properties function on the node variable in the return statement. 
For example, I can match the nodes based on the order label, then return the properties associated with any order node. This statement returns the properties for all order nodes. You can limit the number of results using the limit keyword. So here I'll specify that I want the properties of the first node. In all the examples you've seen so far, we explored the information about the nodes in the graph, but you can also explore the information about the edges or the relationships between those nodes. You can use square brackets to denote a relationship. Then inside the brackets, you can specify a variable so you can reference relationships. 
Now, since a relationship exists between two nodes, the way you can relate a relationship r to its source and target nodes is like this. This pattern represents a directed path from a source node to a target node. Let's say you want to count all the directed paths that exist in this graph. You can write a match statement like this where you're looking for any relationship r that goes from any source node to any target node. Then you can return count r. Since I don't need to reference the nodes in the return statement, I don't have to specify any variable for the nodes in the parentheses. You can see that there are 518 directed relationships in the graph and you can modify the return statement to see the distinct types of relationships in the graph. 
You can also specify the type of relationship that you want to investigate by specifying the label for the relationship. For example, I can specify the orders label after the relationship variable r. Now, instead of just returning the count or type of relationship, you can also return its properties. So for the orders relationship, say you want to find the average price for orders, you can return the average of the order price which you can get by multiplying the quantity property by the unit price property. If you want to more easily refer to the return value, you can use the as keyword to create an alias for the return value. So here I'll label the return value as average_price. Now let's say instead of getting the average price for all orders, you want to get the average price for all orders grouped by product category. 
You can start with the query for overall average_price for all orders and add another path to the MATCH statement to get the node category. So the pattern this query matches includes all orders relationships denoted by r that are a part of a specific category denoted by c. Then in the return statement you can add C category name, which is a list of names representing each category. The last type of statements that I'd like to cover are filtering statements. You can filter the results using the where statement, which is similar to the SQL where statement. So let's say you want to retrieve the product name and product unit price of all products that belong to the category Meat/Poultry. First, you need to specify the path where a product node is part of a category node. 
Note that I assign variables to the product and category nodes, but not to the part of relationship. This is because I need to reference these nodes but not the relationship in the rest of the query statements. Next, you need to specify the condition you want to filter by, which is where the categoryName property equals Meet/Poultry. And finally, you need to specify the properties of the products that you want to be returned. In this case, I'll specify the product name and its unit price. Instead of specifying a desired node property in a where statement, you can instead clarify the property inside the node parentheses. So in this case, in the category node, you can use curly brackets like this to specify the category name you want to filter by. 
Now let's say you want to retrieve the product name of all products ordered by the customer with customerID QUEDE. You'll start with a customer node which ill give it a variable name of c1 and specify that the customerID is QUEDE. Then youll chain the purchased and orders relationship to reach the product node. And finally I'll return the product names. Note that for the purchased relationship I didn't have to specify a variable for the target order node because I don't need to reference that variable in the rest of the query statement. So here are the products ordered by the customer QUEDE. Now let's say that for the same customer QUEDE, you want to get the ID of other customers who ordered the same products as QUEDE. 
So again I'll start with the customer QUEDE, then chain the two purchased and orders relationships to get the product node. Now I'll specify the path in reverse by chaining the orders and purchase relationships using the left arrow to go from the product node to another customer node. I'll use a variable c2 to represent this customer who ordered the same products as QUEDE. And then finally I'll return the customer id of the second customer. Let's do one more search over this graph. Let's say you want to retrieve the orders that contain at most two products. Let's think about this step by step. 
First, let's get the total number of products for each order. So I'll start by specifying the orders relationship from order nodes to product nodes, and then I'll return the orderID and the count of products. This statement is equivalent to using the SQL groupby statement to group the orderIDs, then counting the number of products for each orderID. Next, I need to add a where statement to filter the orders that have at most two products. To do this, ill replace the return statement with a with statement which allows you to access the id and count prod outputs. After the with statement, I'll add the filter statement where countprod is less than or equal to 2. Then finally ill return the filtered results. 
In the next lab you'll get the chance to use the match statements and learn more about other statements such as create and delete. You'll be guided to open up the Neo four J graphical interface if you'd like to visualize the graph. But to complete the lab, you'll mainly be working in JupyterLab where you'll write queries in the code cells. Your queries will be wrapped in Python that automatically connects the graph database provided in the lab. You'll also learn how to interact with Neo4j not only as a graph database, but also as a vector database. 



#### Summary
Great job making it to the end of this week's content. We covered lots of details in the first two layers of the storage hierarchy, the raw storage ingredients and the storage systems that build on top of them. We compared the costs and performance of various raw storage ingredients, including magnetic disks, SSDs, and RAM. You considered each of the roles in a variety of storage solutions. Then you built on top of your previous knowledge of object storage and distributed storage systems and explored the differences between file, block, and object storage in a practical lab. Even though object storage is probably the most important mechanism for file storage and retrieval in your work as a data engineer today, you need to be ready to evaluate the performance of each cloud storage option so you can choose the best one for your organization's use case. In the second lesson of this week, our focus was on databases. 
You saw that database storage engines are responsible for physically storing and organizing data on disk, and through exploring various examples, you learned how indexing can be used to improve query performance. We also dove into the details of row versus columnar storage, and you saw different data engineering use cases for each type of format. Finally, we surveyed some databases that have gained popularity with the rise of generative AI. You learned how graph and vector databases store and retrieve data, and then you got a chance to practice the Neo4j graph database, along with the Cypher query language. Next week, we'll move up the storage hierarchy and discuss storage abstractions. You'll see how the storage architectures and frameworks evolved with the emergence of data warehouses, then data lakes, and then data lakehouses, and you'll explore the details of each architecture. I'll see you there. 

